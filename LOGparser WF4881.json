{
  "name": "LOGparser",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "LOGparser",
        "options": {
          "noResponseBody": true
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        5952,
        -2816
      ],
      "id": "15d8b58e-4d9d-445c-bb42-ea8a508348bb",
      "name": "Webhook",
      "webhookId": "62bcabb1-4964-4f50-9488-179a800bc3fb"
    },
    {
      "parameters": {
        "jsCode": "// Extract message — FULL REPLACE v3 (adds audio detection + hasAudio)\n// Input: $json.body.updates[0]\n// Output: { text, chatId, login, hasFile, hasAudio, files[] }\n\nfunction normalizeText(s) {\n  return String(s || '').replace(/\\u00A0/g, ' ').trim();\n}\n\nfunction toArrayMaybe(x) {\n  if (!x) return [];\n  if (Array.isArray(x)) return x;\n  return [x];\n}\n\nfunction uniqPushFile(arr, f) {\n  const kind = String(f.kind || '');\n  const id = String(f.file_id || f.id || '');\n  const key = `${kind}|${id}`;\n  if (!id) return;\n  if (arr._seen && arr._seen.has(key)) return;\n  arr._seen = arr._seen || new Set();\n  arr._seen.add(key);\n  arr.push(f);\n}\n\nfunction guessKindByName(name) {\n  const n = String(name || '').toLowerCase();\n  if (/\\.(png|jpg|jpeg|bmp|gif|tif|tiff|webp)$/i.test(n)) return 'image';\n  if (/\\.(ogg|opus|mp3|wav|m4a|aac|flac)$/i.test(n)) return 'audio';\n  return 'file';\n}\n\nconst update = $json?.body?.updates?.[0];\nif (!update) return [];\n\nconst text = normalizeText(update.text || '') || 'Empty';\n\n// chatId/login\nconst chatId = update?.from?.id ?? update?.chat?.id ?? null;\nconst login = update?.from?.login ?? null;\n\nconst files = [];\n\n// images can come in different shapes: images[0] array, images array, or object\nconst imagesRaw = update.images;\nif (imagesRaw) {\n  // sometimes images is [[...]] or [...]\n  const imgs = Array.isArray(imagesRaw) && Array.isArray(imagesRaw[0]) ? imagesRaw[0] : imagesRaw;\n  for (const src of toArrayMaybe(imgs)) {\n    if (!src) continue;\n    const file_id = src.file_id || src.id;\n    uniqPushFile(files, {\n      kind: 'image',\n      file_id,\n      name: src.name || '',\n      size: src.size,\n      width: src.width,\n      height: src.height,\n    });\n  }\n}\n\n// generic file (docs, logs, voice, etc)\nif (update.file) {\n  const f = update.file;\n  const file_id = f.file_id || f.id;\n  const name = f.name || '';\n  const kind = guessKindByName(name);\n  uniqPushFile(files, {\n    kind,\n    file_id,\n    name,\n    size: f.size,\n  });\n}\n\nconst hasFile = files.length > 0;\nconst hasAudio = files.some(f => String(f.kind).toLowerCase() === 'audio');\n\nreturn [{\n  json: {\n    text,\n    chatId,\n    login,\n    hasFile,\n    hasAudio,\n    files,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        6176,
        -2816
      ],
      "id": "eebbb14a-fe38-4a78-b7a6-68c98627153a",
      "name": "Extract message"
    },
    {
      "parameters": {
        "jsCode": "// Prepare reply — FULL REPLACE v2-B (LOGparser)\n// - Always returns TXT file with final report\n// - File name: \"<archiveBase>.txt\" (fallback logparser_report.txt)\n// - Preserves chatId/login; sendText splitting handled by downstream If file + Send Text/Send file nodes\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction normalizeChatId(chatId) {\n  if (!chatId) return null;\n  const t = String(chatId).trim();\n  return t.includes('/') ? t : `0/0/${t}`;\n}\n\nfunction extractAssistantText(j) {\n  if (!j || typeof j !== 'object') return '';\n  if (typeof j.message?.content === 'string') return j.message.content;\n  if (typeof j.ollama?.message?.content === 'string') return j.ollama.message.content;\n  if (typeof j.response === 'string') return j.response;\n  if (typeof j.text === 'string') return j.text;\n  return '';\n}\n\nfunction stripThink(raw) {\n  return s(raw).replace(/<think>[\\s\\S]*?<\\/think>/gi, '').trim();\n}\n\nfunction baseName(name) {\n  const t = s(name).trim();\n  if (!t) return 'logparser_report';\n  const just = t.split(/[\\\\/]/).pop();\n  return just.replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z)$/i, '');\n}\n\nconst base = $input.item.json || {};\nconst chatId = normalizeChatId(base.chatId || base.chat_id);\nconst login = base.login || base.user || null;\n\nconst content = stripThink(extractAssistantText(base)).trim() || stripThink(s(base.message?.content)).trim();\nconst fileBase = baseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\nconst outName = `${fileBase}.txt`;\n\n// Add UTF-8 BOM (Excel-friendly if needed)\nconst bom = Buffer.from([0xEF,0xBB,0xBF]);\nconst buf = Buffer.concat([bom, Buffer.from(content || 'N/A', 'utf8')]);\n\nreturn [{\n  json: {\n    ...base,\n    chatId,\n    login,\n    fileName: outName,\n    isFile: true,\n  },\n  binary: {\n    file: {\n      data: buf.toString('base64'),\n      fileName: outName,\n      mimeType: 'text/plain; charset=utf-8',\n    }\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        11488,
        -2816
      ],
      "id": "d210e9a2-5a53-43a5-8f85-227e47040a72",
      "name": "Prepare reply"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "processed": false,
            "chatId": "={{$json.chatId}}",
            "timestamp": "={{Date.now()}}",
            "text": "={{$json.text}}",
            "files": "={{ JSON.stringify($json.files || []) }}",
            "login": "={{$json.login}}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "chatId",
              "displayName": "chatId",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "timestamp",
              "displayName": "timestamp",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "number",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "text",
              "displayName": "text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "processed",
              "displayName": "processed",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "boolean",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "files",
              "displayName": "files",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "login",
              "displayName": "login",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {
          "optimizeBulk": false
        }
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        6400,
        -2816
      ],
      "id": "7b74ddfe-3491-417f-9d7d-675f60a7062b",
      "name": "Data message"
    },
    {
      "parameters": {
        "operation": "get",
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "matchType": "allConditions",
        "filters": {
          "conditions": [
            {
              "keyName": "chatId",
              "keyValue": "={{$json.chatId}}"
            },
            {
              "keyName": "processed",
              "condition": "isFalse"
            }
          ]
        },
        "returnAll": true
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        6624,
        -2816
      ],
      "id": "9fa95fe1-97cc-41a8-991f-ec5013050109",
      "name": "Read messages"
    },
    {
      "parameters": {
        "operation": "update",
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "matchType": "allConditions",
        "filters": {
          "conditions": [
            {
              "keyName": "chatId",
              "keyValue": "={{$json.chatId}}"
            },
            {
              "keyName": "processed",
              "condition": "isFalse"
            }
          ]
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "processed": true
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "chatId",
              "displayName": "chatId",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "timestamp",
              "displayName": "timestamp",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "number",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "text",
              "displayName": "text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "processed",
              "displayName": "processed",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "boolean",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "files",
              "displayName": "files",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "login",
              "displayName": "login",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        7504,
        -3008
      ],
      "id": "760f60ce-4481-4027-847e-a8a398926ca6",
      "name": "Update row(s)",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "9762fa70-8640-4f1b-9966-3e747cacf98f",
              "leftValue": "={{$json[\"isLeader\"]}}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        7072,
        -2816
      ],
      "id": "fc6cca5e-4254-4062-b22e-3e61e3b70633",
      "name": "If  leader"
    },
    {
      "parameters": {
        "jsCode": "// Code node \"Stop\"\n// Ничего не делаем и останавливаем цепочку для этих сообщений\nreturn [];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        7504,
        -2624
      ],
      "id": "27457a42-4f85-4d49-939e-1cd36cd9e670",
      "name": "Stop"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://botapi.messenger.yandex.net/bot/v1/messages/sendText/",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "{{ 'OAuth ' + ($env.YANDEX_BOT_TOKEN_LOGPARSER || '') }}"
            }
          ]
        },
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "login",
              "value": "={{$json.login}}"
            },
            {
              "name": "text",
              "value": "={{$json.text}}"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        11936,
        -2720
      ],
      "id": "230d048c-49c0-4832-b6e5-19d9a4ca76e8",
      "name": "Send Text"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://botapi.messenger.yandex.net/bot/v1/messages/sendFile/",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "={{ 'OAuth ' + ($env.YANDEX_BOT_TOKEN_LOGPARSER || '') }}"
            }
          ]
        },
        "sendBody": true,
        "contentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "name": "login",
              "value": "={{$json.login}}"
            },
            {
              "parameterType": "formBinaryData",
              "name": "document",
              "inputDataFieldName": "file"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        11936,
        -2912
      ],
      "id": "4e623cb1-f899-4573-97b3-42b42dd308bb",
      "name": "Send file"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "15527451-ad55-4b9e-a7e4-5db6dfdfe946",
              "leftValue": "={{ !!$binary.file }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        11712,
        -2816
      ],
      "id": "c933d8d3-2e32-478b-9da1-e59ee128aafd",
      "name": "If file"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "59471c9d-4615-4b45-9c46-3e20c1b91d9f",
              "leftValue": "={{ String($json.klass || '').trim().toLowerCase() === 'storage' }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        8976,
        -2816
      ],
      "id": "8ab30f44-4f66-4068-8359-e9e19a7e7f27",
      "name": "IF klass == \"Storage\""
    },
    {
      "parameters": {
        "jsCode": "// Storage Structure Extract — FULL REPLACE v5.0 (read combined file, produce digest_structure_text only)\n\nconst fs = require('fs');\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst fp = s(base.combinedLogPath).trim();\nconst MAX_READ = Number($env.LLM2_READ_CHARS || 4000000); // 4M chars head\n\nfunction readHead(p, maxChars){\n  try { return fs.readFileSync(p, 'utf8').slice(0, maxChars); } catch { return ''; }\n}\n\nfunction dedupMax(text, maxPer){\n  const counts = new Map();\n  const out = [];\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const k = line.trim().replace(/\\s+/g,' ').toLowerCase();\n    if (!k) { out.push(line); continue; }\n    const n = counts.get(k) || 0;\n    if (n >= maxPer) continue;\n    counts.set(k, n+1);\n    out.push(line);\n  }\n  return out.join('\\n');\n}\n\nconst head = readHead(fp, MAX_READ);\n\n// Pick “structure-ish” lines\nconst keep = [];\nconst rx = /(enclosure|shelf|chassis|controller|mc|fru|serial|part|firmware|expansion|disk\\s+group|vdisk|pool|drawer)/i;\n\nfor (const line of head.replace(/\\r\\n/g,'\\n').split('\\n')) {\n  if (rx.test(line)) keep.push(line);\n  if (keep.length >= 12000) break;\n}\n\nlet digest = keep.join('\\n');\ndigest = dedupMax(digest, 3);\n\nreturn [{\n  json: {\n    ...base,\n    digest_structure_text: digest || 'N/A',\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9280,
        -3104
      ],
      "id": "9f253fa6-dd0a-4b31-8b09-1ac2274920c4",
      "name": "Storage Structure Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Disks Extract — FULL REPLACE v5.0 (read combined file, produce digest_disks_text only)\n\nconst fs = require('fs');\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst fp = s(base.combinedLogPath).trim();\nconst MAX_READ = Number($env.LLM3_READ_CHARS || 6000000); // 6M chars head\n\nfunction readHead(p, maxChars){\n  try { return fs.readFileSync(p, 'utf8').slice(0, maxChars); } catch { return ''; }\n}\n\nfunction dedupMax(text, maxPer){\n  const counts = new Map();\n  const out = [];\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const k = line.trim().replace(/\\s+/g,' ').toLowerCase();\n    if (!k) { out.push(line); continue; }\n    const n = counts.get(k) || 0;\n    if (n >= maxPer) continue;\n    counts.set(k, n+1);\n    out.push(line);\n  }\n  return out.join('\\n');\n}\n\nconst head = readHead(fp, MAX_READ);\n\nconst keep = [];\nconst rx = /(disk|drive|slot|bay|serial|wwn|sas|sata|nvme|part|model|rpm|size|gb|tb|health|smart|pd|physical|enclosure\\s+\\d+)/i;\n\nfor (const line of head.replace(/\\r\\n/g,'\\n').split('\\n')) {\n  if (rx.test(line)) keep.push(line);\n  if (keep.length >= 20000) break;\n}\n\nlet digest = keep.join('\\n');\ndigest = dedupMax(digest, 3);\n\nreturn [{\n  json: {\n    ...base,\n    digest_disks_text: digest || 'N/A',\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9280,
        -2912
      ],
      "id": "1bac4ed6-b713-4909-9fb8-a0bcd620aff0",
      "name": "Storage Disks Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Periphery Extract — FULL REPLACE v5.0 (read combined file, produce digest_periphery_text only)\n\nconst fs = require('fs');\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst fp = s(base.combinedLogPath).trim();\nconst MAX_READ = Number($env.LLM4_READ_CHARS || 3000000); // 3M chars head\n\nfunction readHead(p, maxChars){\n  try { return fs.readFileSync(p, 'utf8').slice(0, maxChars); } catch { return ''; }\n}\n\nfunction dedupMax(text, maxPer){\n  const counts = new Map();\n  const out = [];\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const k = line.trim().replace(/\\s+/g,' ').toLowerCase();\n    if (!k) { out.push(line); continue; }\n    const n = counts.get(k) || 0;\n    if (n >= maxPer) continue;\n    counts.set(k, n+1);\n    out.push(line);\n  }\n  return out.join('\\n');\n}\n\nconst head = readHead(fp, MAX_READ);\n\nconst keep = [];\nconst rx = /(psu|power\\s*supply|fan|sfp|qsfp|transceiver|port|i2c|temperature|volt|current|sensor|module|iom|expander)/i;\n\nfor (const line of head.replace(/\\r\\n/g,'\\n').split('\\n')) {\n  if (rx.test(line)) keep.push(line);\n  if (keep.length >= 12000) break;\n}\n\nlet digest = keep.join('\\n');\ndigest = dedupMax(digest, 3);\n\nreturn [{\n  json: {\n    ...base,\n    digest_periphery_text: digest || 'N/A',\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9280,
        -2720
      ],
      "id": "d7ac765a-62d4-4bd2-b9e4-f80620ee749b",
      "name": "Storage Periphery Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Error Candidates — FULL REPLACE v7.1-no-xlsx\n// Reads STORAGE_ERROR_KEYWORDS_XLSX without external modules (xlsx is disallowed in n8n task-runner).\n// Uses: unzip -p <file.xlsx> <inner.xml> and parses sharedStrings + sheet XML.\n\nconst fs = require('fs');\nconst path = require('path');\nconst { spawnSync } = require('child_process');\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\nfunction fileExists(p){ try { return !!p && fs.existsSync(p) && fs.statSync(p).isFile(); } catch { return false; } }\n\nconst DUP_MAX = 3;\nconst MODEL = s($env.LOGPARSER_REPORT_LLM).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst MAX_LINES_OUT = Number(s($env.LLM5_MAX_LINES).trim() || '1800');\nconst CONTEXT = Number(s($env.LLM5_ERR_CONTEXT).trim() || '2');\n\nfunction resolveCombinedPath(b){\n  const p = s(b.combinedLogPath).trim();\n  if (p && fileExists(p)) return { ok:true, path:p };\n  return { ok:false, error:'No combinedLogPath from ULC (or file not found).' };\n}\n\nfunction pushDedupe(out, line, counts){\n  const c = (counts.get(line) || 0) + 1;\n  counts.set(line, c);\n  if (c > DUP_MAX) return;\n  out.push(line);\n}\n\nfunction norm(ln){ return s(ln).replace(/\\r/g,'').trimEnd(); }\n\nfunction readLinesStream(fp, onLine){\n  const fd = fs.openSync(fp, 'r');\n  const BUF = Buffer.alloc(1024 * 1024);\n  let carry = '';\n  try {\n    while (true){\n      const n = fs.readSync(fd, BUF, 0, BUF.length, null);\n      if (!n) break;\n      const chunk = carry + BUF.slice(0, n).toString('utf8');\n      const parts = chunk.split(/\\r?\\n/);\n      carry = parts.pop() || '';\n      for (const ln of parts) onLine(ln);\n    }\n    if (carry) onLine(carry);\n  } finally {\n    try { fs.closeSync(fd); } catch {}\n  }\n}\n\nfunction shBuf(cmd, args){\n  // Return Buffer from stdout\n  const r = spawnSync(cmd, args, { encoding: null, maxBuffer: 200 * 1024 * 1024 });\n  return { ok: r.status === 0, status: r.status, stdout: r.stdout, stderr: r.stderr };\n}\n\nfunction shTxt(cmd, args){\n  const r = spawnSync(cmd, args, { encoding: 'utf8', maxBuffer: 50 * 1024 * 1024 });\n  return { ok: r.status === 0, status: r.status, stdout: s(r.stdout), stderr: s(r.stderr) };\n}\n\nfunction decodeXmlEntities(t){\n  let x = s(t);\n  x = x.replace(/&lt;/g,'<').replace(/&gt;/g,'>').replace(/&quot;/g,'\"').replace(/&apos;/g,\"'\").replace(/&amp;/g,'&');\n  x = x.replace(/&#x([0-9a-f]+);/ig, (_,h)=>String.fromCharCode(parseInt(h,16)));\n  x = x.replace(/&#(\\d+);/g, (_,d)=>String.fromCharCode(parseInt(d,10)));\n  return x;\n}\n\nfunction unzipList(xlsxPath){\n  const r = shTxt('unzip', ['-Z1', xlsxPath]);\n  if (!r.ok) return [];\n  return r.stdout.split('\\n').map(x=>x.trim()).filter(Boolean);\n}\n\nfunction unzipReadText(xlsxPath, innerPath){\n  const r = shBuf('unzip', ['-p', xlsxPath, innerPath]);\n  if (!r.ok) return '';\n  try { return r.stdout.toString('utf8'); } catch { return ''; }\n}\n\nfunction parseSharedStrings(xml){\n  // returns array sharedStrings[index] = text\n  const out = [];\n  if (!xml) return out;\n  const siRe = /<si\\b[^>]*>([\\s\\S]*?)<\\/si>/g;\n  let m;\n  while ((m = siRe.exec(xml))) {\n    const block = m[1];\n    const tRe = /<t\\b[^>]*>([\\s\\S]*?)<\\/t>/g;\n    let t, parts = [];\n    while ((t = tRe.exec(block))) {\n      parts.push(decodeXmlEntities(t[1]));\n    }\n    out.push(parts.join(''));\n  }\n  return out;\n}\n\nfunction parseSheetKeywords(sheetXml, sharedStrings){\n  if (!sheetXml) return [];\n\n  // Parse rows\n  const rowRe = /<row\\b[^>]*r=\"(\\d+)\"[^>]*>([\\s\\S]*?)<\\/row>/g;\n  const cellRe = /<c\\b([^>]*)>([\\s\\S]*?)<\\/c>/g;\n\n  const rows = new Map(); // rowNum -> { colLetters: value }\n  let rm;\n  while ((rm = rowRe.exec(sheetXml))) {\n    const rowNum = parseInt(rm[1], 10);\n    const rowBlock = rm[2];\n    const rowObj = {};\n\n    let cm;\n    while ((cm = cellRe.exec(rowBlock))) {\n      const attrs = cm[1] || '';\n      const inner = cm[2] || '';\n\n      const refM = attrs.match(/\\br=\"([A-Z]+)(\\d+)\"/);\n      if (!refM) continue;\n      const col = refM[1];\n      // const rnum = parseInt(refM[2],10);\n\n      const tM = attrs.match(/\\bt=\"([^\"]+)\"/);\n      const typ = tM ? tM[1] : '';\n\n      let val = '';\n      if (typ === 's') {\n        const vM = inner.match(/<v>([\\s\\S]*?)<\\/v>/);\n        if (vM) {\n          const idx = parseInt(vM[1],10);\n          val = (idx >= 0 && idx < sharedStrings.length) ? sharedStrings[idx] : '';\n        }\n      } else if (typ === 'inlineStr') {\n        const tM2 = inner.match(/<t\\b[^>]*>([\\s\\S]*?)<\\/t>/);\n        if (tM2) val = decodeXmlEntities(tM2[1]);\n      } else {\n        const vM = inner.match(/<v>([\\s\\S]*?)<\\/v>/);\n        if (vM) val = decodeXmlEntities(vM[1]);\n      }\n\n      val = s(val).trim();\n      if (val) rowObj[col] = val;\n    }\n\n    rows.set(rowNum, rowObj);\n  }\n\n  // Determine header and target column\n  const header = rows.get(1) || {};\n  let targetCol = '';\n  const want = ['keyword','phrase','text'];\n\n  for (const col of Object.keys(header)) {\n    const h = s(header[col]).trim().toLowerCase();\n    if (want.includes(h) || want.some(w => h.includes(w))) { targetCol = col; break; }\n  }\n  if (!targetCol) {\n    // fallback: first column present in header, else 'A'\n    targetCol = Object.keys(header)[0] || 'A';\n  }\n\n  const keywords = [];\n  const seen = new Set();\n  const maxRows = 200000;\n\n  for (let r = 2; r <= maxRows; r++) {\n    const rowObj = rows.get(r);\n    if (!rowObj) continue;\n    const k = s(rowObj[targetCol]).trim();\n    if (!k) continue;\n    const kk = k.toLowerCase();\n    if (kk === 'keyword' || kk === 'phrase' || kk === 'text') continue;\n    if (k.length > 240) continue;\n    if (seen.has(kk)) continue;\n    seen.add(kk);\n    keywords.push(k);\n    if (keywords.length >= 5000) break;\n  }\n\n  return keywords;\n}\n\nfunction loadKeywordsFromXlsxNoLib(xlsxPath){\n  if (!fileExists(xlsxPath)) return [];\n\n  // Need unzip\n  const test = shTxt('sh', ['-lc', 'command -v unzip >/dev/null 2>&1 && echo ok || echo no']);\n  if (!test.ok || !test.stdout.includes('ok')) return [];\n\n  const list = unzipList(xlsxPath);\n  const sheetPath =\n    list.find(x => /^xl\\/worksheets\\/sheet\\d+\\.xml$/i.test(x)) ||\n    list.find(x => x.toLowerCase().startsWith('xl/worksheets/') && x.toLowerCase().endsWith('.xml')) ||\n    '';\n\n  const sharedPath = list.find(x => x.toLowerCase() === 'xl/sharedstrings.xml') || '';\n\n  const sharedXml = sharedPath ? unzipReadText(xlsxPath, sharedPath) : '';\n  const sharedStrings = sharedXml ? parseSharedStrings(sharedXml) : [];\n\n  const sheetXml = sheetPath ? unzipReadText(xlsxPath, sheetPath) : '';\n  const keywords = parseSheetKeywords(sheetXml, sharedStrings);\n\n  return keywords;\n}\n\n// load keywords from env path\nconst XLSX_PATH =\n  s($env.STORAGE_ERROR_KEYWORDS_XLSX).trim()\n  || '/srv/yadisk/LLM QLoRA/LOGparser/storage_error_keywords_for_llm5.xlsx';\n\nlet keywords = [];\nlet keywords_debug = { mode: 'no-xlsx-lib', xlsxPath: XLSX_PATH, loaded: 0, usedFallback: false };\n\ntry {\n  keywords = loadKeywordsFromXlsxNoLib(XLSX_PATH);\n  keywords_debug.loaded = keywords.length;\n} catch (e) {\n  keywords = [];\n  keywords_debug.error = s(e && e.message);\n}\n\n// if xlsx empty - fallback generic\nif (!keywords.length){\n  keywords_debug.usedFallback = true;\n  keywords = [\n    'fault', 'critical', 'error', 'degraded', 'quarantined', 'quarantine',\n    'disk group', 'dg', 'I/O error', 'failed', 'panic', 'assert',\n  ];\n}\n\n// compile regex (case-insensitive OR)\nfunction escapeRe(x){ return x.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'); }\nconst bigRe = new RegExp(keywords.map(escapeRe).slice(0, 200).join('|'), 'i'); // keep regex sane\n\nconst src = resolveCombinedPath(base);\nif (!src.ok){\n  const digest = `[n8n] Storage Error Candidates failed: ${s(src.error)}`;\n  return [{\n    json: {\n      ...base,\n      task:'storage_errors',\n      model: MODEL,\n      error_candidates_text: digest,\n      _keywords_debug: keywords_debug,\n      messages:[{role:'user', content: digest}]\n    }\n  }];\n}\n\n// scan with context window\nconst out = [];\nconst counts = new Map();\nconst ring = []; // prev lines\nlet remainingBudget = MAX_LINES_OUT;\n\nreadLinesStream(src.path, (raw) => {\n  if (remainingBudget <= 0) return;\n\n  const L = norm(raw);\n  if (!L) return;\n\n  // maintain ring\n  ring.push(L);\n  if (ring.length > CONTEXT) ring.shift();\n\n  if (bigRe.test(L)){\n    for (const prev of ring) pushDedupe(out, prev, counts);\n    pushDedupe(out, L, counts);\n    remainingBudget = MAX_LINES_OUT - out.length;\n  }\n});\n\nconst header =\n`=== Error candidates (keyword hits, context=${CONTEXT}, dup<=${DUP_MAX}) ===`;\n\nlet error_candidates_text = header + '\\n' + (out.length ? out.join('\\n') : 'N/A');\nif (out.length > MAX_LINES_OUT) {\n  error_candidates_text = header + '\\n' + out.slice(0, MAX_LINES_OUT).join('\\n') + '\\n[n8n] TRUNCATED';\n}\n\nconst llm_input =\n`TASK: STORAGE ERRORS (LLM5)\n\nYou are given error candidate lines extracted from the combined bundle using a keyword list.\nMatch and group errors/events. Focus on disk group / disk issues (quarantined/fault), and list affected components if evidenced.\n\nRULES:\n- Evidence-first. No guessing.\n- Do NOT echo logs.\n- If no real errors -> N/A.\n\nEVIDENCE:\n<<<BEGIN\n${error_candidates_text}\nEND>>>`;\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_errors',\n    model: MODEL,\n    error_candidates_text,\n    _keywords_debug: keywords_debug,\n    messages: [{ role:'user', content: llm_input }],\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9280,
        -2528
      ],
      "id": "7e87a6f3-7094-4035-b769-383b5e2c8816",
      "name": "Storage Error Candidates"
    },
    {
      "parameters": {
        "jsCode": "// Assemble Storage Report — FULL REPLACE v5-table-format\n// Output formatting rules:\n// 1) Drop columns where ALL rows are empty or N/A.\n// 2) Render tables as aligned text (spaces), not semicolon-separated.\n// 3) Between tables: \"======================================================\"\n\nfunction s(x){ return x == null ? \"\" : String(x); }\n\nconst SEP_LINE = \"======================================================\";\nconst COL_GAP = \"  \"; // at least 2 spaces\n\nfunction extractAssistantText(j) {\n  if (!j || typeof j !== \"object\") return \"\";\n  if (typeof j.message?.content === \"string\") return j.message.content;\n  if (typeof j.ollama?.message?.content === \"string\") return j.ollama.message.content;\n  if (typeof j.response === \"string\") return j.response;\n  if (typeof j.text === \"string\") return j.text;\n  return \"\";\n}\n\nfunction stripThink(raw) { return s(raw).replace(/<think>[\\s\\S]*?<\\/think>/gi, \"\").trim(); }\n\nfunction stripCodeFences(raw) {\n  const t = s(raw).trim();\n  const m = t.match(/^```[a-z0-9_-]*\\s*([\\s\\S]*?)\\s*```$/i);\n  return m ? m[1].trim() : t;\n}\n\nfunction normTask(t){\n  const x = s(t).trim().toLowerCase();\n  if (x.includes(\"structure\")) return \"structure\";\n  if (x.includes(\"disk\")) return \"disks\";\n  if (x.includes(\"periph\")) return \"periphery\";\n  if (x.includes(\"error\")) return \"errors\";\n  return \"unknown\";\n}\n\nfunction collectAllItems(){\n  const out = [];\n  try { for (const it of $input.all()) out.push(it.json || {}); }\n  catch { out.push($input.item.json || {}); }\n  return out;\n}\n\nfunction nowMSK() {\n  try {\n    const d = new Date();\n    const dtf = new Intl.DateTimeFormat(\"en-GB\", {\n      timeZone: \"Europe/Moscow\",\n      year: \"numeric\", month: \"2-digit\", day: \"2-digit\",\n      hour: \"2-digit\", minute: \"2-digit\", second: \"2-digit\",\n      hour12: false,\n    });\n    const parts = dtf.format(d).replace(\",\", \"\");\n    const m = parts.match(/^(\\d{2})\\/(\\d{2})\\/(\\d{4})\\s+(\\d{2}):(\\d{2}):(\\d{2})$/);\n    if (!m) return parts + \" (MSK)\";\n    return `${m[1]}-${m[2]}-${m[3]} ${m[4]}:${m[5]}:${m[6]} (MSK)`;\n  } catch {\n    const d = new Date(Date.now() + 3*3600*1000);\n    return d.toISOString().replace(\"T\",\" \").replace(\"Z\",\"\") + \" (MSK~)\";\n  }\n}\n\nfunction extractErrorsOnly(raw) {\n  let t = stripCodeFences(stripThink(raw)).trim();\n  if (!t) return \"\";\n\n  const idxTable = t.toUpperCase().indexOf(\"TABLE: ERRORS\");\n  if (idxTable >= 0) return t.slice(idxTable).trim();\n\n  const reAll = /(^|\\n)\\s*4\\)\\s*Errors:\\s*/gi;\n  let last = -1;\n  let m;\n  while ((m = reAll.exec(t)) !== null) last = m.index;\n  if (last >= 0) {\n    const cut = t.slice(last);\n    return cut.replace(/^\\s*4\\)\\s*Errors:\\s*/i, \"\").trim();\n  }\n\n  return t.trim();\n}\n\nfunction isNA(val) {\n  const v = s(val).trim();\n  if (!v) return true;\n  const low = v.toLowerCase();\n  return low === \"n/a\" || low === \"na\" || low === \"-\" || low === \"none\";\n}\n\nfunction parseRow(line, delim, nCols) {\n  const parts = s(line).split(delim).map(x => x.trim());\n  if (nCols <= 1) return parts;\n  if (parts.length === nCols) return parts;\n  if (parts.length < nCols) {\n    while (parts.length < nCols) parts.push(\"\");\n    return parts;\n  }\n  // too many splits: merge tail into last cell (safe for Message that may contain ';')\n  const head = parts.slice(0, nCols - 1);\n  const tail = parts.slice(nCols - 1).join(delim).trim();\n  return head.concat([tail]);\n}\n\nfunction detectDelimiter(headerLine) {\n  const l = s(headerLine);\n  if (l.includes(\";\")) return \";\";\n  if (l.includes(\"\\t\")) return \"\\t\";\n  return \"\";\n}\n\nfunction formatTable(name, headerLine, rowLines) {\n  const delim = detectDelimiter(headerLine);\n  if (!delim) {\n    // Not a delimited table; return as-is\n    const body = [headerLine].concat(rowLines || []).join(\"\\n\").trim();\n    return [`TABLE: ${name}`, body || \"N/A\"].join(\"\\n\").trim();\n  }\n\n  const headers = parseRow(headerLine, delim, 999);\n  const nCols = headers.length;\n\n  // rows\n  let rows = [];\n  for (const rl of rowLines) {\n    const r = s(rl).trim();\n    if (!r) continue;\n    if (r.toUpperCase() === \"N/A\") continue;\n    rows.push(parseRow(r, delim, nCols));\n  }\n\n  if (!rows.length) {\n    return [`TABLE: ${name}`, \"N/A\"].join(\"\\n\");\n  }\n\n  // drop columns where ALL rows are N/A/empty\n  const keepIdx = [];\n  for (let c = 0; c < nCols; c++) {\n    let allEmpty = true;\n    for (const r of rows) {\n      if (!isNA(r[c])) { allEmpty = false; break; }\n    }\n    if (!allEmpty) keepIdx.push(c);\n  }\n\n  // If everything was dropped, table is effectively empty\n  if (!keepIdx.length) {\n    return [`TABLE: ${name}`, \"N/A\"].join(\"\\n\");\n  }\n\n  const keptHeaders = keepIdx.map(i => headers[i] || \"\");\n  const keptRows = rows.map(r => keepIdx.map(i => r[i] || \"\"));\n\n  // widths (align all but last column; last column prints as-is)\n  const widths = keptHeaders.map(h => Math.max(3, s(h).length));\n  for (const r of keptRows) {\n    for (let i = 0; i < r.length; i++) {\n      if (i === r.length - 1) continue; // last column no width pressure\n      widths[i] = Math.max(widths[i], s(r[i]).length);\n    }\n  }\n\n  function fmtLine(cells) {\n    const out = [];\n    for (let i = 0; i < cells.length; i++) {\n      const v = s(cells[i]).trim();\n      if (i === cells.length - 1) out.push(v);\n      else out.push(v.padEnd(widths[i], \" \"));\n    }\n    return out.join(COL_GAP).trimEnd();\n  }\n\n  const lines = [];\n  lines.push(`TABLE: ${name}`);\n  lines.push(fmtLine(keptHeaders));\n  for (const r of keptRows) lines.push(fmtLine(r));\n  return lines.join(\"\\n\").trim();\n}\n\nfunction formatTablesInText(text) {\n  const t = s(text).replace(/\\r\\n/g, \"\\n\").trim();\n  if (!t) return \"\";\n\n  const lines = t.split(\"\\n\");\n  const out = [];\n  let i = 0;\n  let tablesEmitted = 0;\n\n  while (i < lines.length) {\n    const line = lines[i];\n\n    const m = line.match(/^\\s*TABLE:\\s*(.+?)\\s*$/i);\n    if (!m) {\n      out.push(line);\n      i++;\n      continue;\n    }\n\n    const name = s(m[1]).trim();\n    // find header line\n    i++;\n    while (i < lines.length && !lines[i].trim()) i++;\n\n    if (i >= lines.length) {\n      // table name without content\n      if (tablesEmitted > 0) out.push(SEP_LINE);\n      out.push(`TABLE: ${name}`);\n      out.push(\"N/A\");\n      tablesEmitted++;\n      break;\n    }\n\n    // if directly N/A\n    if (lines[i].trim().toUpperCase() === \"N/A\") {\n      if (tablesEmitted > 0) out.push(SEP_LINE);\n      out.push(`TABLE: ${name}`);\n      out.push(\"N/A\");\n      tablesEmitted++;\n      i++;\n      continue;\n    }\n\n    const headerLine = lines[i];\n    i++;\n\n    const rowLines = [];\n    while (i < lines.length) {\n      const cur = lines[i];\n      if (/^\\s*TABLE:\\s*/i.test(cur)) break;\n      // allow blank lines inside table: we keep them as row separators (ignored by parser)\n      rowLines.push(cur);\n      i++;\n    }\n\n    if (tablesEmitted > 0) out.push(SEP_LINE);\n    out.push(formatTable(name, headerLine, rowLines));\n    tablesEmitted++;\n  }\n\n  return out.join(\"\\n\").replace(/\\n{3,}/g, \"\\n\\n\").trim();\n}\n\n// ---------------- MAIN ----------------\nconst items = collectAllItems();\nconst any = items.find(it => it.hw_vendor || it.vendor || it.klass || it.modelDetected) || items[0] || {};\n\nconst byTask = { structure: [], disks: [], periphery: [], errors: [] };\nfor (const it of items) {\n  const t = normTask(it.task);\n  if (byTask[t]) byTask[t].push(it);\n}\n\nfunction bestRaw(task){\n  const arr = byTask[task] || [];\n  if (!arr.length) return \"\";\n  arr.sort((a,b) => extractAssistantText(b).length - extractAssistantText(a).length);\n  return extractAssistantText(arr[0]);\n}\n\nconst rawStruct0 = stripCodeFences(stripThink(bestRaw(\"structure\")));\nconst rawDisks0  = stripCodeFences(stripThink(bestRaw(\"disks\")));\nconst rawPer0    = stripCodeFences(stripThink(bestRaw(\"periphery\")));\nconst rawErrRaw  = bestRaw(\"errors\");\nconst rawErr0    = extractErrorsOnly(rawErrRaw);\n\n// APPLY TABLE FORMATTER\nconst rawStruct = formatTablesInText(rawStruct0) || (rawStruct0.trim() ? rawStruct0.trim() : \"N/A\");\nconst rawDisks  = formatTablesInText(rawDisks0)  || rawDisks0.trim();\nconst rawPer    = formatTablesInText(rawPer0)    || rawPer0.trim();\nconst rawErr    = rawErr0.trim() ? formatTablesInText(rawErr0) : \"\";\n\nconst reportLLM = s(any.ReportLLM || $env.LOGPARSER_REPORT_LLM || \"qwen2.5:32b-instruct-q8_0\").trim() || \"qwen2.5:32b-instruct-q8_0\";\nconst reportTs  = s(any.ReportTimestampMSK).trim() || nowMSK();\n\nconst vendor = s(any.hw_vendor || any.vendor || \"\").trim() || \"N/A\";\nconst klass  = s(any.hw_class || any.klass || any.class || \"\").trim() || \"N/A\";\nconst model  = s(any.hw_model || any.modelDetected || any.model || \"\").trim() || \"N/A\";\n\nconst sec1 =\n`1) Parser metadata:\n   - LLM: ${reportLLM}\n   - Timestamp (MSK): ${reportTs}`.trim();\n\nconst sec2_parts = [];\nsec2_parts.push(`Vendor: ${vendor}`);\nsec2_parts.push(`Class: ${klass}`);\nsec2_parts.push(`Model: ${model}`);\nsec2_parts.push(rawStruct && rawStruct.trim() ? rawStruct.trim() : \"N/A\");\n\nconst sec2 =\n`2) Hardware identification:\n${sec2_parts.join(\"\\n\")}`.trim();\n\nconst sec3_parts = [];\nif (rawDisks && rawDisks.trim()) sec3_parts.push(rawDisks.trim());\nif (rawPer && rawPer.trim()) sec3_parts.push(rawPer.trim());\nif (!sec3_parts.length) sec3_parts.push(\"N/A\");\n\nconst sec3 =\n`3) Component status:\n${sec3_parts.join(\"\\n\\n\")}`.trim();\n\nconst sec4 =\n`4) Errors:\n${rawErr && rawErr.trim() ? rawErr.trim() : \"N/A\"}`.trim();\n\nconst finalReport = [sec1, \"\", sec2, \"\", sec3, \"\", sec4].join(\"\\n\").trim();\n\nreturn [{\n  json: Object.assign({}, any, {\n    message: { role: \"assistant\", content: finalReport },\n  })\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        11264,
        -2816
      ],
      "id": "f371fb27-93a4-4489-ad39-bb6d892fe8be",
      "name": "Assemble Storage Report"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Structure input — FULL REPLACE v3-evidence-file-fit128k\n// New:\n// - Save per-LLM evidence to tmp file (for debugging / re-use)\n// - Only if prompt risks exceeding 128K ctx: drop duplicate lines beyond 3 repeats (late)\n// - If still too large: head+tail clip as last-resort\n\nconst fs = require('fs');\nconst path = require('path');\nconst crypto = require('crypto');\n\nconst s = (x) => (x == null ? '' : String(x));\n\nconst TMP_ROOT = s($env.LOGPARSER_LLM_TMP_ROOT || '/home/node/.n8n/tmp/logparser_llm').trim() || '/home/node/.n8n/tmp/logparser_llm';\nconst TTL_HOURS = Number($env.LOGPARSER_LLM_TTL_HOURS || 72);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive: true }); }\n\nfunction cleanupOld(root, ttlHours){\n  try {\n    if (!fs.existsSync(root)) return;\n    const now = Date.now();\n    for (const name of fs.readdirSync(root)) {\n      const p = path.join(root, name);\n      let st;\n      try { st = fs.statSync(p); } catch { continue; }\n      if (!st.isDirectory()) continue;\n      const ageH = (now - st.mtimeMs) / 3600000;\n      if (ageH > ttlHours) {\n        try { fs.rmSync(p, { recursive:true, force:true }); } catch {}\n      }\n    }\n  } catch {}\n}\n\nfunction safeBaseName(name){\n  const t = s(name).trim();\n  if (!t) return 'archive';\n  const just = t.split(/[\\/]/).pop();\n  return just.replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z|tbz2|tar\\.bz2|bz2)$/i,'').replace(/[^\\w.\\-]+/g,'_').slice(0,120) || 'archive';\n}\n\nfunction pickRunToken(base){\n  const t = s(base.combinedLogToken || '').trim();\n  if (t) return t.replace(/[^\\w.\\-]+/g,'_').slice(0,80);\n  const f = s(base.sourceFileId || '').trim();\n  if (f) return ('fid_' + f.replace(/[^\\w.\\-]+/g,'_').slice(0,60));\n  return `${Date.now()}_${crypto.randomBytes(6).toString('hex')}`;\n}\n\nfunction dedupLinesMax(text, maxPerLine){\n  const counts = new Map();\n  const out = [];\n  let removed = 0;\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const key = line.trim().replace(/\\s+/g,' ');\n    if (!key) { out.push(line); continue; }\n    const c = counts.get(key) || 0;\n    if (c >= maxPerLine) { removed++; continue; }\n    counts.set(key, c + 1);\n    out.push(line);\n  }\n  return { text: out.join('\\n'), removed };\n}\n\nfunction clipHeadTail(text, maxChars){\n  const t = s(text);\n  if (t.length <= maxChars) return t;\n  if (maxChars <= 0) return '';\n  const sep = '\\n[n8n] ... TRUNCATED (head+tail) ...\\n';\n  const sepLen = sep.length;\n  const headLen = Math.max(0, Math.floor((maxChars - sepLen) * 0.65));\n  const tailLen = Math.max(0, maxChars - sepLen - headLen);\n  if (tailLen <= 0) return t.slice(0, Math.max(0, maxChars - 30)) + '\\n[n8n] TRUNCATED';\n  return t.slice(0, headLen) + sep + t.slice(Math.max(0, t.length - tailLen));\n}\n\nfunction writeEvidenceFile(base, taskTag, evidenceText){\n  cleanupOld(TMP_ROOT, TTL_HOURS);\n  ensureDir(TMP_ROOT);\n  const runToken = pickRunToken(base);\n  const srcBase = safeBaseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\n  const dir = path.join(TMP_ROOT, `run_${runToken}`);\n  ensureDir(dir);\n  const fn = `${srcBase}__${taskTag}.txt`.replace(/[^\\w.\\-]+/g,'_').slice(0, 180);\n  const fp = path.join(dir, fn);\n  fs.writeFileSync(fp, s(evidenceText), 'utf8');\n  return fp;\n}\n\nconst base = $input.item.json || {};\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_STRUCTURE).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  s($env.LOGPARSER_LLM_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nconst CTX_TOKENS = Number($env.LOGPARSER_NUM_CTX_TOKENS || 128000);\nconst TOKEN_CHARS = Number($env.LOGPARSER_TOKEN_CHARS || 4);\nconst MAX_CTX_CHARS = CTX_TOKENS * TOKEN_CHARS;\nconst MAX_REPEAT = Number($env.LOGPARSER_MAX_REPEAT_LINES || 3);\n\nfunction getDigest(b) {\n  return (\n    s(b.digest_structure_text) ||\n    s(b.structure_digest) ||\n    s(b.structureText) ||\n    ''\n  );\n}\n\nlet evidence = getDigest(base).trim();\n\n// context (do NOT ask model to output these)\nconst vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\nconst klass  = s(base.hw_class || base.klass || base.class || '').trim() || 'N/A';\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || '').trim() || 'N/A';\n\nfunction buildPrompt(ev) {\n  return `TASK: STORAGE / SYSTEM CONFIG (SECTION 2 TABLES ONLY)\n\nYou are given evidence extracted from the ORIGINAL log archive.\nYour job: produce ONLY the two tables below. No other text. No explanations.\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  1) Enclosures table\n  2) Controllers table\n- No markdown headings. No bullet points. No extra lines outside tables.\n- Each table must start with exactly: \"TABLE: <name>\"\n- Delimiter for tables: semicolon (;)\n- If a table has no data: print the table name and a single line \"N/A\"\n\nTABLE: Enclosures\nenclosure_id;enclosure_type;product_name;serial_number;part_number;enclosure_status\n\nTABLE: Controllers\ncontroller_id;role;product_name;serial_number;part_number;firmware;status\n\nCONTEXT (do NOT echo):\nvendor=${vendor}\nclass=${klass}\nmodel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${ev || '[n8n] No structure evidence found.'}\nEND EVIDENCE>>>`;\n}\n\nlet llm_input = buildPrompt(evidence);\nlet fitMode = 'raw';\nlet dedupRemoved = 0;\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const r = dedupLinesMax(evidence, MAX_REPEAT);\n  evidence = r.text.trim();\n  dedupRemoved = r.removed;\n  fitMode = 'dedup';\n  llm_input = buildPrompt(evidence);\n}\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const overhead = llm_input.length - evidence.length;\n  const budget = Math.max(0, MAX_CTX_CHARS - overhead - 2000);\n  evidence = clipHeadTail(evidence, budget).trim();\n  fitMode = 'clip_head_tail';\n  llm_input = buildPrompt(evidence);\n}\n\nconst evidencePath = writeEvidenceFile(base, 'llm2_structure_evidence', evidence);\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_structure',\n    taskOrder: 2,\n    model,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n\n    llm2_evidence_path: evidencePath,\n    llm2_evidence_chars: evidence.length,\n    llm2_fit_mode: fitMode,\n    llm2_dedup_removed: dedupRemoved,\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9504,
        -3104
      ],
      "id": "37bf7c25-f15e-4fb7-8747-6803b50b373d",
      "name": "Compose Storage Structure input"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Disk input — FULL REPLACE v4-evidence-file-fit128k\n// New:\n// - Save per-LLM evidence to tmp file\n// - Only if prompt risks exceeding 128K ctx: drop duplicate lines beyond 3 repeats (late)\n// - If still too large: head+tail clip as last-resort\n\nconst fs = require('fs');\nconst path = require('path');\nconst crypto = require('crypto');\n\nconst s = (x) => (x == null ? '' : String(x));\n\nconst TMP_ROOT = s($env.LOGPARSER_LLM_TMP_ROOT || '/home/node/.n8n/tmp/logparser_llm').trim() || '/home/node/.n8n/tmp/logparser_llm';\nconst TTL_HOURS = Number($env.LOGPARSER_LLM_TTL_HOURS || 72);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive: true }); }\n\nfunction cleanupOld(root, ttlHours){\n  try {\n    if (!fs.existsSync(root)) return;\n    const now = Date.now();\n    for (const name of fs.readdirSync(root)) {\n      const p = path.join(root, name);\n      let st;\n      try { st = fs.statSync(p); } catch { continue; }\n      if (!st.isDirectory()) continue;\n      const ageH = (now - st.mtimeMs) / 3600000;\n      if (ageH > ttlHours) {\n        try { fs.rmSync(p, { recursive:true, force:true }); } catch {}\n      }\n    }\n  } catch {}\n}\n\nfunction safeBaseName(name){\n  const t = s(name).trim();\n  if (!t) return 'archive';\n  const just = t.split(/[\\/]/).pop();\n  return just.replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z|tbz2|tar\\.bz2|bz2)$/i,'').replace(/[^\\w.\\-]+/g,'_').slice(0,120) || 'archive';\n}\n\nfunction pickRunToken(base){\n  const t = s(base.combinedLogToken || '').trim();\n  if (t) return t.replace(/[^\\w.\\-]+/g,'_').slice(0,80);\n  const f = s(base.sourceFileId || '').trim();\n  if (f) return ('fid_' + f.replace(/[^\\w.\\-]+/g,'_').slice(0,60));\n  return `${Date.now()}_${crypto.randomBytes(6).toString('hex')}`;\n}\n\nfunction dedupLinesMax(text, maxPerLine){\n  const counts = new Map();\n  const out = [];\n  let removed = 0;\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const key = line.trim().replace(/\\s+/g,' ');\n    if (!key) { out.push(line); continue; }\n    const c = counts.get(key) || 0;\n    if (c >= maxPerLine) { removed++; continue; }\n    counts.set(key, c + 1);\n    out.push(line);\n  }\n  return { text: out.join('\\n'), removed };\n}\n\nfunction clipHeadTail(text, maxChars){\n  const t = s(text);\n  if (t.length <= maxChars) return t;\n  if (maxChars <= 0) return '';\n  const sep = '\\n[n8n] ... TRUNCATED (head+tail) ...\\n';\n  const sepLen = sep.length;\n  const headLen = Math.max(0, Math.floor((maxChars - sepLen) * 0.65));\n  const tailLen = Math.max(0, maxChars - sepLen - headLen);\n  if (tailLen <= 0) return t.slice(0, Math.max(0, maxChars - 30)) + '\\n[n8n] TRUNCATED';\n  return t.slice(0, headLen) + sep + t.slice(Math.max(0, t.length - tailLen));\n}\n\nfunction writeEvidenceFile(base, taskTag, evidenceText){\n  cleanupOld(TMP_ROOT, TTL_HOURS);\n  ensureDir(TMP_ROOT);\n  const runToken = pickRunToken(base);\n  const srcBase = safeBaseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\n  const dir = path.join(TMP_ROOT, `run_${runToken}`);\n  ensureDir(dir);\n  const fn = `${srcBase}__${taskTag}.txt`.replace(/[^\\w.\\-]+/g,'_').slice(0, 180);\n  const fp = path.join(dir, fn);\n  fs.writeFileSync(fp, s(evidenceText), 'utf8');\n  return fp;\n}\n\nconst base = $input.item.json || {};\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_DISKS).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nconst CTX_TOKENS = Number($env.LOGPARSER_NUM_CTX_TOKENS || 128000);\nconst TOKEN_CHARS = Number($env.LOGPARSER_TOKEN_CHARS || 4);\nconst MAX_CTX_CHARS = CTX_TOKENS * TOKEN_CHARS;\nconst MAX_REPEAT = Number($env.LOGPARSER_MAX_REPEAT_LINES || 3);\n\nfunction getDigest(b) {\n  return (\n    s(b.digest_disks_text) ||\n    s(b.disks_digest) ||\n    s(b.disksText) ||\n    ''\n  );\n}\n\nlet evidence = getDigest(base).trim();\n\nconst vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\nconst klass  = s(base.hw_class || base.klass || base.class || '').trim() || 'N/A';\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || '').trim() || 'N/A';\n\nfunction buildPrompt(ev) {\n  return `TASK: STORAGE / DISKS INVENTORY (SECTION 3 TABLES ONLY)\n\nYou are given evidence extracted from the ORIGINAL log archive.\nYour job: produce ONLY the table below. No other text.\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY the table.\n- No markdown headings. No bullet points. No explanations.\n- Table must start with exactly: \"TABLE: Disks\"\n- Delimiter: semicolon (;)\n- If no data: print the table name and a single line \"N/A\"\n\nTABLE: Disks\nenclosure_id;slot;disk_id;serial_number;part_number;model;vendor;size_gb;type;speed_rpm;state;health;firmware\n\nCONTEXT (do NOT echo):\nvendor=${vendor}\nclass=${klass}\nmodel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${ev || '[n8n] DISKS DIGEST EMPTY'}\nEND EVIDENCE>>>`;\n}\n\nlet llm_input = buildPrompt(evidence);\nlet fitMode = 'raw';\nlet dedupRemoved = 0;\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const r = dedupLinesMax(evidence, MAX_REPEAT);\n  evidence = r.text.trim();\n  dedupRemoved = r.removed;\n  fitMode = 'dedup';\n  llm_input = buildPrompt(evidence);\n}\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const overhead = llm_input.length - evidence.length;\n  const budget = Math.max(0, MAX_CTX_CHARS - overhead - 2000);\n  evidence = clipHeadTail(evidence, budget).trim();\n  fitMode = 'clip_head_tail';\n  llm_input = buildPrompt(evidence);\n}\n\nconst evidencePath = writeEvidenceFile(base, 'llm3_disks_evidence', evidence);\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_disks',\n    taskOrder: 3,\n    model,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n\n    llm3_evidence_path: evidencePath,\n    llm3_evidence_chars: evidence.length,\n    llm3_fit_mode: fitMode,\n    llm3_dedup_removed: dedupRemoved,\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9504,
        -2912
      ],
      "id": "57e86bc6-83de-4b4b-9bb5-6bc394f151dc",
      "name": "Compose Storage Disk input"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Periphery input — FULL REPLACE v3-evidence-file-fit128k\n// New:\n// - Save per-LLM evidence to tmp file\n// - Only if prompt risks exceeding 128K ctx: drop duplicate lines beyond 3 repeats (late)\n// - If still too large: head+tail clip as last-resort\n\nconst fs = require('fs');\nconst path = require('path');\nconst crypto = require('crypto');\n\nconst s = (x) => (x == null ? '' : String(x));\n\nconst TMP_ROOT = s($env.LOGPARSER_LLM_TMP_ROOT || '/home/node/.n8n/tmp/logparser_llm').trim() || '/home/node/.n8n/tmp/logparser_llm';\nconst TTL_HOURS = Number($env.LOGPARSER_LLM_TTL_HOURS || 72);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive: true }); }\n\nfunction cleanupOld(root, ttlHours){\n  try {\n    if (!fs.existsSync(root)) return;\n    const now = Date.now();\n    for (const name of fs.readdirSync(root)) {\n      const p = path.join(root, name);\n      let st;\n      try { st = fs.statSync(p); } catch { continue; }\n      if (!st.isDirectory()) continue;\n      const ageH = (now - st.mtimeMs) / 3600000;\n      if (ageH > ttlHours) {\n        try { fs.rmSync(p, { recursive:true, force:true }); } catch {}\n      }\n    }\n  } catch {}\n}\n\nfunction safeBaseName(name){\n  const t = s(name).trim();\n  if (!t) return 'archive';\n  const just = t.split(/[\\/]/).pop();\n  return just.replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z|tbz2|tar\\.bz2|bz2)$/i,'').replace(/[^\\w.\\-]+/g,'_').slice(0,120) || 'archive';\n}\n\nfunction pickRunToken(base){\n  const t = s(base.combinedLogToken || '').trim();\n  if (t) return t.replace(/[^\\w.\\-]+/g,'_').slice(0,80);\n  const f = s(base.sourceFileId || '').trim();\n  if (f) return ('fid_' + f.replace(/[^\\w.\\-]+/g,'_').slice(0,60));\n  return `${Date.now()}_${crypto.randomBytes(6).toString('hex')}`;\n}\n\nfunction dedupLinesMax(text, maxPerLine){\n  const counts = new Map();\n  const out = [];\n  let removed = 0;\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const key = line.trim().replace(/\\s+/g,' ');\n    if (!key) { out.push(line); continue; }\n    const c = counts.get(key) || 0;\n    if (c >= maxPerLine) { removed++; continue; }\n    counts.set(key, c + 1);\n    out.push(line);\n  }\n  return { text: out.join('\\n'), removed };\n}\n\nfunction clipHeadTail(text, maxChars){\n  const t = s(text);\n  if (t.length <= maxChars) return t;\n  if (maxChars <= 0) return '';\n  const sep = '\\n[n8n] ... TRUNCATED (head+tail) ...\\n';\n  const sepLen = sep.length;\n  const headLen = Math.max(0, Math.floor((maxChars - sepLen) * 0.65));\n  const tailLen = Math.max(0, maxChars - sepLen - headLen);\n  if (tailLen <= 0) return t.slice(0, Math.max(0, maxChars - 30)) + '\\n[n8n] TRUNCATED';\n  return t.slice(0, headLen) + sep + t.slice(Math.max(0, t.length - tailLen));\n}\n\nfunction writeEvidenceFile(base, taskTag, evidenceText){\n  cleanupOld(TMP_ROOT, TTL_HOURS);\n  ensureDir(TMP_ROOT);\n  const runToken = pickRunToken(base);\n  const srcBase = safeBaseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\n  const dir = path.join(TMP_ROOT, `run_${runToken}`);\n  ensureDir(dir);\n  const fn = `${srcBase}__${taskTag}.txt`.replace(/[^\\w.\\-]+/g,'_').slice(0, 180);\n  const fp = path.join(dir, fn);\n  fs.writeFileSync(fp, s(evidenceText), 'utf8');\n  return fp;\n}\n\nconst base = $input.item.json || {};\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_PERIPHERY).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  s($env.LOGPARSER_LLM_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nconst CTX_TOKENS = Number($env.LOGPARSER_NUM_CTX_TOKENS || 128000);\nconst TOKEN_CHARS = Number($env.LOGPARSER_TOKEN_CHARS || 4);\nconst MAX_CTX_CHARS = CTX_TOKENS * TOKEN_CHARS;\nconst MAX_REPEAT = Number($env.LOGPARSER_MAX_REPEAT_LINES || 3);\n\nfunction getDigest(b) {\n  return (\n    s(b.digest_periphery_text) ||\n    s(b.periphery_digest) ||\n    s(b.peripheryText) ||\n    ''\n  );\n}\n\nlet evidence = getDigest(base).trim();\n\nconst vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\nconst klass  = s(base.hw_class || base.klass || base.class || '').trim() || 'N/A';\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || '').trim() || 'N/A';\n\nfunction buildPrompt(ev) {\n  return `TASK: STORAGE / PERIPHERY (SECTION 4 TABLES ONLY)\n\nYou are given evidence extracted from the ORIGINAL log archive.\nYour job: produce ONLY the two tables below. No other text.\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  1) Power Supplies table\n  2) SFP/Transceivers table\n- No markdown headings. No bullet points. No explanations.\n- Each table must start with exactly: \"TABLE: <name>\"\n- Delimiter: semicolon (;)\n- If a table has no data: print the table name and a single line \"N/A\"\n\nTABLE: PowerSupplies\npsu_id;product_name;serial_number;part_number;status\n\nTABLE: SFP\nport_id;port_type;speed;vendor;part_number;serial_number;status\n\nCONTEXT (do NOT echo):\nvendor=${vendor}\nclass=${klass}\nmodel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${ev || '[n8n] No periphery evidence found.'}\nEND EVIDENCE>>>`;\n}\n\nlet llm_input = buildPrompt(evidence);\nlet fitMode = 'raw';\nlet dedupRemoved = 0;\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const r = dedupLinesMax(evidence, MAX_REPEAT);\n  evidence = r.text.trim();\n  dedupRemoved = r.removed;\n  fitMode = 'dedup';\n  llm_input = buildPrompt(evidence);\n}\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const overhead = llm_input.length - evidence.length;\n  const budget = Math.max(0, MAX_CTX_CHARS - overhead - 2000);\n  evidence = clipHeadTail(evidence, budget).trim();\n  fitMode = 'clip_head_tail';\n  llm_input = buildPrompt(evidence);\n}\n\nconst evidencePath = writeEvidenceFile(base, 'llm4_periphery_evidence', evidence);\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_periphery',\n    taskOrder: 4,\n    model,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n\n    llm4_evidence_path: evidencePath,\n    llm4_evidence_chars: evidence.length,\n    llm4_fit_mode: fitMode,\n    llm4_dedup_removed: dedupRemoved,\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9728,
        -2720
      ],
      "id": "0b2ac920-7603-4914-b86b-1f2bc0c75ceb",
      "name": "Compose Storage Periphery input"
    },
    {
      "parameters": {
        "jsCode": "// RAG Retrieve Storage (offline) — FULL REPLACE v3-local-error-catalog\n// Repurposed: DO NOT call rag-api here. We rely on local XLSX keyword catalog prepared in previous node.\n// This node just normalizes/ensures the fields used downstream exist.\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nfunction clip(txt, max = 120000) {\n  const t = s(txt);\n  return t.length > max ? t.slice(0, max) + '\\n[n8n] TRUNCATED' : t;\n}\n\nconst out = {\n  ...base,\n  // Ensure required fields exist for downstream nodes\n  docs_context_text: s(base.docs_context_text || ''),\n  event_catalog_context_text: s(base.event_catalog_context_text || ''),\n  event_catalog_targets: Array.isArray(base.event_catalog_targets) ? base.event_catalog_targets : [],\n};\n\nif (!out.event_catalog_context_text) {\n  // Fallback: if previous node did not provide catalog context, show minimal info\n  const p = s(base._errors_keywords_path || 'N/A');\n  out.event_catalog_context_text = `ERROR KEYWORDS CATALOG (LOCAL)\\nSource: ${p}\\nStatus: ${base._errors_keywords_ok ? 'OK' : 'N/A'}`.trim();\n}\n\nout.docs_context_text = clip(out.docs_context_text, 80000);\nout.event_catalog_context_text = clip(out.event_catalog_context_text, 160000);\n\n// Preserve legacy debug flags (so UI shows why RAG is empty)\nout._rag_mode = 'local-error-keywords-xlsx';\nout._rag_error = ''; // not an error: intentionally offline without rag-api\n\nreturn [{ json: out }];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9504,
        -2528
      ],
      "id": "19cd8f71-9f2e-420f-a0a1-2193915e8f44",
      "name": "RAG Retrieve Storage (offline)"
    },
    {
      "parameters": {
        "jsCode": "// RAG Target Matcher — FULL REPLACE v4-local-keyword-matcher\n// Goal: produce confirmed_events + dgA01_hits using local keyword catalog targets and the digest created upstream.\n// Evidence-first: we NEVER invent events that are not present in digest_errors_text.\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst digest = s(base.digest_errors_text || '');\nconst targets = Array.isArray(base.event_catalog_targets) ? base.event_catalog_targets : [];\n\nfunction escapeRegex(x) { return s(x).replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'); }\n\nfunction severityHeuristic(termOrKw) {\n  const k = s(termOrKw).toLowerCase();\n  const critical = ['quarantine', 'fault', 'fatal', 'panic', 'assert', 'uncorrectable', 'offline', 'missing', 'failed', 'fail'];\n  const major = ['degraded', 'error', 'timeout', 'rebuild', 'recovery', 'failover', 'crc', 'corrupt'];\n  const warning = ['warn', 'warning', 'alarm', 'battery', 'temp', 'temperature', 'fan', 'power', 'voltage'];\n  if (critical.some((x) => k.includes(x))) return 'Critical';\n  if (major.some((x) => k.includes(x))) return 'Major';\n  if (warning.some((x) => k.includes(x))) return 'Warning';\n  return 'Minor';\n}\nfunction componentHeuristic(line) {\n  const t = s(line).toLowerCase();\n  if (t.includes('disk group') || t.includes('vdisk') || t.includes('dg')) return 'DiskGroup';\n  if (t.includes('drive') || t.includes('disk') || t.includes('slot') || t.includes('enclosure')) return 'Disk';\n  if (t.includes('controller') || t.includes('iom')) return 'Controller';\n  if (t.includes('psu') || t.includes('power supply')) return 'PSU';\n  if (t.includes('fan')) return 'Fan';\n  if (t.includes('port') || t.includes('host')) return 'HostPort';\n  return 'Other';\n}\n\nfunction buildRegex(terms) {\n  const clean = terms\n    .map((t) => s(t).trim())\n    .filter((t) => t && t.length >= 3)\n    .slice(0, 1200)\n    .map(escapeRegex);\n  if (!clean.length) return null;\n  // single regex is OK here because digest is already filtered\n  return new RegExp(`(?:${clean.join('|')})`, 'ig');\n}\n\nconst re = buildRegex(targets);\nconst confirmed = [];\nconst dgHits = [];\n\nif (digest && re) {\n  const lines = digest.split(/\\r?\\n/);\n  for (let i = 0; i < lines.length; i++) {\n    const line = lines[i];\n    if (!line) continue;\n\n    // Prefer the real evidence lines (we annotate with \"[source ...]\" upstream)\n    if (line.startsWith('[source ')) {\n      re.lastIndex = 0;\n      const m = re.exec(line);\n      if (m) {\n        const term = s(m[0]).toLowerCase();\n        const sev = severityHeuristic(term);\n        const comp = componentHeuristic(line);\n        let source = '';\n        let sourceLine = '';\n        const sm = line.match(/^\\[source\\s+([^\\]]+)\\]\\s*/);\n        if (sm) {\n          const parts = s(sm[1]).split(':');\n          source = parts.slice(0, -1).join(':') || s(sm[1]);\n          sourceLine = parts.length > 1 ? parts[parts.length - 1] : '';\n        }\n\n        const rec = {\n          phrase: term,\n          term,\n          severity: sev,\n          component: comp,\n          kb: 'N/A',\n          source: source || 'N/A',\n          line: sourceLine ? Number(sourceLine) || 'N/A' : 'N/A',\n          evidence: line.replace(/^\\[source\\s+[^\\]]+\\]\\s*/, '').trim(),\n          digest_line: i + 1,\n        };\n        confirmed.push(rec);\n\n        const low = line.toLowerCase();\n        if (low.includes('dga01') || (low.includes('disk group') && (low.includes('quarantin') || low.includes('fault')))) {\n          dgHits.push({ source: rec.source, line: rec.line, evidence: rec.evidence });\n        }\n      }\n    }\n    if (confirmed.length >= 300) break;\n  }\n}\n\nconst out = {\n  ...base,\n  confirmed_events: Array.isArray(base.confirmed_events) && base.confirmed_events.length\n    ? base.confirmed_events\n    : confirmed,\n  dgA01_hits: Array.isArray(base.dgA01_hits) && base.dgA01_hits.length\n    ? base.dgA01_hits\n    : dgHits.slice(0, 30),\n  _matcher_mode: 'local-keyword-matcher',\n  _matcher_confirmed: (Array.isArray(base.confirmed_events) && base.confirmed_events.length) ? base.confirmed_events.length : confirmed.length,\n};\n\nreturn [{ json: out }];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9728,
        -2528
      ],
      "id": "ca9932a7-6f75-4141-86f3-a4b4584b78c8",
      "name": "RAG Target Matcher"
    },
    {
      "parameters": {
        "jsCode": "// Compose input ERRORS — FULL REPLACE v9 (fix .base/.dgBlock + max_depth=10 + include Config)\n// Purpose: build LLM input for Storage Errors (Section 4 only) + write evidence file\n\nconst fs = require('fs');\nconst path = require('path');\nconst crypto = require('crypto');\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst TMP_ROOT = '/home/node/.n8n/tmp/logparser_evidence';\nconst TTL_HOURS = Number($env.LOGPARSER_EVID_TTL_HOURS || 72);\n\nfunction ensureDir(p){ try { fs.mkdirSync(p, { recursive: true }); } catch {} }\n\nfunction cleanupOld(root, ttlHours){\n  try {\n    if (!fs.existsSync(root)) return;\n    const ttlMs = ttlHours * 3600 * 1000;\n    const now = Date.now();\n    for (const d of fs.readdirSync(root)) {\n      const fp = path.join(root, d);\n      let st;\n      try { st = fs.statSync(fp); } catch { continue; }\n      const age = now - Number(st.mtimeMs || 0);\n      if (age > ttlMs) {\n        try { fs.rmSync(fp, { recursive: true, force: true }); } catch {}\n      }\n    }\n  } catch {}\n}\n\nfunction safeBaseName(name){\n  const t = s(name).trim();\n  if (!t) return 'archive';\n  const just = t.split(/[\\\\/]/).pop();\n  return just\n    .replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z|tbz2|tar\\.bz2|bz2)$/i,'')\n    .replace(/[^\\w.\\-]+/g,'_')\n    .slice(0,120) || 'archive';\n}\n\nfunction pickRunToken(base){\n  const t = s(base.combinedLogToken || '').trim();\n  if (t) return t.replace(/[^\\w.\\-]+/g,'_').slice(0,80);\n  const f = s(base.sourceFileId || '').trim();\n  if (f) return ('fid_' + f.replace(/[^\\w.\\-]+/g,'_').slice(0,60));\n  return `${Date.now()}_${crypto.randomBytes(6).toString('hex')}`;\n}\n\nfunction writeEvidenceFile(base, taskTag, evidenceText){\n  cleanupOld(TMP_ROOT, TTL_HOURS);\n  ensureDir(TMP_ROOT);\n  const runToken = pickRunToken(base);\n  const srcBase = safeBaseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\n  const dir = path.join(TMP_ROOT, `run_${runToken}`);\n  ensureDir(dir);\n  const fn = `${srcBase}__${taskTag}.txt`.replace(/[^\\w.\\-]+/g,'_').slice(0, 180);\n  const fp = path.join(dir, fn);\n  fs.writeFileSync(fp, s(evidenceText), 'utf8');\n  return fp;\n}\n\nfunction dedupLinesMax(text, maxPerLine){\n  const counts = new Map();\n  const out = [];\n  let removed = 0;\n  const lines = s(text).replace(/\\r\\n/g,'\\n').split('\\n');\n  for (const line of lines) {\n    const key = line.trim().replace(/\\s+/g,' ');\n    if (!key) { out.push(line); continue; }\n    const c = counts.get(key) || 0;\n    if (c >= maxPerLine) { removed++; continue; }\n    counts.set(key, c + 1);\n    out.push(line);\n  }\n  return { text: out.join('\\n'), removed };\n}\n\nfunction clipHeadTail(text, maxChars){\n  const t = s(text);\n  if (t.length <= maxChars) return t;\n  if (maxChars <= 0) return '';\n  const sep = '\\n[n8n] . TRUNCATED (head+tail) .\\n';\n  const sepLen = sep.length;\n  const headLen = Math.max(0, Math.floor((maxChars - sepLen) * 0.65));\n  const tailLen = Math.max(0, maxChars - sepLen - headLen);\n  if (tailLen <= 0) return t.slice(0, Math.max(0, maxChars - 30)) + '\\n[n8n] TRUNCATED';\n  return t.slice(0, headLen) + sep + t.slice(Math.max(0, t.length - tailLen));\n}\n\nconst base = $input.item.json || {};\n\nconst CTX_TOKENS = Number($env.LOGPARSER_NUM_CTX_TOKENS || 128000);\nconst TOKEN_CHARS = Number($env.LOGPARSER_TOKEN_CHARS || 4);\nconst MAX_CTX_CHARS = CTX_TOKENS * TOKEN_CHARS;\n\nconst MAX_EXAMPLES_PER_NOMINAL = Number($env.LOGPARSER_MAX_EXAMPLES_PER_NOMINAL || 3);\nconst MAX_REPEAT = Number($env.LOGPARSER_MAX_REPEAT_LINES || 3);\n\nconst confirmed = Array.isArray(base.confirmed_events) ? base.confirmed_events : [];\nconst dgHits = Array.isArray(base.dgA01_hits) ? base.dgA01_hits : [];\n\nfunction sevRank(sev) {\n  const t = String(sev || '').toLowerCase();\n  if (t.includes('crit')) return 4;\n  if (t.includes('maj')) return 3;\n  if (t.includes('warn')) return 2;\n  return 1;\n}\n\n// 1) Bucket by nominal (phrase+nominal+component) to preserve uniqueness\nconst buckets = new Map();\nfor (const e of confirmed) {\n  const phrase = s(e.phrase || e.term || '').trim() || 'N/A';\n  const nominal = s(e.nominal || '').trim() || 'N/A';\n  const comp = s(e.component || '').trim() || 'Other';\n  const key = `${phrase}||${nominal}||${comp}`;\n  if (!buckets.has(key)) buckets.set(key, []);\n  buckets.get(key).push(e);\n}\n\n// 2) Build evidence lines (cap per nominal)\nconst blocks = [];\nfor (const arr of buckets.values()) {\n  const sorted = arr.slice(0).sort((a,b) => sevRank(b.severity) - sevRank(a.severity));\n  const limited = sorted.slice(0, MAX_EXAMPLES_PER_NOMINAL);\n  for (const e of limited) {\n    const line =\n      `ts=${s(e.timestamp || 'N/A')}; ` +\n      `sev=${s(e.severity || 'N/A')}; ` +\n      `comp=${s(e.component || 'N/A')}; ` +\n      `nominal=${s(e.nominal || 'N/A')}; ` +\n      `code=${s(e.phrase || e.term || 'N/A')}; ` +\n      `source=${s(e.source || 'N/A')}:${s(e.line || 'N/A')}; ` +\n      `msg=${s(e.evidence || e.lineText || '').replace(/\\s+/g, ' ').slice(0, 600)}`;\n    blocks.push(line);\n  }\n}\n\n// 3) dgA01 evidence always first\nconst dgBlock = dgHits.slice(0, 3).map(h =>\n  `DG: ${s(h.evidence || '').replace(/\\s+/g, ' ').slice(0, 800)} ` +\n  `[${s(h.source || 'N/A')}:${s(h.line || 'N/A')}]`\n);\n\n// 4) Evidence text (raw)\nlet evidenceText = [dgBlock.join('\\n'), blocks.join('\\n')].join('\\n').trim();\n\nconst HEADER = 'Timestamp;Severity;Component;CodeOrPhrase;Message;Source;KB';\n\nfunction buildPrompt(ev) {\n  return `TASK: STORAGE / ERRORS (SECTION 4 ONLY)\n\nOUTPUT STRICT:\nTABLE: Errors\n${HEADER}\n<rows OR N/A>\n\nRULES:\n- semicolon \";\" delimiter\n- Each nominal error only once (use most representative message)\n- If dgA01 quarantine/missing disk exists — FIRST ROW must summarize it\n- Evidence-first, do NOT invent\n- NEWEST -> OLDEST\n\nEVIDENCE:\n${ev || '[NO EVIDENCE]'}`.trim();\n}\n\nlet llm_input = buildPrompt(evidenceText);\nlet fitMode = 'raw';\nlet dedupRemoved = 0;\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const r = dedupLinesMax(evidenceText, MAX_REPEAT);\n  evidenceText = r.text.trim();\n  dedupRemoved = r.removed;\n  fitMode = 'dedup';\n  llm_input = buildPrompt(evidenceText);\n}\n\nif (llm_input.length > MAX_CTX_CHARS) {\n  const overhead = llm_input.length - evidenceText.length;\n  const budget = Math.max(0, MAX_CTX_CHARS - overhead - 2000);\n  evidenceText = clipHeadTail(evidenceText, budget).trim();\n  fitMode = 'clip_head_tail';\n  llm_input = buildPrompt(evidenceText);\n}\n\nconst evidencePath = writeEvidenceFile(base, 'llm5_errors_evidence', evidenceText);\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_errors',\n    taskOrder: 5,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n\n    llm5_evidence_path: evidencePath,\n    llm5_evidence_chars: evidenceText.length,\n    llm5_fit_mode: fitMode,\n    llm5_dedup_removed: dedupRemoved,\n\n    _errors_nominals: buckets.size,\n    _errors_samples_used: blocks.length,\n    _errors_compose_version: 'v9-evidence-file-fit128k',\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        9952,
        -2528
      ],
      "id": "e96ca195-ccbd-4e4b-92f3-2c3704e87432",
      "name": "Compose input ERRORS"
    },
    {
      "parameters": {
        "numberInputs": 4
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        10816,
        -2848
      ],
      "id": "dd755d66-acf8-41c0-b924-cc2fca84c920",
      "name": "Merge 3"
    },
    {
      "parameters": {
        "jsCode": "// Prepare prompt — FULL REPLACE v1.1 (FIX: \".f\" -> \"...f\", \".base\" -> \"...base\")\n// Purpose: pick leader message with non-image attachment; output single item for LOGparser pipeline.\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction isImageFile(f){\n  const kind = s(f?.kind).toLowerCase();\n  if (kind === 'image') return true;\n  const name = s(f?.name).toLowerCase();\n  return /\\.(png|jpg|jpeg|webp|bmp|tiff|gif)$/i.test(name);\n}\n\nfunction scoreAttachment(f){\n  const name = s(f?.name).toLowerCase();\n  let sc = 0;\n  if (/\\.(zip|tar\\.gz|tgz|tar|gz|7z|tbz2|bz2|xz|zst)$/i.test(name)) sc += 50;\n  if (/\\.(log|logs|txt|csv|xml|json)$/i.test(name)) sc += 10;\n  const size = Number(f?.size);\n  if (Number.isFinite(size)) sc += Math.min(20, Math.floor(size / 5_000_000));\n  return sc;\n}\n\nfunction toTs(r){\n  const t = Number(r?.timestamp);\n  if (Number.isFinite(t) && t > 0) return t;\n  const c = Number(r?.createdAt);\n  if (Number.isFinite(c) && c > 0) return c;\n  return 0;\n}\n\nfunction parseFiles(v){\n  if (!v) return [];\n  if (Array.isArray(v)) return v;\n  if (typeof v === 'string') {\n    try { const j = JSON.parse(v); return Array.isArray(j) ? j : []; } catch { return []; }\n  }\n  return [];\n}\n\nfunction nowMSK(){\n  try {\n    const dt = new Date();\n    const f = new Intl.DateTimeFormat('ru-RU', {\n      timeZone: 'Europe/Moscow',\n      year:'numeric', month:'2-digit', day:'2-digit',\n      hour:'2-digit', minute:'2-digit', second:'2-digit',\n      hour12:false\n    }).formatToParts(dt).reduce((a,p)=>{a[p.type]=p.value; return a;},{});\n    // DD.MM.YYYY -> DD-MM-YYYY\n    return `${f.hour}:${f.minute}:${f.second} ${f.day}-${f.month}-${f.year} (MSK)`;\n  } catch {\n    const d = new Date(Date.now() + 3*3600*1000);\n    return d.toISOString().replace('T',' ').replace('Z','') + ' (MSK~)';\n  }\n}\n\n// Read messages returns multiple items\nconst rows = $input.all().map(it => it.json || {});\nif (!rows.length) return [];\n\nrows.sort((a,b) => toTs(a) - toTs(b));\n\nconst withFiles = rows.map(r => {\n  const files = parseFiles(r.files).map(f => ({ ...f, _msgId: r.id, _msgTimestamp: toTs(r) }));\n  const nonImage = files.filter(f => !isImageFile(f));\n  return { r, files, nonImage, hasNonImage: nonImage.length > 0 };\n});\n\nconst candidates = withFiles.filter(x => x.hasNonImage);\nif (!candidates.length) return []; // no parsing if no non-image files\n\nconst leader = candidates.slice().sort((a,b)=>toTs(a.r)-toTs(b.r)).slice(-1)[0];\nconst leaderRow = leader.r;\n\n// Aggregate all files from all rows (debug)\nconst allFiles = [];\nfor (const x of withFiles) for (const f of x.files) allFiles.push(f);\n\n// Pick source attachment from leader non-image files\nconst leaderNon = leader.nonImage.slice().sort((a,b)=>scoreAttachment(b)-scoreAttachment(a));\nconst main = leaderNon[0] || null;\n\nreturn [{\n  json: {\n    id: leaderRow.id,\n    chatId: leaderRow.chatId ?? null,\n    login: leaderRow.login ?? null,\n\n    files: allFiles,\n    hasFile: allFiles.length > 0,\n    hasImage: allFiles.some(isImageFile),\n    hasNonImage: allFiles.some(f => !isImageFile(f)),\n\n    sourceFileId: main ? s(main.file_id || main.id).trim() : '',\n    sourceFileName: main ? s(main.name).trim() : '',\n\n    ReportTimestampMSK: nowMSK(),\n    ReportLLM: (s($env.LOGPARSER_REPORT_LLM).trim() || 'qwen2.5:32b-instruct-q8_0'),\n\n    wantsLogParsing: true,\n    wantsTxt: true,\n    lastUserText: '',\n\n    isLeader: true,\n    myId: leaderRow.id,\n    leaderId: leaderRow.id,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        6848,
        -2816
      ],
      "id": "dc4b8b4a-adff-4f7b-b6bb-c46699930db9",
      "name": "Prepare prompt"
    },
    {
      "parameters": {
        "jsCode": "// Build Identify Pack — FULL REPLACE v5.3 (lightweight payload)\n// Goal:\n// - Keep execution payload small (do NOT pass large identify_prompt/evidence text between nodes)\n// - Persist only minimal metadata for next node\n\nconst fs = require('fs');\nconst path = require('path');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nfunction fileSizeSafe(fp) {\n  try { return fs.statSync(fp).size; } catch { return 0; }\n}\n\nconst combinedPath = s(base.combinedLogPath).trim();\nconst ulcEvidencePath = s(base.identify_evidence_path).trim();\nconst evidencePath = ulcEvidencePath || combinedPath;\n\nconst hints = [\n  `archiveName=${s(base.archiveName).trim()}`,\n  `sourceFileName=${s(base.sourceFileName).trim()}`,\n  `combinedLogName=${s(base.combinedLogName).trim()}`,\n].filter(Boolean).join(' | ');\n\nconst identifyModel =\n  s(base.identify_model).trim() ||\n  s($env.IDENTIFY_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nreturn [{\n  json: {\n    ...base,\n    identify_model: identifyModel,\n    ollama_format: 'json',\n    identify_evidence_path: evidencePath,\n    identify_evidence_name: s(base.identify_evidence_name).trim() || path.basename(evidencePath || 'combined.txt'),\n    identify_evidence_size_bytes: fileSizeSafe(evidencePath),\n    identify_hints: hints,\n    rag_query_seed: hints.replace(/\\s+/g, ' ').trim().slice(0, 380),\n\n    // keep payload tiny for runData storage\n    identify_prompt: '',\n    llm_input: '',\n    messages: [],\n    identify_evidence_text: '',\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        8000,
        -2816
      ],
      "id": "392fe7e4-0637-4b80-9a7b-f219a0b6321a",
      "name": "Build Identify Pack"
    },
    {
      "parameters": {
        "jsCode": "// Parse Identify Result — FULL REPLACE v5.4 (do NOT override LLM with RAG N/A/Other)\n// Fixes ID4667:\n// - LLM returned Dell EMC / Storage / ME4012, but parser overwrote with rag_identify_vendor=N/A and rag_identify_klass=Other.\n// - Keep legacy fields: vendor/klass/modelDetected for IF and downstream\n// - Keep canonical fields: hw_vendor/hw_class/hw_model\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nfunction stripFences(t) {\n  const m = s(t).match(/```(?:json)?\\s*([\\s\\S]*?)\\s*```/i);\n  return (m ? m[1] : s(t)).trim();\n}\n\nfunction firstJsonObject(text) {\n  const t = stripFences(text);\n  const start = t.indexOf('{');\n  if (start < 0) return null;\n  let depth = 0;\n  for (let i = start; i < t.length; i++) {\n    const ch = t[i];\n    if (ch === '{') depth++;\n    if (ch === '}') {\n      depth--;\n      if (depth === 0) {\n        const block = t.slice(start, i + 1);\n        try { return JSON.parse(block); } catch { return null; }\n      }\n    }\n  }\n  return null;\n}\n\nfunction normNA(x) {\n  const v = s(x).trim();\n  if (!v) return 'N/A';\n  const low = v.toLowerCase();\n  if (['na','n/a','none','null','unknown','-'].includes(low)) return 'N/A';\n  return v;\n}\n\nfunction normClass(x) {\n  const v = normNA(x);\n  if (v === 'N/A') return 'N/A';\n  const low = v.toLowerCase();\n  if (low === 'storage') return 'Storage';\n  if (low === 'server') return 'Server';\n  if (low === 'switch') return 'Switch';\n  if (low === 'ups') return 'UPS';\n  if (low === 'other') return 'Other';\n  // tolerate variants\n  if (low.includes('stor')) return 'Storage';\n  if (low.includes('swit')) return 'Switch';\n  if (low.includes('serv')) return 'Server';\n  return v;\n}\n\nfunction better(current, candidate, { allowOther=false } = {}) {\n  const cur = normNA(current);\n  const cand = normNA(candidate);\n\n  if (cand === 'N/A') return cur;\n  if (!allowOther && cand === 'Other') return cur;\n\n  if (cur === 'N/A') return cand;\n  if (!allowOther && cur === 'Other') return cand;\n\n  return cur; // keep what we already have\n}\n\nfunction extractRaw() {\n  const prefer = [\n    base.identify_raw,\n    base.ollama_text,\n    base.ollama?.message?.content,\n    base.ollama?.response,\n    base.message?.content,\n    base.text,\n  ];\n  for (const x of prefer) {\n    const t = s(x).trim();\n    if (t) return t;\n  }\n  return '';\n}\n\nconst raw = extractRaw();\nconst j = firstJsonObject(raw) || {};\n\nlet vendor = normNA(j.vendor ?? j.manufacturer ?? j.hw_vendor);\nlet klass  = normClass(j.class ?? j.klass ?? j.hw_class);\nlet model  = normNA(j.model ?? j.hw_model ?? j.product_id ?? j.productId);\nlet product_name = normNA(j.product_name ?? j.productName ?? j.name ?? j.array_name);\n\nlet confidence = Number(j.confidence);\nif (!Number.isFinite(confidence)) confidence = (vendor !== 'N/A' && klass !== 'N/A') ? 70 : 35;\n\n// RAG hints: only fill gaps, NEVER override good LLM answer\nvendor = better(vendor, base.rag_identify_vendor, { allowOther: true });\nklass  = better(klass,  base.rag_identify_klass || base.rag_identify_class, { allowOther: false });\nmodel  = better(model,  base.rag_identify_model, { allowOther: true });\n\n// Also allow explicit overrides only if they are meaningful (not N/A/Other)\nvendor = better(vendor, base.vendor, { allowOther: true });\nklass  = better(klass,  base.klass,  { allowOther: false });\nmodel  = better(model,  base.modelDetected, { allowOther: true });\n\nconst identify_ok = (vendor !== 'N/A') && (klass !== 'N/A') && (klass !== 'Other');\n\nreturn [{\n  json: {\n    ...base,\n\n    // legacy (WF4225-compatible)\n    vendor,\n    klass,\n    modelDetected: model,\n    productNameDetected: product_name,\n    confidence,\n\n    // canonical\n    hw_vendor: vendor,\n    hw_class: klass,\n    hw_model: model,\n    hw_product_name: product_name,\n\n    identify_raw: raw,\n    identify_confidence: confidence,\n    _identify_ok: identify_ok,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        8704,
        -2816
      ],
      "id": "e7bda6ca-7537-405e-8b9d-feea14abc263",
      "name": "Parse Identify Result"
    },
    {
      "parameters": {
        "jsCode": "/**\n * LLM Request Identify — FULL REPLACE v3.3 (build prompt from file at runtime)\n * Fixes:\n * - Reads evidence directly from identify_evidence_path / combinedLogPath\n * - Keeps output payload small (no giant prompt/messages in runData)\n * - Forces Ollama JSON mode when requested\n */\n\nconst fs = require('fs');\n\nconst base = $json || {};\nconst OLLAMA = String($env.OLLAMA_URL || 'http://127.0.0.1:11434').replace(/\\/+$/, '');\nconst url = `${OLLAMA}/api/chat`;\n\nfunction s(v, d='') { return (v===undefined || v===null) ? d : String(v); }\n\nfunction maskPII(t) {\n  let x = s(t);\n  x = x.replace(/[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}/ig, '<email>');\n  x = x.replace(/(?:\\+?\\d[\\d\\s\\-()]{7,}\\d)/g, '<phone>');\n  return x;\n}\n\nfunction safeReadHead(fp, maxChars) {\n  try {\n    const buf = fs.readFileSync(fp);\n    return buf.toString('utf8').slice(0, maxChars);\n  } catch {\n    return '';\n  }\n}\n\nfunction prune(out) {\n  const drop = [\n    'messages','llm_input','chatInput','prompt',\n    'identify_prompt','identify_evidence_text','identify_evidence_inline',\n    'docs_context','event_catalog_context',\n    'logsCombined','combinedLogs','logDigest','logDigestText',\n    'error_candidates_text','errors_index_text',\n  ];\n  for (const k of drop) if (k in out) delete out[k];\n  return out;\n}\n\nasync function main() {\n  const model = s(base.identify_model) || s(base.ReportLLM) || s($env.IDENTIFY_MODEL) || 'qwen2.5:32b-instruct-q8_0';\n  const hints = s(base.identify_hints).trim();\n\n  const evidencePath = s(base.identify_evidence_path).trim() || s(base.combinedLogPath).trim();\n  const READ_CHARS = Number($env.IDENTIFY_READ_CHARS || 220000);\n  const EVID_MAX = Number($env.IDENTIFY_EVIDENCE_MAX_CHARS || 160000);\n\n  let evidence = safeReadHead(evidencePath, READ_CHARS);\n  evidence = maskPII(evidence);\n  if (evidence.length > EVID_MAX) evidence = evidence.slice(0, EVID_MAX);\n\n  const llm_input = `TASK: IDENTIFY HARDWARE (LLM1)\n\nReturn ONLY strict JSON (no markdown, no comments, no extra keys):\n\n{\n  \"vendor\": \"N/A|Dell EMC|HPE|IBM|Huawei|NetApp|Cisco|Lenovo|Other\",\n  \"class\": \"Storage|Server|Switch|UPS|Other\",\n  \"model\": \"string or N/A\",\n  \"confidence\": 0-100,\n  \"evidence\": [\"up to 5 short quotes from EVIDENCE below\"]\n}\n\nRULES:\n- Evidence-first: if not present in EVIDENCE -> use \"N/A\" and confidence <= 40\n- Do NOT greet, do NOT ask questions.\n\nEVIDENCE (filenames + hit-lines, masked):\n${hints}\n${evidence}\n`;\n\n  const payload = {\n    model,\n    stream: false,\n    messages: [{ role: 'user', content: llm_input }],\n    options: { temperature: 0, top_p: 1, top_k: 1 },\n  };\n\n  if (String(base.ollama_format || '').toLowerCase() === 'json') {\n    payload.format = 'json';\n  }\n\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url,\n    headers: { 'Content-Type': 'application/json' },\n    body: payload,\n    json: true,\n    timeout: 60 * 60 * 1000,\n  });\n\n  const txt = s(resp?.message?.content);\n\n  const out = { ...base };\n  out.ollama = {\n    model: resp?.model || model,\n    created_at: resp?.created_at,\n    done_reason: resp?.done_reason,\n  };\n  out.identify_raw = txt;\n  out.ollama_text = txt;\n\n  // useful diagnostics without large payload\n  out.identify_runtime = {\n    evidence_path: evidencePath,\n    evidence_chars_used: evidence.length,\n  };\n\n  prune(out);\n  return [{ json: out }];\n}\n\nreturn await main();\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        8464,
        -2816
      ],
      "id": "3e749eb1-c987-4cf8-99f9-fa44195e4416",
      "name": "LLM Request Identify"
    },
    {
      "parameters": {
        "jsCode": "// Ingest Attachments (Universal) — FULL REPLACE v3-universal-pick\n// Goal:\n// - pick \"main\" non-image attachment deterministically\n// - normalize archiveName/sourceFileId/sourceFileName\n// - do NOT download here (Universal Log Collector will download+unpack)\n// Output fields (contract):\n//   sourceFileId, sourceFileName, archiveName, archiveExt, _ingest_debug\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction isImageFile(f){\n  const kind = s(f?.kind).toLowerCase();\n  if (kind === 'image') return true;\n  const name = s(f?.name).toLowerCase();\n  return /\\.(png|jpg|jpeg|webp|bmp|tiff|gif)$/i.test(name);\n}\n\nfunction extLower(name){\n  const n = s(name).trim().toLowerCase();\n  const just = n.split(/[\\\\/]/).pop();\n  // keep multi-ext first\n  const multi = ['.tar.gz','.tgz','.tar.bz2','.tbz2','.tar.xz','.txz','.tar.zst'];\n  for (const m of multi) if (just.endsWith(m)) return m;\n  const m2 = just.match(/(\\.[a-z0-9]+)$/i);\n  return m2 ? m2[1].toLowerCase() : '';\n}\n\nfunction scoreAttachment(f){\n  const name = s(f?.name).toLowerCase();\n  const ext = extLower(name);\n\n  let sc = 0;\n\n  // archives (highest)\n  if (['.zip','.tar.gz','.tgz','.tar','.gz','.tbz2','.tar.bz2','.bz2','.xz','.tar.xz','.txz','.7z','.zst','.tar.zst'].includes(ext)) sc += 200;\n\n  // typical log/text payload\n  if (/\\.(log|txt|csv|xml|json|yml|yaml|cfg|conf|ini|out|err)$/i.test(name)) sc += 40;\n\n  // size weight\n  const size = Number(f?.size);\n  if (Number.isFinite(size) && size > 0) sc += Math.min(80, Math.floor(size / 10_000_000)); // +1 per 10MB, capped\n\n  // prefer \"disk/...\" file_id (original) vs sized images etc\n  const fid = s(f?.file_id || f?.id);\n  if (fid.startsWith('disk/')) sc += 10;\n\n  return sc;\n}\n\nconst base = $input.item.json || {};\nconst files = Array.isArray(base.files) ? base.files : [];\n\nconst nonImage = files.filter(f => !isImageFile(f));\nlet chosen = null;\n\n// 1) if Prepare prompt already set sourceFileId/sourceFileName — trust it\nconst preId = s(base.sourceFileId).trim();\nif (preId) {\n  chosen = nonImage.find(f => s(f.file_id || f.id).trim() === preId) || null;\n}\n\n// 2) else pick best-scored non-image\nif (!chosen && nonImage.length) {\n  chosen = nonImage\n    .slice()\n    .sort((a,b)=> scoreAttachment(b) - scoreAttachment(a))[0] || null;\n}\n\n// 3) fallback: any file\nif (!chosen && files.length) chosen = files[0];\n\nconst sourceFileId = chosen ? s(chosen.file_id || chosen.id).trim() : '';\nconst sourceFileName = chosen ? s(chosen.name).trim() : '';\n\nconst archiveName = sourceFileName || s(base.archiveName).trim() || 'attachment';\nconst archiveExt = extLower(archiveName);\n\nreturn [{\n  json: {\n    ...base,\n\n    sourceFileId,\n    sourceFileName,\n\n    archiveName,\n    archiveExt,\n\n    _ingest_debug: {\n      files_total: files.length,\n      non_image_total: nonImage.length,\n      chosen: chosen ? {\n        file_id: s(chosen.file_id || chosen.id),\n        name: s(chosen.name),\n        size: Number(chosen.size || 0) || 0,\n        ext: archiveExt,\n        score: scoreAttachment(chosen),\n      } : null\n    }\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        7504,
        -2816
      ],
      "id": "450b49c0-790a-41de-a17e-24de070f1a7e",
      "name": "Ingest Attachments (Universal)"
    },
    {
      "parameters": {
        "jsCode": "// RAG Identify Lookup (offline) — FULL REPLACE v2-B-FIX8\n// FIX: ...base and ...(TOKEN ? ...) spreads\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst RAG_URL = (String($env.RAG_API_URL || 'http://rag-api:8099')).replace(/\\/$/, '');\nconst TOKEN = String($env.RAG_API_TOKEN || '').trim();\nconst TOP_K = Number($env.RAG_IDENTIFY_TOP_K || 25);\n\nfunction guessVendor(text){\n  const t = (text||'').toUpperCase();\n  if (t.includes('DELL EMC') || t.includes('DELL')) return 'Dell EMC';\n  if (t.includes('HPE') || t.includes('HEWLETT')) return 'HPE';\n  if (t.includes('LENOVO')) return 'Lenovo';\n  if (t.includes('NETAPP')) return 'NetApp';\n  if (t.includes('CISCO')) return 'Cisco';\n  if (t.includes('BROCADE')) return 'Brocade';\n  if (t.includes('HUAWEI')) return 'Huawei';\n  if (t.includes('IBM')) return 'IBM';\n  return 'N/A';\n}\nfunction guessKlass(text){\n  const t = (text||'').toUpperCase();\n  if (/\\bME\\d{3,5}\\b/.test(t) || t.includes('DISK GROUP') || t.includes('ENCLOSURE')) return 'Storage';\n  return 'Other';\n}\nfunction clip(x, n){ x=s(x); return x.length<=n?x:x.slice(0,n); }\n\nconst evidence = s(base.identify_evidence_text);\nconst vendor = guessVendor(evidence);\nconst klass = guessKlass(evidence);\nconst query = clip((s(base.rag_query_seed).trim() || evidence.replace(/\\s+/g,' ').trim()), 380);\n\nif (!vendor || vendor === 'N/A') {\n  return [{\n    json: {\n      ...base,\n      identify_rag_best: null,\n      identify_rag_hints: '',\n      _rag_identify_error: 'vendor_missing',\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n    }\n  }];\n}\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${RAG_URL}/search/all`,\n    json: true,\n    timeout: 30000,\n    headers: {\n      ...(TOKEN ? { Authorization: TOKEN.startsWith('Bearer ') ? TOKEN : `Bearer ${TOKEN}` } : {}),\n      'Content-Type': 'application/json',\n    },\n    body: { query, vendor, klass, top_k: TOP_K },\n  });\n\n  const docs = Array.isArray(resp?.docs) ? resp.docs : (Array.isArray(resp?.hits?.docs) ? resp.hits.docs : []);\n  const cat  = Array.isArray(resp?.event_catalog) ? resp.event_catalog : (Array.isArray(resp?.hits?.event_catalog) ? resp.hits.event_catalog : []);\n\n  let best = null;\n  for (const h of cat) {\n    const m = s(h.model).trim();\n    const c = s(h.class || h.klass).trim();\n    if (!m && !c) continue;\n    best = {\n      vendor: s(h.vendor).trim() || vendor,\n      class: c || klass,\n      model: m || 'N/A',\n      product_name: s(h.product_name || '').trim() || 'N/A',\n      ref: s(h.rel_path || h.file_name || '').trim(),\n      page: h.page,\n      score: h.score,\n      phrase: s(h.phrase || '').trim(),\n    };\n    break;\n  }\n\n  if (!best) {\n    for (const d of docs) {\n      const ex = s(d.excerpt || d.text || '').toUpperCase();\n      const mm = ex.match(/\\bME\\d{3,5}\\b/);\n      if (mm) { best = { vendor, class: klass, model: mm[0], product_name:'N/A', ref: s(d.rel_path||d.file_name), page:d.page, score:d.score, phrase:'' }; break; }\n    }\n  }\n\n  const hints = [];\n  if (best) {\n    const ref = best.ref ? `ref=${best.ref}${best.page!=null?` p.${best.page}`:''}` : '';\n    hints.push(`BEST: vendor=\"${best.vendor}\" class=\"${best.class}\" model=\"${best.model}\" ${ref}`.trim());\n  }\n  let n=0;\n  for (const h of cat.slice(0, 8)) {\n    const ph = s(h.phrase).trim();\n    const m = s(h.model).trim();\n    const c = s(h.class||h.klass).trim();\n    const ref = s(h.rel_path||h.file_name).trim();\n    if (!ph && !m) continue;\n    hints.push(`- ${m||'N/A'} | ${c||'N/A'} | ${ph||'.'} | ${ref}${h.page!=null?` p.${h.page}`:''}`.trim());\n    if (++n>=8) break;\n  }\n\n  return [{\n    json: {\n      ...base,\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n      identify_rag_best: best,\n      identify_rag_hints: hints.join('\\n'),\n    }\n  }];\n\n} catch (e) {\n  return [{\n    json: {\n      ...base,\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n      identify_rag_best: null,\n      identify_rag_hints: '',\n      _rag_identify_error: s(e?.message || e),\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        8208,
        -2816
      ],
      "id": "b534a8b9-bd40-4ce2-b176-46040161c277",
      "name": "RAG Identify Lookup (offline)"
    },
    {
      "parameters": {
        "jsCode": "// LLM Request Storage Structure — FULL REPLACE v3-B-FIX5\n// FIX: \".base\" -> \"...base\" (catch), Qwen default, num_ctx=131072\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role: 'user', content: '' }];\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: {\n      model,\n      stream: false,\n      messages,\n      options: {\n        temperature: 0,\n        top_k: 1,\n        top_p: 1,\n        num_ctx: Number($env.STORAGE_NUM_CTX || 131072),\n        num_predict: Number($env.STORAGE_NUM_PREDICT || 3500),\n      },\n    },\n    timeout: 3600000,\n  });\n\n  const content =\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp);\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama: resp,\n      message: { role: 'assistant', content: String(content || '').trim() },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 2000) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role: 'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        10096,
        -3104
      ],
      "id": "6389488e-91ff-4aeb-a4be-a87db42cbf10",
      "name": "LLM Request Storage Structure"
    },
    {
      "parameters": {
        "jsCode": "/**\n * LLM Request Storage Disk — FULL REPLACE v3.1 (prune wave output)\n */\nconst base = $json || {};\nconst OLLAMA = String($env.OLLAMA_URL || 'http://127.0.0.1:11434').replace(/\\/+$/, '');\nconst url = `${OLLAMA}/api/chat`;\n\nfunction s(v, d='') { return (v===undefined || v===null) ? d : String(v); }\nfunction prune(out) {\n  const drop = ['messages','llm_input','chatInput','prompt','docs_context','event_catalog_context','logsCombined','combinedLogs','logDigest','logDigestText'];\n  for (const k of drop) if (k in out) delete out[k];\n  return out;\n}\n\nasync function main() {\n  const model = s(base.ReportLLM) || 'deepseek-logs:latest';\n  const content = s(base.llm_input) || s(base.prompt) || s(base.chatInput) || s(base.digest_disks_text) || 'N/A';\n\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url,\n    headers: { 'Content-Type': 'application/json' },\n    body: { model, stream:false, messages:[{role:'user', content}], options:{temperature:0, top_p:1, top_k:1} },\n    json: true,\n    timeout: 60 * 60 * 1000,\n  });\n\n  const out = { ...base };\n  out.ollama_disks = { model: resp?.model || model, done_reason: resp?.done_reason, message: resp?.message };\n  out.disks_raw = s(resp?.message?.content);\n\n  prune(out);\n  return [{ json: out }];\n}\nreturn await main();\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        10096,
        -2912
      ],
      "id": "0925279b-5c26-4b63-bd13-770fd1778dfe",
      "name": "LLM Request Storage Disk"
    },
    {
      "parameters": {
        "jsCode": "/**\n * LLM Request Storage Errors — FULL REPLACE v3.1 (prune wave output)\n */\nconst base = $json || {};\nconst OLLAMA = String($env.OLLAMA_URL || 'http://127.0.0.1:11434').replace(/\\/+$/, '');\nconst url = `${OLLAMA}/api/chat`;\n\nfunction s(v, d='') { return (v===undefined || v===null) ? d : String(v); }\nfunction prune(out) {\n  const drop = [\n    'messages','llm_input','chatInput','prompt',\n    'docs_context','event_catalog_context','logsCombined','combinedLogs','logDigest','logDigestText',\n    'error_candidates_text','errors_index_text'\n  ];\n  for (const k of drop) if (k in out) delete out[k];\n  return out;\n}\n\nasync function main() {\n  const model = s(base.ReportLLM) || 'deepseek-logs:latest';\n  const content = s(base.llm_input) || s(base.prompt) || s(base.chatInput) || s(base.digest_errors_text) || 'N/A';\n\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url,\n    headers: { 'Content-Type': 'application/json' },\n    body: { model, stream:false, messages:[{role:'user', content}], options:{temperature:0, top_p:1, top_k:1} },\n    json: true,\n    timeout: 60 * 60 * 1000,\n  });\n\n  const out = { ...base };\n  out.ollama_errors = { model: resp?.model || model, done_reason: resp?.done_reason, message: resp?.message };\n  out.errors_raw = s(resp?.message?.content);\n\n  prune(out);\n  return [{ json: out }];\n}\nreturn await main();\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        10240,
        -2528
      ],
      "id": "44c8c29e-0c07-4102-8688-92640cbb6c2f",
      "name": "LLM Request Storage Errors"
    },
    {
      "parameters": {
        "jsCode": "/**\n * LLM Request Storage Periphery — FULL REPLACE v3.1 (prune wave output)\n */\nconst base = $json || {};\nconst OLLAMA = String($env.OLLAMA_URL || 'http://127.0.0.1:11434').replace(/\\/+$/, '');\nconst url = `${OLLAMA}/api/chat`;\n\nfunction s(v, d='') { return (v===undefined || v===null) ? d : String(v); }\nfunction prune(out) {\n  const drop = ['messages','llm_input','chatInput','prompt','docs_context','event_catalog_context','logsCombined','combinedLogs','logDigest','logDigestText'];\n  for (const k of drop) if (k in out) delete out[k];\n  return out;\n}\n\nasync function main() {\n  const model = s(base.ReportLLM) || 'deepseek-logs:latest';\n  const content = s(base.llm_input) || s(base.prompt) || s(base.chatInput) || s(base.digest_periphery_text) || 'N/A';\n\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url,\n    headers: { 'Content-Type': 'application/json' },\n    body: { model, stream:false, messages:[{role:'user', content}], options:{temperature:0, top_p:1, top_k:1} },\n    json: true,\n    timeout: 60 * 60 * 1000,\n  });\n\n  const out = { ...base };\n  out.ollama_periphery = { model: resp?.model || model, done_reason: resp?.done_reason, message: resp?.message };\n  out.periphery_raw = s(resp?.message?.content);\n\n  prune(out);\n  return [{ json: out }];\n}\nreturn await main();\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        10096,
        -2720
      ],
      "id": "3f970781-061c-42e3-b081-23b31c17d189",
      "name": "LLM Request Storage Periphery"
    },
    {
      "parameters": {
        "jsCode": "// Universal Log Collector — FULL REPLACE v6.5 (disk-only handoff + compact debug)\n// - Uses $env.YANDEX_BOT_TOKEN_LOGPARSER, fallback to $env.YANDEX_BOT_TOKEN\n// - POST JSON + GET fallback, endpoint with and without trailing slash\n// - Normalizes file_id: strips ?size=..., keeps disk/... intact\n// - Writes combined + identify evidence to disk and passes only file paths between nodes\n// - Returns _collector_ok=false on any download/unpack failure (so workflow must STOP)\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst crypto = require('crypto');\nconst child_process = require('child_process');\nconst zlib = require('zlib');\n\nconst base = $input.item.json || {};\nconst env = $env || {};\n\nconst runId = `ulc_${Date.now()}_${crypto.randomBytes(4).toString('hex')}`;\n\nconst OUT_ROOT = String(env.ULC_OUT_ROOT || path.join(os.tmpdir(), 'logparser_combined'));\nconst TMP_ROOT = String(env.ULC_TMP_DIR || path.join(os.tmpdir(), 'logparser_ulc'));\nconst RUN_DIR = path.join(TMP_ROOT, runId);\nconst DL_DIR = path.join(RUN_DIR, 'download');\nconst EX_DIR = path.join(RUN_DIR, 'extract');\n\nconst MAX_DEPTH = Number(env.ULC_MAX_DEPTH || 10);\nconst MAX_FILES = Number(env.ULC_MAX_FILES || 4000);\nconst MAX_COMBINED_CHARS = Number(env.ULC_MAX_COMBINED_CHARS || 3500000);\nconst MAX_PER_FILE_CHARS = Number(env.ULC_MAX_PER_FILE_CHARS || 250000);\nconst MAX_FILE_BYTES = Number(env.ULC_MAX_FILE_BYTES || 80 * 1024 * 1024);\n\nconst IDENTIFY_HEAD_CHARS = Number(env.ULC_IDENTIFY_HEAD_CHARS || 160000);\nconst IDENTIFY_TAIL_CHARS = Number(env.ULC_IDENTIFY_TAIL_CHARS || 80000);\nconst DEBUG_MAX_TRIES = Number(env.ULC_DEBUG_MAX_TRIES || 12);\n\nfunction s(x) { return x == null ? '' : String(x); }\nfunction ensureDir(p) { fs.mkdirSync(p, { recursive: true }); }\nfunction safeName(name) {\n  const t = s(name).trim() || 'attachment.bin';\n  return t.replace(/[^\\w.\\-()+ ]+/g, '_').slice(0, 180);\n}\nfunction extLower(name) {\n  const n = s(name).toLowerCase();\n  const m = n.match(/(\\.[a-z0-9.]+)$/i);\n  return m ? m[1] : '';\n}\nfunction isProbablyBinary(buf) {\n  if (!buf || !buf.length) return true;\n  return buf.indexOf(0) !== -1;\n}\nfunction decodeSmart(buf) {\n  try { return buf.toString('utf8'); } catch {}\n  try { return buf.toString('latin1'); } catch {}\n  return '';\n}\n\n// ---- CRITICAL: normalize file_id (strip ?size=...) ----\nfunction normalizeFileId(raw) {\n  const t = s(raw).trim();\n  if (!t) return '';\n  // keep \"disk/...\" intact, just strip query if present\n  const q = t.indexOf('?');\n  return q >= 0 ? t.slice(0, q) : t;\n}\n\n\nfunction safeHeadText(buf, n = 160) {\n  const b = Buffer.isBuffer(buf) ? buf : Buffer.from(String(buf || ''), 'utf8');\n  return b.slice(0, n).toString('utf8').replace(/[^\\x09\\x0a\\x0d\\x20-\\x7e]+/g, '.');\n}\nfunction pushTry(debug, row) {\n  if (!debug || !Array.isArray(debug.getFileTries)) return;\n  const next = { ...row };\n  if (next.bodyHead) next.bodyHead = safeHeadText(next.bodyHead, 120);\n  debug.getFileTries.push(next);\n  if (debug.getFileTries.length > DEBUG_MAX_TRIES) debug.getFileTries.shift();\n}\n\nfunction httpRequestBuffer(urlString, opts = {}, bodyBuf = null) {\n  return new Promise((resolve, reject) => {\n    const req = https.request(urlString, opts, (res) => {\n      const chunks = [];\n      res.on('data', (d) => chunks.push(d));\n      res.on('end', () => resolve({\n        status: res.statusCode || 0,\n        headers: res.headers || {},\n        body: Buffer.concat(chunks),\n      }));\n    });\n    req.on('error', reject);\n    if (bodyBuf) req.write(bodyBuf);\n    req.end();\n  });\n}\n\n// ---- Yandex getFile (token fallback) ----\nasync function yandexGetFileBinary(fileId, debug) {\n  const tokenCandidates = [\n    { name: 'YANDEX_BOT_TOKEN_LOGPARSER', value: s(env.YANDEX_BOT_TOKEN_LOGPARSER).trim() },\n    { name: 'YANDEX_BOT_TOKEN', value: s(env.YANDEX_BOT_TOKEN).trim() },\n    { name: 'YANDEX_BOT_TOKEN_ITCOSTIK', value: s(env.YANDEX_BOT_TOKEN_ITCOSTIK).trim() },\n  ].filter((t, i, arr) => t.value && arr.findIndex(x => x.value === t.value) === i);\n\n  if (!tokenCandidates.length) {\n    throw new Error('Не настроен токен: заполните YANDEX_BOT_TOKEN_LOGPARSER / YANDEX_BOT_TOKEN / YANDEX_BOT_TOKEN_ITCOSTIK');\n  }\n  debug.tokenCandidates = tokenCandidates.map(t => t.name);\n\n  const fid = normalizeFileId(fileId);\n  debug.file_id_raw = s(fileId);\n  debug.file_id_norm = fid;\n\n  if (!fid) throw new Error('Empty file_id after normalize');\n\n  const endpoints = [\n    {\n      kind: 'files',\n      post: 'https://botapi.messenger.yandex.net/bot/v1/files/getFile/',\n      getBase: 'https://botapi.messenger.yandex.net/bot/v1/files/getFile',\n    },\n    {\n      kind: 'messages',\n      post: 'https://botapi.messenger.yandex.net/bot/v1/messages/getFile/',\n      getBase: 'https://botapi.messenger.yandex.net/bot/v1/messages/getFile',\n    },\n  ];\n\n  let lastErr = '';\n\n  const tryDirectBody = (resp, tokenName, epKind, stepTag) => {\n    if (!(resp.status >= 200 && resp.status < 300)) return null;\n    const ct = String(resp.headers?.['content-type'] || '').toLowerCase();\n    const body = resp.body || Buffer.alloc(0);\n    const looksJson = ct.includes('application/json') || (body.length && body[0] === 0x7b);\n    if (looksJson) {\n      let obj = null;\n      try { obj = JSON.parse(body.toString('utf8')); } catch {}\n      if (obj && (obj.ok === false || obj.error || obj.description || obj.message)) {\n        const errText = String(obj.description || obj.message || obj.error || 'json error');\n        lastErr = `${epKind} ${stepTag}: ${errText}`;\n        pushTry(debug, { step: `${stepTag}_json_error`, token: tokenName, endpoint: epKind, err: errText });\n        return null;\n      }\n      // files/getFile usually returns JSON with URL, handled separately\n      return null;\n    }\n    if (body.length > 0) return body;\n    lastErr = `${epKind} ${stepTag}: empty body`;\n    return null;\n  };\n\n  for (const tokenInfo of tokenCandidates) {\n    const token = tokenInfo.value;\n\n    for (const ep of endpoints) {\n      // 1) POST {file_id}\n      {\n        const payload = Buffer.from(JSON.stringify({ file_id: fid }), 'utf8');\n        let r1;\n        try {\n          r1 = await httpRequestBuffer(ep.post, {\n            method: 'POST',\n            headers: {\n              'Authorization': `OAuth ${token}`,\n              'Content-Type': 'application/json',\n              'Content-Length': payload.length,\n            },\n          }, payload);\n        } catch (e) {\n          lastErr = e?.message || String(e);\n          pushTry(debug, { step: 'post_exc', token: tokenInfo.name, endpoint: ep.kind, apiUrl: ep.post, err: lastErr });\n          r1 = null;\n        }\n\n        if (r1) {\n          const bodyText = r1.body ? r1.body.toString('utf8') : '';\n          pushTry(debug, {\n            step: 'post', token: tokenInfo.name, endpoint: ep.kind, apiUrl: ep.post, status: r1.status, bodyHead: bodyText,\n          });\n\n          const direct = tryDirectBody(r1, tokenInfo.name, ep.kind, 'post');\n          if (direct) return direct;\n\n          // files/getFile variant: JSON with pre-signed URL\n          if (ep.kind === 'files' && r1.status >= 200 && r1.status < 300) {\n            let obj = {};\n            try { obj = JSON.parse(bodyText || '{}'); } catch {}\n            const url = obj.url || obj.file_url || obj.download_url;\n            if (url) {\n              const r2 = await httpRequestBuffer(url, { method: 'GET' }, null);\n              pushTry(debug, { step: 'files_url_get', token: tokenInfo.name, endpoint: ep.kind, status: r2.status });\n              if (r2.status >= 200 && r2.status < 300 && (r2.body || Buffer.alloc(0)).length) return r2.body;\n              lastErr = `download failed: HTTP ${r2.status}`;\n            } else {\n              lastErr = `files post HTTP ${r1.status}: empty download url`;\n            }\n          } else if (!(r1.status >= 200 && r1.status < 300)) {\n            lastErr = `${ep.kind} post HTTP ${r1.status}`;\n          }\n        }\n      }\n\n      // 2) GET ?file_id=\n      {\n        const url = `${ep.getBase}${ep.getBase.endsWith('/') ? '' : '/'}?file_id=${encodeURIComponent(fid)}`;\n        let rG;\n        try {\n          rG = await httpRequestBuffer(url, {\n            method: 'GET',\n            headers: { 'Authorization': `OAuth ${token}` },\n          }, null);\n        } catch (e) {\n          lastErr = e?.message || String(e);\n          pushTry(debug, { step: 'get_exc', token: tokenInfo.name, endpoint: ep.kind, apiUrl: url, err: lastErr });\n          rG = null;\n        }\n\n        if (rG) {\n          const bodyText = rG.body ? rG.body.toString('utf8') : '';\n          pushTry(debug, {\n            step: 'getFile_get', token: tokenInfo.name, endpoint: ep.kind, apiUrl: url, status: rG.status, bodyHead: bodyText,\n          });\n\n          const direct = tryDirectBody(rG, tokenInfo.name, ep.kind, 'get');\n          if (direct) return direct;\n\n          if (ep.kind === 'files' && rG.status >= 200 && rG.status < 300) {\n            let obj = {};\n            try { obj = JSON.parse(bodyText || '{}'); } catch {}\n            const dl = obj.url || obj.file_url || obj.download_url;\n            if (dl) {\n              const r2 = await httpRequestBuffer(dl, { method: 'GET' }, null);\n              pushTry(debug, { step: 'files_url_get2', token: tokenInfo.name, endpoint: ep.kind, status: r2.status });\n              if (r2.status >= 200 && r2.status < 300 && (r2.body || Buffer.alloc(0)).length) return r2.body;\n              lastErr = `download failed: HTTP ${r2.status}`;\n            } else {\n              lastErr = `files get HTTP ${rG.status}: empty download url`;\n            }\n          } else if (!(rG.status >= 200 && rG.status < 300)) {\n            lastErr = `${ep.kind} get HTTP ${rG.status}`;\n          }\n        }\n      }\n    }\n  }\n\n  const triesSummary = (debug.getFileTries || [])\n    .map(t => `${t.token || 'n/a'}:${t.endpoint || 'n/a'}:${t.step}:${t.status || t.err || 'n/a'}`)\n    .slice(-12)\n    .join(' | ');\n\n  throw new Error(`Не удалось скачать архив из чата (file_id недоступен для текущих токенов). Пробовали: ${(debug.tokenCandidates || []).join(', ') || 'нет токенов'}. ${lastErr ? 'Последняя ошибка: ' + lastErr + '. ' : ''}${triesSummary ? 'Трейс: ' + triesSummary : ''}`.trim());\n}\n\n// extraction helpers\nfunction runCmd(cmd, args) {\n  child_process.execFileSync(cmd, args, { stdio: ['ignore', 'ignore', 'ignore'] });\n}\nfunction isZip(p) {\n  const b = fs.readFileSync(p);\n  return b.length > 4 && b[0] === 0x50 && b[1] === 0x4b; // PK\n}\nfunction isGzip(p) {\n  const b = fs.readFileSync(p);\n  return b.length > 2 && b[0] === 0x1f && b[1] === 0x8b;\n}\nfunction isTar(p) {\n  const b = fs.readFileSync(p);\n  if (b.length < 265) return false;\n  return b.slice(257, 262).toString('utf8') === 'ustar';\n}\nfunction extractOne(fileAbs, targetDir, warnings) {\n  const name = path.basename(fileAbs).toLowerCase();\n  const ext = extLower(name);\n  try {\n    if (ext === '.zip' || isZip(fileAbs)) { ensureDir(targetDir); runCmd('unzip', ['-o', '-qq', fileAbs, '-d', targetDir]); return true; }\n    if (ext === '.tgz' || ext === '.tar.gz' || (isGzip(fileAbs) && name.includes('.tar'))) { ensureDir(targetDir); runCmd('tar', ['-xzf', fileAbs, '-C', targetDir]); return true; }\n    if (ext === '.tar' || isTar(fileAbs)) { ensureDir(targetDir); runCmd('tar', ['-xf', fileAbs, '-C', targetDir]); return true; }\n    if (ext === '.gz' || isGzip(fileAbs)) {\n      ensureDir(targetDir);\n      const outName = path.basename(fileAbs).replace(/\\.gz$/i, '') || 'gunzipped';\n      const outPath = path.join(targetDir, outName);\n      fs.writeFileSync(outPath, zlib.gunzipSync(fs.readFileSync(fileAbs)));\n      return true;\n    }\n  } catch (e) {\n    warnings.push(`extract failed: ${path.basename(fileAbs)}: ${e?.message || String(e)}`);\n    return false;\n  }\n  return false;\n}\nfunction listFilesRecursive(rootDir, maxFiles) {\n  const out = [];\n  const q = [rootDir];\n  while (q.length) {\n    const d = q.shift();\n    let items = [];\n    try { items = fs.readdirSync(d, { withFileTypes: true }); } catch { continue; }\n    for (const it of items) {\n      const abs = path.join(d, it.name);\n      if (it.isDirectory()) q.push(abs);\n      else out.push(abs);\n      if (out.length >= maxFiles) return out;\n    }\n  }\n  return out;\n}\nfunction buildCombinedText(filesAbs, rootAbs, warnings) {\n  let combined = '';\n  let combinedChars = 0;\n\n  function addBlock(header, text) {\n    const block = `${header}\\n${text}\\n\\n`;\n    if (combinedChars + block.length > MAX_COMBINED_CHARS) {\n      const remain = Math.max(0, MAX_COMBINED_CHARS - combinedChars);\n      if (remain > 0) combined += block.slice(0, remain) + '\\n[COMBINED TRUNCATED]\\n';\n      return false;\n    }\n    combined += block;\n    combinedChars += block.length;\n    return true;\n  }\n\n  for (const abs of filesAbs) {\n    if (combinedChars >= MAX_COMBINED_CHARS) break;\n\n    let st; try { st = fs.statSync(abs); } catch { continue; }\n    if (!st.isFile() || st.size <= 0) continue;\n    if (st.size > MAX_FILE_BYTES) { warnings.push(`skip huge file: ${abs} (${st.size} bytes)`); continue; }\n\n    let buf; try { buf = fs.readFileSync(abs); } catch { continue; }\n    if (isProbablyBinary(buf)) continue;\n\n    let text = decodeSmart(buf);\n    if (!text.trim()) continue;\n\n    if (text.length > MAX_PER_FILE_CHARS) text = text.slice(0, MAX_PER_FILE_CHARS) + '\\n[FILE TRUNCATED]\\n';\n\n    const rel = path.relative(rootAbs, abs);\n    if (!addBlock(`===== FILE: ${rel} (size=${st.size}) =====`, text)) break;\n  }\n\n  return combined;\n}\nfunction buildIdentifyEvidence(combinedText) {\n  const t = s(combinedText);\n  if (!t) return '';\n  const head = t.slice(0, IDENTIFY_HEAD_CHARS);\n  const tail = t.length > IDENTIFY_TAIL_CHARS ? t.slice(-IDENTIFY_TAIL_CHARS) : '';\n  return [\n    '=== IDENTIFY EVIDENCE (HEAD) ===',\n    head,\n    '',\n    '=== IDENTIFY EVIDENCE (TAIL) ===',\n    tail,\n  ].join('\\n');\n}\n\nasync function main() {\n  const debug = { runId, runDir: RUN_DIR, warnings: [], extractedCount: 0, extractedSample: [], getFileTries: [] };\n\n  ensureDir(DL_DIR);\n  ensureDir(EX_DIR);\n  ensureDir(OUT_ROOT);\n\n  let files = base.files;\n  if (typeof files === 'string') { try { files = JSON.parse(files); } catch { files = null; } }\n  if (!Array.isArray(files)) files = [];\n\n  const nonImage = files.filter(f => String(f?.kind || '').toLowerCase() !== 'image');\n  if (!nonImage.length) {\n    throw new Error('Во входящем сообщении нет не-графических вложений для разбора.');\n  }\n\n  // choose largest attachment\n  const pick = nonImage.slice().sort((a, b) => Number(b.size || 0) - Number(a.size || 0))[0] || nonImage[0];\n\n  const sourceFileIdRaw = s(pick.file_id || pick.id);\n  const sourceFileId = normalizeFileId(sourceFileIdRaw);\n  const sourceFileName = safeName(pick.name || 'attachment.bin');\n\n  if (!sourceFileId) {\n    throw new Error('Не удалось определить file_id вложения. Переотправьте архив одним файлом.');\n  }\n\n  const dlPath = path.join(DL_DIR, sourceFileName);\n\n  let bin;\n  try {\n    bin = await yandexGetFileBinary(sourceFileId, debug);\n  } catch (e) {\n    throw new Error(`Не удалось скачать архив: ${e?.message || String(e)}`);\n  }\n\n  fs.writeFileSync(dlPath, bin);\n\n  // BFS extraction (nested)\n  const q = [{ abs: dlPath, depth: 0, parent: EX_DIR }];\n  const visited = new Set();\n  const extractedRoots = new Set();\n\n  while (q.length) {\n    const cur = q.shift();\n    if (!cur || !cur.abs) continue;\n    if (visited.has(cur.abs)) continue;\n    visited.add(cur.abs);\n    if (cur.depth > MAX_DEPTH) continue;\n\n    const target = path.join(cur.parent, safeName(path.basename(cur.abs)) + '_dir');\n    const ok = extractOne(cur.abs, target, debug.warnings);\n    if (ok) {\n      extractedRoots.add(target);\n      const filesAbs = listFilesRecursive(target, MAX_FILES);\n      debug.extractedCount += 1;\n      for (const f of filesAbs) if (cur.depth + 1 <= MAX_DEPTH) q.push({ abs: f, depth: cur.depth + 1, parent: target });\n    }\n  }\n\n  let allFiles = [];\n  if (extractedRoots.size) for (const r of extractedRoots) allFiles = allFiles.concat(listFilesRecursive(r, MAX_FILES));\n  else allFiles = [dlPath];\n\n  const combinedText = buildCombinedText(allFiles, extractedRoots.size ? Array.from(extractedRoots)[0] : DL_DIR, debug.warnings);\n\n  const outDir = path.join(OUT_ROOT, crypto.createHash('sha1').update(runId).digest('hex').slice(0, 16));\n  ensureDir(outDir);\n\n  const archiveBase = safeName((base.archiveName || base.sourceFileName || sourceFileName).replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z)$/i, '')) || 'combined';\n  const combinedName = `${archiveBase}.txt`;\n  const combinedPath = path.join(outDir, combinedName);\n\n  fs.writeFileSync(combinedPath, combinedText, 'utf8');\n  const combinedBytes = fs.statSync(combinedPath).size;\n\n  const identifyEvidence = buildIdentifyEvidence(combinedText);\n  const identifyEvidenceName = `${archiveBase}__identify_evidence.txt`;\n  const identifyEvidencePath = path.join(outDir, identifyEvidenceName);\n  fs.writeFileSync(identifyEvidencePath, identifyEvidence, 'utf8');\n\n  return [{\n    json: {\n      ...base,\n      _collector_ok: true,\n      _collector_error: '',\n      _collector_sourceFileId: sourceFileId,\n      _collector_sourceFileName: sourceFileName,\n      _collector_debug: {\n        runId: debug.runId,\n      },\n\n      combinedLogPath: combinedPath,\n      combinedLogName: combinedName,\n      combinedLogBytesWritten: combinedBytes,\n\n      // IMPORTANT: NO logsCombined / combinedLogs here\n      identify_evidence_path: identifyEvidencePath,\n      identify_evidence_name: identifyEvidenceName,\n      identify_evidence_text: '',\n      identify_evidence_len: identifyEvidence.length,\n    }\n  }];\n}\n\nreturn await main();\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        7744,
        -2816
      ],
      "id": "b5f80c02-5647-4bb7-ab8a-2c7ba948ece2",
      "name": "Universal Log Collector"
    }
  ],
  "pinData": {
    "Webhook": [
      {
        "json": {
          "headers": {
            "connection": "upgrade",
            "host": "n8n.itcost.ru",
            "x-real-ip": "10.10.200.1",
            "x-forwarded-for": "10.10.200.1",
            "x-forwarded-proto": "https",
            "content-length": "375",
            "user-agent": "Mozilla/5.0 (compatible; YandexUserproxy; robot; +http://yandex.com/bots)",
            "accept-encoding": "gzip, x-gzip, deflate",
            "content-type": "application/json; charset=UTF-8"
          },
          "params": {},
          "query": {},
          "body": {
            "updates": [
              {
                "message_id": 1763730909592004,
                "timestamp": 1763730909,
                "chat": {
                  "type": "private"
                },
                "from": {
                  "id": "360eed3e-84b1-40cd-8af5-f9042df164ad",
                  "display_name": "Кузин Евгений",
                  "login": "kuzin@itcost.ru",
                  "robot": false
                },
                "update_id": 1763730909592004,
                "text": "Как сам?"
              }
            ]
          },
          "webhookUrl": "http://localhost:5678/webhook/Assistbot",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Extract message",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract message": {
      "main": [
        [
          {
            "node": "Data message",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare reply": {
      "main": [
        [
          {
            "node": "If file",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Data message": {
      "main": [
        [
          {
            "node": "Read messages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Read messages": {
      "main": [
        [
          {
            "node": "Prepare prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If  leader": {
      "main": [
        [
          {
            "node": "Update row(s)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Ingest Attachments (Universal)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Stop",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If file": {
      "main": [
        [
          {
            "node": "Send file",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Send Text",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "IF klass == \"Storage\"": {
      "main": [
        [
          {
            "node": "Storage Structure Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Disks Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Periphery Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Error Candidates",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Structure Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Structure input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Disks Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Disk input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Periphery Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Periphery input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Error Candidates": {
      "main": [
        [
          {
            "node": "RAG Retrieve Storage (offline)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Assemble Storage Report": {
      "main": [
        [
          {
            "node": "Prepare reply",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Structure input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Structure",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Disk input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Disk",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Periphery input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Periphery",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Retrieve Storage (offline)": {
      "main": [
        [
          {
            "node": "RAG Target Matcher",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Target Matcher": {
      "main": [
        [
          {
            "node": "Compose input ERRORS",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose input ERRORS": {
      "main": [
        [
          {
            "node": "LLM Request Storage Errors",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge 3": {
      "main": [
        [
          {
            "node": "Assemble Storage Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare prompt": {
      "main": [
        [
          {
            "node": "If  leader",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Identify Pack": {
      "main": [
        [
          {
            "node": "RAG Identify Lookup (offline)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Identify Result": {
      "main": [
        [
          {
            "node": "IF klass == \"Storage\"",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Identify": {
      "main": [
        [
          {
            "node": "Parse Identify Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ingest Attachments (Universal)": {
      "main": [
        [
          {
            "node": "Universal Log Collector",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Identify Lookup (offline)": {
      "main": [
        [
          {
            "node": "LLM Request Identify",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Storage Structure": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Storage Disk": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "LLM Request Storage Errors": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 3
          }
        ]
      ]
    },
    "LLM Request Storage Periphery": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "Universal Log Collector": {
      "main": [
        [
          {
            "node": "Build Identify Pack",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "timezone": "Europe/Moscow",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": false
  },
  "versionId": "8a5258ea-828c-4786-8dfd-6779fc7346c2",
  "meta": {
    "instanceId": "b7aa380705656969fa39759e0868f4a2cdd733e71248db37848cd014e207c2bc"
  },
  "id": "D2kDm2q0ZjabLod1",
  "tags": []
}