{
  "name": "LOGparser",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "LOGparser",
        "options": {
          "noResponseBody": true
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -1232,
        32
      ],
      "id": "19a80f9c-b29c-4206-9aef-3270c6a49a34",
      "name": "Webhook",
      "webhookId": "62bcabb1-4964-4f50-9488-179a800bc3fb"
    },
    {
      "parameters": {
        "jsCode": "// Extract message — FULL REPLACE v3 (adds audio detection + hasAudio)\n// Input: $json.body.updates[0]\n// Output: { text, chatId, login, hasFile, hasAudio, files[] }\n\nfunction normalizeText(s) {\n  return String(s || '').replace(/\\u00A0/g, ' ').trim();\n}\n\nfunction toArrayMaybe(x) {\n  if (!x) return [];\n  if (Array.isArray(x)) return x;\n  return [x];\n}\n\nfunction uniqPushFile(arr, f) {\n  const kind = String(f.kind || '');\n  const id = String(f.file_id || f.id || '');\n  const key = `${kind}|${id}`;\n  if (!id) return;\n  if (arr._seen && arr._seen.has(key)) return;\n  arr._seen = arr._seen || new Set();\n  arr._seen.add(key);\n  arr.push(f);\n}\n\nfunction guessKindByName(name) {\n  const n = String(name || '').toLowerCase();\n  if (/\\.(png|jpg|jpeg|bmp|gif|tif|tiff|webp)$/i.test(n)) return 'image';\n  if (/\\.(ogg|opus|mp3|wav|m4a|aac|flac)$/i.test(n)) return 'audio';\n  return 'file';\n}\n\nconst update = $json?.body?.updates?.[0];\nif (!update) return [];\n\nconst text = normalizeText(update.text || '') || 'Empty';\n\n// chatId/login\nconst chatId = update?.from?.id ?? update?.chat?.id ?? null;\nconst login = update?.from?.login ?? null;\n\nconst files = [];\n\n// images can come in different shapes: images[0] array, images array, or object\nconst imagesRaw = update.images;\nif (imagesRaw) {\n  // sometimes images is [[...]] or [...]\n  const imgs = Array.isArray(imagesRaw) && Array.isArray(imagesRaw[0]) ? imagesRaw[0] : imagesRaw;\n  for (const src of toArrayMaybe(imgs)) {\n    if (!src) continue;\n    const file_id = src.file_id || src.id;\n    uniqPushFile(files, {\n      kind: 'image',\n      file_id,\n      name: src.name || '',\n      size: src.size,\n      width: src.width,\n      height: src.height,\n    });\n  }\n}\n\n// generic file (docs, logs, voice, etc)\nif (update.file) {\n  const f = update.file;\n  const file_id = f.file_id || f.id;\n  const name = f.name || '';\n  const kind = guessKindByName(name);\n  uniqPushFile(files, {\n    kind,\n    file_id,\n    name,\n    size: f.size,\n  });\n}\n\nconst hasFile = files.length > 0;\nconst hasAudio = files.some(f => String(f.kind).toLowerCase() === 'audio');\n\nreturn [{\n  json: {\n    text,\n    chatId,\n    login,\n    hasFile,\n    hasAudio,\n    files,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1008,
        32
      ],
      "id": "a1946d73-02fc-4494-86ae-4450ec5df51c",
      "name": "Extract message"
    },
    {
      "parameters": {
        "jsCode": "// Prepare reply — FULL REPLACE v2-B (LOGparser)\n// - Always returns TXT file with final report\n// - File name: \"<archiveBase>.txt\" (fallback logparser_report.txt)\n// - Preserves chatId/login; sendText splitting handled by downstream If file + Send Text/Send file nodes\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction normalizeChatId(chatId) {\n  if (!chatId) return null;\n  const t = String(chatId).trim();\n  return t.includes('/') ? t : `0/0/${t}`;\n}\n\nfunction extractAssistantText(j) {\n  if (!j || typeof j !== 'object') return '';\n  if (typeof j.message?.content === 'string') return j.message.content;\n  if (typeof j.ollama?.message?.content === 'string') return j.ollama.message.content;\n  if (typeof j.response === 'string') return j.response;\n  if (typeof j.text === 'string') return j.text;\n  return '';\n}\n\nfunction stripThink(raw) {\n  return s(raw).replace(/<think>[\\s\\S]*?<\\/think>/gi, '').trim();\n}\n\nfunction baseName(name) {\n  const t = s(name).trim();\n  if (!t) return 'logparser_report';\n  const just = t.split(/[\\\\/]/).pop();\n  return just.replace(/\\.(zip|tgz|tar\\.gz|gz|tar|7z)$/i, '');\n}\n\nconst base = $input.item.json || {};\nconst chatId = normalizeChatId(base.chatId || base.chat_id);\nconst login = base.login || base.user || null;\n\nconst content = stripThink(extractAssistantText(base)).trim() || stripThink(s(base.message?.content)).trim();\nconst fileBase = baseName(base.sourceFileName || base.archiveName || base.fileName || (Array.isArray(base.files) && base.files[0] && base.files[0].name) || '');\nconst outName = `${fileBase}.txt`;\n\n// Add UTF-8 BOM (Excel-friendly if needed)\nconst bom = Buffer.from([0xEF,0xBB,0xBF]);\nconst buf = Buffer.concat([bom, Buffer.from(content || 'N/A', 'utf8')]);\n\nreturn [{\n  json: {\n    ...base,\n    chatId,\n    login,\n    fileName: outName,\n    isFile: true,\n  },\n  binary: {\n    file: {\n      data: buf.toString('base64'),\n      fileName: outName,\n      mimeType: 'text/plain; charset=utf-8',\n    }\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        4032,
        32
      ],
      "id": "fe14cc88-133c-4d05-b292-e2027bd6bf2f",
      "name": "Prepare reply"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "processed": false,
            "chatId": "={{$json.chatId}}",
            "timestamp": "={{Date.now()}}",
            "text": "={{$json.text}}",
            "files": "={{ JSON.stringify($json.files || []) }}",
            "login": "={{$json.login}}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "chatId",
              "displayName": "chatId",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "timestamp",
              "displayName": "timestamp",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "number",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "text",
              "displayName": "text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "processed",
              "displayName": "processed",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "boolean",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "files",
              "displayName": "files",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "login",
              "displayName": "login",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {
          "optimizeBulk": false
        }
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        -784,
        32
      ],
      "id": "2eb5f0c3-157b-449a-90ba-2470421d03b0",
      "name": "Data message"
    },
    {
      "parameters": {
        "operation": "get",
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "matchType": "allConditions",
        "filters": {
          "conditions": [
            {
              "keyName": "chatId",
              "keyValue": "={{$json.chatId}}"
            },
            {
              "keyName": "processed",
              "condition": "isFalse"
            }
          ]
        },
        "returnAll": true
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        -560,
        32
      ],
      "id": "33fb5954-9d66-464d-abd6-21a128c9489e",
      "name": "Read messages"
    },
    {
      "parameters": {
        "operation": "update",
        "dataTableId": {
          "__rl": true,
          "value": "64Y6oJPQRzpKmBIv",
          "mode": "list",
          "cachedResultName": "Yandex_messages",
          "cachedResultUrl": "/projects/NzQv03hxe8w3kPzF/datatables/64Y6oJPQRzpKmBIv"
        },
        "matchType": "allConditions",
        "filters": {
          "conditions": [
            {
              "keyName": "chatId",
              "keyValue": "={{$json.chatId}}"
            },
            {
              "keyName": "processed",
              "condition": "isFalse"
            }
          ]
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "processed": true
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "chatId",
              "displayName": "chatId",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "timestamp",
              "displayName": "timestamp",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "number",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "text",
              "displayName": "text",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "processed",
              "displayName": "processed",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "boolean",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "files",
              "displayName": "files",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "login",
              "displayName": "login",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        320,
        -160
      ],
      "id": "50f7f041-44d9-4e8f-b222-68017e03ac0a",
      "name": "Update row(s)",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "9762fa70-8640-4f1b-9966-3e747cacf98f",
              "leftValue": "={{$json[\"isLeader\"]}}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        -112,
        32
      ],
      "id": "fb3377b5-cb65-449e-aff9-fe4c0feef9cd",
      "name": "If  leader"
    },
    {
      "parameters": {
        "jsCode": "// Code node \"Stop\"\n// Ничего не делаем и останавливаем цепочку для этих сообщений\nreturn [];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        320,
        224
      ],
      "id": "cc5262ad-1d46-4a3e-a58f-c0a4947517ce",
      "name": "Stop"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://botapi.messenger.yandex.net/bot/v1/messages/sendText/",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "OAuth y0__xDRiujSCBiWkRMgj8CkjBZS9XWHl_U3FI2k7-E763DNGiO34w"
            }
          ]
        },
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "login",
              "value": "={{$json.login}}"
            },
            {
              "name": "text",
              "value": "={{$json.text}}"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        4480,
        128
      ],
      "id": "6b3f54b7-860c-4531-8bc7-ff570d1e0cd4",
      "name": "Send Text"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://botapi.messenger.yandex.net/bot/v1/messages/sendFile/",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "OAuth y0__xDRiujSCBiWkRMgj8CkjBZS9XWHl_U3FI2k7-E763DNGiO34w"
            }
          ]
        },
        "sendBody": true,
        "contentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "name": "login",
              "value": "={{$json.login}}"
            },
            {
              "parameterType": "formBinaryData",
              "name": "document",
              "inputDataFieldName": "file"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.3,
      "position": [
        4480,
        -64
      ],
      "id": "20c625f3-68f2-4e2c-b1ef-4d319bce2c70",
      "name": "Send file"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "15527451-ad55-4b9e-a7e4-5db6dfdfe946",
              "leftValue": "={{ !!$binary.file }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        4256,
        32
      ],
      "id": "5c535264-ccb2-4898-8313-6a8a3064d294",
      "name": "If file"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "59471c9d-4615-4b45-9c46-3e20c1b91d9f",
              "leftValue": "={{ String($json.klass || '').trim().toLowerCase() === 'storage' }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        1520,
        32
      ],
      "id": "aa05ec26-0403-4f69-b457-e604e38edf2d",
      "name": "IF klass == \"Storage\""
    },
    {
      "parameters": {
        "jsCode": "// Storage Structure Extract — FULL REPLACE v2-B-FIX+META\n// WF1171 logic + ADD: pass-through hw_vendor/hw_class/hw_model from LLM1 to this node output\n// No other logic changes.\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst { execFileSync } = require('child_process');\nconst zlib = require('zlib');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst MAX_CHARS = Number($env.STRUCTURE_EXTRACT_MAX_CHARS || 900000);\n\nfunction getToken() {\n  const t2 = s($env?.YANDEX_BOT_TOKEN_LOGPARSER).trim();\n  if (t2) return t2;\n  return s($env?.YANDEX_BOT_TOKEN).trim();\n}\n\nfunction safeHead(buf, n=200){\n  const b = Buffer.isBuffer(buf) ? buf : Buffer.from(String(buf||''), 'utf8');\n  return b.slice(0, n).toString('utf8').replace(/[^\\x09\\x0a\\x0d\\x20-\\x7e]+/g,'.');\n}\n\nfunction looksLikeJson(headers, buf){\n  const ct = s(headers?.['content-type']).toLowerCase();\n  if (ct.includes('application/json')) return true;\n  const h = safeHead(buf, 60).trim();\n  return h.startsWith('{') || h.startsWith('[');\n}\n\nfunction extractYandexJsonError(buf){\n  try {\n    const obj = JSON.parse(buf.toString('utf8'));\n    if (obj && obj.ok === false) return obj.description || obj.code || 'getFile failed';\n  } catch {}\n  return null;\n}\n\nfunction httpRequestBuffer({ method, hostname, path: p, headers }, bodyBuf){\n  return new Promise((resolve, reject) => {\n    const req = https.request({ method, hostname, path: p, headers }, (res) => {\n      const chunks = [];\n      res.on('data', (d) => chunks.push(Buffer.isBuffer(d) ? d : Buffer.from(d)));\n      res.on('end', () => resolve({ statusCode: (res.statusCode|0), headers: res.headers||{}, buf: Buffer.concat(chunks) }));\n    });\n    req.on('error', reject);\n    if (bodyBuf && bodyBuf.length) req.write(bodyBuf);\n    req.end();\n  });\n}\n\nasync function yandexGetFileBuffer(fileId){\n  const token = getToken();\n  if (!token) throw new Error('YANDEX_BOT_TOKEN_LOGPARSER/YANDEX_BOT_TOKEN is not set');\n  if (!fileId) throw new Error('file_id is empty');\n\n  const body = Buffer.from(JSON.stringify({ file_id: fileId }), 'utf8');\n  const r1 = await httpRequestBuffer(\n    {\n      method:'POST',\n      hostname:'botapi.messenger.yandex.net',\n      path:'/bot/v1/messages/getFile/',\n      headers: { 'Authorization': `OAuth ${token}`, 'Content-Type':'application/json', 'Content-Length': String(body.length) }\n    },\n    body\n  );\n\n  if (looksLikeJson(r1.headers, r1.buf)) {\n    const err = extractYandexJsonError(r1.buf);\n    if (err) throw new Error(err);\n  }\n  if (r1.statusCode >= 200 && r1.statusCode < 300) return r1.buf;\n\n  const p = `/bot/v1/messages/getFile/?file_id=${encodeURIComponent(fileId)}`;\n  const r2 = await httpRequestBuffer(\n    { method:'GET', hostname:'botapi.messenger.yandex.net', path:p, headers:{ 'Authorization': `OAuth ${token}` } },\n    null\n  );\n\n  if (looksLikeJson(r2.headers, r2.buf)) {\n    const err = extractYandexJsonError(r2.buf);\n    if (err) throw new Error(err);\n  }\n  if (r2.statusCode >= 200 && r2.statusCode < 300) return r2.buf;\n\n  throw new Error(`getFile failed: POST=${r1.statusCode} head=${safeHead(r1.buf)} | GET=${r2.statusCode} head=${safeHead(r2.buf)}`);\n}\n\nfunction sniffTypeByMagic(buf, nameLower){\n  if (buf.length >= 4 && buf[0]===0x50 && buf[1]===0x4b && buf[2]===0x03 && buf[3]===0x04) return 'zip';\n  if (buf.length >= 2 && buf[0]===0x1f && buf[1]===0x8b) {\n    if (nameLower.endsWith('.tar.gz') || nameLower.endsWith('.tgz')) return 'targz';\n    return 'gz';\n  }\n  if (buf.length >= 262 && buf.slice(257,262).toString('utf8') === 'ustar') return 'tar';\n  return 'plain';\n}\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive:true }); }\n\nfunction decodeSmart(buf){\n  if (!Buffer.isBuffer(buf)) buf = Buffer.from(String(buf||''), 'utf8');\n  if (buf.length >= 2 && buf[0]===0xff && buf[1]===0xfe) return buf.slice(2).toString('utf16le');\n  if (buf.length >= 2 && buf[0]===0xfe && buf[1]===0xff) {\n    const swapped = Buffer.allocUnsafe(buf.length-2);\n    for (let i=2;i<buf.length;i+=2){ swapped[i-2]=buf[i+1]||0; swapped[i-1]=buf[i]||0; }\n    return swapped.toString('utf16le');\n  }\n  const nul = buf.slice(0, Math.min(buf.length, 20000)).includes(0);\n  if (nul) return '';\n  return buf.toString('utf8');\n}\n\nfunction walkFiles(root){\n  const out = [];\n  const stack = [root];\n  while (stack.length) {\n    const p = stack.pop();\n    const st = fs.statSync(p);\n    if (st.isDirectory()) for (const name of fs.readdirSync(p)) stack.push(path.join(p, name));\n    else if (st.isFile()) out.push(p);\n  }\n  return out;\n}\n\nfunction pickPrimaryLog(files){\n  let best=null, bestScore=-1;\n  for (const fp of files) {\n    const name = path.basename(fp).toLowerCase();\n    const rel = fp.toLowerCase();\n    let sc = 0;\n    if (/^store_.*\\.logs$/.test(name)) sc += 100;\n    if (name.endsWith('.logs')) sc += 20;\n    if (name.endsWith('.log') || name.endsWith('.txt')) sc += 5;\n    if (rel.includes('store')) sc += 10;\n    if (rel.includes('messages')) sc -= 5;\n    const st = fs.statSync(fp);\n    sc += Math.min(10, Math.floor(st.size / 50_000_000));\n    if (sc > bestScore) { bestScore=sc; best=fp; }\n  }\n  return best;\n}\n\nfunction extractCliBlock(text, marker, maxLines=3500){\n  const lines = text.split(/\\r?\\n/);\n  let i=-1;\n  for (let k=0;k<lines.length;k++){ if (lines[k].trim() === marker) { i=k; break; } }\n  if (i<0) return '';\n  const out=[];\n  for (let k=i; k<lines.length && out.length<maxLines; k++){\n    const ln = lines[k];\n    if (k>i && ln.startsWith('# ') && ln.trim() !== marker) break;\n    out.push(ln);\n  }\n  return out.join('\\n').trim();\n}\n\nconst fileId = s(base.sourceFileId).trim();\nconst fileName = s(base.sourceFileName).trim() || 'attachment.bin';\nif (!fileId) throw new Error('sourceFileId is empty');\n\nconst buf = await yandexGetFileBuffer(fileId);\nconst type = sniffTypeByMagic(buf, fileName.toLowerCase());\n\nconst tmp = fs.mkdtempSync(path.join(os.tmpdir(), 'st-struct-'));\nconst outDir = path.join(tmp, 'out');\nensureDir(outDir);\n\ntry {\n  if (type === 'zip' || type === 'tar' || type === 'targz') {\n    const inFile = path.join(tmp, 'in' + (type==='zip'?'.zip': type==='tar'?'.tar':'.tgz'));\n    fs.writeFileSync(inFile, buf);\n    if (type === 'zip') execFileSync('unzip', ['-qq', inFile, '-d', outDir]);\n    if (type === 'tar') execFileSync('tar', ['-xf', inFile, '-C', outDir]);\n    if (type === 'targz') execFileSync('tar', ['-xzf', inFile, '-C', outDir]);\n  } else if (type === 'gz') {\n    const gunz = zlib.gunzipSync(buf);\n    fs.writeFileSync(path.join(outDir, path.basename(fileName.replace(/\\.gz$/i,'')) || 'file.txt'), gunz);\n  } else {\n    fs.writeFileSync(path.join(outDir, path.basename(fileName) || 'file.txt'), buf);\n  }\n\n  const files = walkFiles(outDir);\n  const primary = pickPrimaryLog(files) || files[0];\n  let text = '';\n  if (primary) text = decodeSmart(fs.readFileSync(primary));\n\n  const b1 = extractCliBlock(text, '# show enclosures', 5000);\n  const b2 = extractCliBlock(text, '# show redundancy-mode', 1200);\n  const b3 = extractCliBlock(text, '# show versions detail', 1800);\n\n  let digest = [\n    b1 ? '=== # show enclosures ===\\n' + b1 : '',\n    b2 ? '=== # show redundancy-mode ===\\n' + b2 : '',\n    b3 ? '=== # show versions detail ===\\n' + b3 : '',\n  ].filter(Boolean).join('\\n\\n');\n\n  if (!digest) digest = '[n8n] No structure blocks found.';\n  if (digest.length > MAX_CHARS) digest = digest.slice(0, MAX_CHARS) + '\\n[n8n] TRUNCATED';\n\n  // >>> PASS-THROUGH FROM LLM1 (only these 3 fields) <<<\n  const hw_vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\n  const hw_class  = s(base.hw_class  || base.klass || base.class || '').trim() || 'N/A';\n  const hw_model  = s(base.hw_model  || base.modelDetected || base.model || '').trim() || 'N/A';\n\n  return [{\n    json: {\n      ...base,\n\n      // canonical\n      hw_vendor,\n      hw_class,\n      hw_model,\n\n      // legacy (so report builder always sees them)\n      vendor: hw_vendor,\n      klass: hw_class,\n      modelDetected: hw_model,\n\n      digest_structure_text: digest,\n    }\n  }];\n\n} finally {\n  try { fs.rmSync(tmp, { recursive:true, force:true }); } catch {}\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1824,
        -256
      ],
      "id": "784c41be-d482-4e81-ad4b-3a6fc4f89574",
      "name": "Storage Structure Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Disks Extract — FULL REPLACE v3-C-UNPACK-RESUME\n// (Same resilient download + unzip retry; extraction logic remains \"# show disks\")\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst { execFileSync } = require('child_process');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\nconst MAX_CHARS = Number($env.DISKS_EXTRACT_MAX_CHARS || 1200000);\n\nconst MAX_DOWNLOAD_ATTEMPTS = Number($env.GETFILE_MAX_ATTEMPTS || 50);\nconst REQ_TIMEOUT_MS = Number($env.GETFILE_REQ_TIMEOUT_MS || 60 * 60 * 1000);\nconst BACKOFF_MS = Number($env.GETFILE_BACKOFF_MS || 800);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive:true }); }\nfunction sleep(ms){ return new Promise(r => setTimeout(r, ms)); }\nfunction getToken(){ const t2=s($env?.YANDEX_BOT_TOKEN_LOGPARSER).trim(); return t2 || s($env?.YANDEX_BOT_TOKEN).trim(); }\nfunction extractJsonErr(text){ try{ const obj=JSON.parse(String(text||'')); if(obj&&obj.ok===false) return obj.description||obj.code||'getFile failed'; }catch{} return null; }\nfunction readFirstBytes(fp, n=700){ const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(n); const r=fs.readSync(fd,buf,0,n,0); return buf.slice(0,r);} finally{try{fs.closeSync(fd);}catch{}} }\nfunction isZipMagicBuf(b){ return b&&b.length>=4&&b[0]===0x50&&b[1]===0x4b&&b[2]===0x03&&b[3]===0x04; }\nfunction zipHasEOCD(fp){ try{ const st=fs.statSync(fp); const size=st.size||0; if(size<64) return false; const tail=Math.min(70000,size); const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(tail); fs.readSync(fd,buf,0,tail,size-tail); for(let i=buf.length-22;i>=0;i--){ if(buf[i]===0x50&&buf[i+1]===0x4b&&buf[i+2]===0x05&&buf[i+3]===0x06) return true; } return false; } finally{try{fs.closeSync(fd);}catch{}} }catch{ return false; } }\nfunction hasCmd(cmd){ try{ execFileSync('which',[cmd],{stdio:'ignore'}); return true;}catch{ return false;} }\n\nfunction sniffTypeByMagic(filePath, nameLower){\n  const b = readFirstBytes(filePath, 520);\n  if (b.length>=4 && b[0]===0x50 && b[1]===0x4b && b[2]===0x03 && b[3]===0x04) return 'zip';\n  if (b.length>=3 && b[0]===0x42 && b[1]===0x5a && b[2]===0x68) return (nameLower.endsWith('.tbz2')||nameLower.endsWith('.tar.bz2')||nameLower.endsWith('.tbz'))?'tarbz2':'bz2';\n  if (b.length>=6 && b[0]===0xfd && b[1]===0x37 && b[2]===0x7a && b[3]===0x58 && b[4]===0x5a && b[5]===0x00) return (nameLower.endsWith('.tar.xz')||nameLower.endsWith('.txz'))?'tarxz':'xz';\n  if (b.length>=6 && b[0]===0x37 && b[1]===0x7a && b[2]===0xbc && b[3]===0xaf && b[4]===0x27 && b[5]===0x1c) return '7z';\n  if (b.length>=2 && b[0]===0x1f && b[1]===0x8b) return (nameLower.endsWith('.tar.gz')||nameLower.endsWith('.tgz'))?'targz':'gz';\n  if (b.length>=262 && b.slice(257,262).toString('utf8')==='ustar') return 'tar';\n  return 'plain';\n}\n\nfunction unpackToDir(inFile, type, outDir){\n  ensureDir(outDir);\n  if (type==='zip'){ if(hasCmd('unzip')){ execFileSync('unzip',['-qq',inFile,'-d',outDir]); return; } if(hasCmd('bsdtar')){ execFileSync('bsdtar',['-xf',inFile,'-C',outDir]); return; } if(hasCmd('7z')){ execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; } throw new Error('No unzip/bsdtar/7z'); }\n  if (type==='tar'){ execFileSync('tar',['-xf',inFile,'-C',outDir]); return; }\n  if (type==='targz'){ execFileSync('tar',['-xzf',inFile,'-C',outDir]); return; }\n  if (type==='tarbz2'){ execFileSync('tar',['-xjf',inFile,'-C',outDir]); return; }\n  if (type==='tarxz'){ execFileSync('tar',['-xJf',inFile,'-C',outDir]); return; }\n  if (type==='gz'){ const out=path.join(outDir,'file_from_gz'); execFileSync('bash',['-lc',`gzip -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='bz2'){ const out=path.join(outDir,'file_from_bz2'); execFileSync('bash',['-lc',`bzip2 -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='xz'){ const out=path.join(outDir,'file_from_xz'); execFileSync('bash',['-lc',`xz -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='7z'){ if(!hasCmd('7z')) throw new Error('7z not found'); execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; }\n  fs.copyFileSync(inFile, path.join(outDir, path.basename(inFile)||'file.bin'));\n}\n\nfunction requestRangeToFile(fileId, token, outPath, offset){\n  return new Promise((resolve, reject) => {\n    const p = `/bot/v1/messages/getFile/?file_id=${encodeURIComponent(fileId)}`;\n    const headers = { 'Authorization':`OAuth ${token}`, 'Accept-Encoding':'identity', 'Connection':'close' };\n    if (offset>0) headers['Range'] = `bytes=${offset}-`;\n\n    const req = https.request({ method:'GET', hostname:'botapi.messenger.yandex.net', path:p, headers, timeout: REQ_TIMEOUT_MS }, (res) => {\n      const status = res.statusCode|0;\n      if (offset>0 && status===200) { res.resume(); return resolve({ restart:true, status, bytes:0, headBuf: Buffer.alloc(0) }); }\n\n      const ws = fs.createWriteStream(outPath, { flags: offset>0 ? 'a' : 'w' });\n      let bytes = 0;\n      const headChunks = [];\n      let headLen = 0;\n\n      res.on('data', (chunk) => {\n        const b = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk);\n        bytes += b.length;\n        if (headLen < 2048) {\n          const take = Math.min(2048 - headLen, b.length);\n          if (take>0) headChunks.push(b.slice(0,take));\n          headLen += take;\n        }\n      });\n\n      res.on('aborted', () => reject(new Error('aborted')));\n      res.on('error', reject);\n      res.pipe(ws);\n\n      ws.on('finish', () => resolve({ restart:false, status, bytes, headBuf: Buffer.concat(headChunks).slice(0,2048) }));\n      ws.on('error', reject);\n    });\n\n    req.on('timeout', () => { try{ req.destroy(new Error('aborted')); }catch{} });\n    req.on('error', reject);\n    req.end();\n  });\n}\n\nasync function downloadWithResume(fileId, token, outPath, nameLower){\n  try { fs.rmSync(outPath, { force:true }); } catch {}\n  for (let attempt=1; attempt<=MAX_DOWNLOAD_ATTEMPTS; attempt++){\n    const offset = fs.existsSync(outPath) ? fs.statSync(outPath).size : 0;\n    try {\n      const r = await requestRangeToFile(fileId, token, outPath, offset);\n      if (r.restart) { try{ fs.rmSync(outPath,{force:true}); }catch{}; continue; }\n      const maybeErr = extractJsonErr(r.headBuf.toString('utf8'));\n      if (maybeErr) throw new Error(maybeErr);\n      if (!((r.status>=200 && r.status<300) || r.status===206)) throw new Error(`HTTP ${r.status}`);\n      if ((nameLower.endsWith('.zip') || isZipMagicBuf(readFirstBytes(outPath,4))) && zipHasEOCD(outPath)) return;\n      if (r.bytes===0) return;\n      await sleep(Math.min(10000, BACKOFF_MS * attempt));\n    } catch {\n      await sleep(Math.min(15000, BACKOFF_MS * attempt));\n    }\n  }\n  // final zip check\n  if (isZipMagicBuf(readFirstBytes(outPath,4)) && !zipHasEOCD(outPath)) throw new Error('download incomplete: zip EOCD not found');\n}\n\nfunction walkFiles(root){\n  const out=[]; const stack=[root];\n  while(stack.length){\n    const p = stack.pop();\n    let st; try{ st = fs.statSync(p);}catch{ continue;}\n    if (st.isDirectory()){\n      let names=[]; try{ names=fs.readdirSync(p);}catch{ names=[];}\n      for(const n of names) stack.push(path.join(p,n));\n    } else if (st.isFile()) out.push(p);\n  }\n  return out;\n}\nfunction looksBinaryByHead(buf){ return buf && buf.length ? buf.slice(0, Math.min(buf.length, 65536)).includes(0) : false; }\nfunction decodeSmart(buf){\n  if (!Buffer.isBuffer(buf)) buf = Buffer.from(String(buf||''), 'utf8');\n  if (buf.length>=2 && buf[0]===0xff && buf[1]===0xfe) return buf.slice(2).toString('utf16le');\n  if (looksBinaryByHead(buf)) return '';\n  return buf.toString('utf8');\n}\nfunction pickPrimaryLog(files){\n  let best=null, bestScore=-1;\n  for(const fp of files){\n    const n=path.basename(fp).toLowerCase();\n    let st; try{ st=fs.statSync(fp);}catch{ continue;}\n    let sc=0;\n    if(/^store_.*\\.logs$/.test(n)) sc+=500;\n    if(n.endsWith('.logs')) sc+=30;\n    if(n.endsWith('.log')||n.endsWith('.txt')||n.endsWith('.out')) sc+=10;\n    sc += Math.min(40, Math.floor((st.size||0)/50_000_000));\n    if(sc>bestScore){ bestScore=sc; best=fp; }\n  }\n  return best || files[0];\n}\nfunction extractCliBlock(text, marker, maxLines=20000){\n  const lines = text.split(/\\r?\\n/);\n  let i=-1; for(let k=0;k<lines.length;k++){ if(lines[k].trim()===marker){ i=k; break; } }\n  if(i<0) return '';\n  const out=[];\n  for(let k=i;k<lines.length && out.length<maxLines;k++){\n    const ln=lines[k];\n    if(k>i && ln.startsWith('# ') && ln.trim()!==marker) break;\n    out.push(ln);\n  }\n  return out.join('\\n').trim();\n}\n\nconst fileId = s(base.sourceFileId).trim();\nconst fileName = s(base.sourceFileName).trim() || 'attachment.bin';\nif(!fileId) return [{ json:{ ...base, digest_disks_text:'[n8n] sourceFileId is empty.' } }];\n\nconst token = getToken();\nif(!token) throw new Error('YANDEX_BOT_TOKEN_LOGPARSER/YANDEX_BOT_TOKEN is not set');\n\nconst tmp = fs.mkdtempSync(path.join(os.tmpdir(),'st-disks-'));\nconst outDir = path.join(tmp,'out'); ensureDir(outDir);\nconst inFile = path.join(tmp,'in.bin');\n\ntry{\n  await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n\n  const type = sniffTypeByMagic(inFile, fileName.toLowerCase());\n  let lastErr='';\n  for(let k=1;k<=3;k++){\n    try{\n      try{ fs.rmSync(outDir,{recursive:true,force:true}); }catch{}\n      ensureDir(outDir);\n      unpackToDir(inFile,type,outDir);\n      lastErr=''; break;\n    }catch(e){\n      lastErr=s(e?.message||e);\n      if(type==='zip' && /short read/i.test(lastErr)){\n        await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n        await sleep(400*k);\n        continue;\n      }\n      break;\n    }\n  }\n  if(lastErr) throw new Error(lastErr);\n\n  const files = walkFiles(outDir);\n  if(!files.length) return [{ json:{ ...base, digest_disks_text:'[n8n] No files after unpack.' } }];\n\n  const primary = pickPrimaryLog(files);\n  let text='';\n  try{ text = decodeSmart(fs.readFileSync(primary)); }catch{ text=''; }\n\n  const b1 = extractCliBlock(text, '# show disks', 20000);\n  let digest = b1 ? ('=== # show disks ===\\n' + b1) : '[n8n] No disks block found.';\n  if(digest.length>MAX_CHARS) digest = digest.slice(0,MAX_CHARS) + '\\n[n8n] TRUNCATED';\n\n  return [{ json:{ ...base, digest_disks_text: digest } }];\n} finally {\n  try{ fs.rmSync(tmp,{recursive:true,force:true}); }catch{}\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1824,
        -64
      ],
      "id": "9c5686ed-fe73-4a31-9b02-771a5a725e40",
      "name": "Storage Disks Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Periphery Extract — FULL REPLACE v3-C-UNPACK-RESUME\n// (Same resilient download + unzip retry; extraction logic stays)\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst { execFileSync } = require('child_process');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\nconst MAX_CHARS = Number($env.PERIPHERY_EXTRACT_MAX_CHARS || 800000);\n\nconst MAX_DOWNLOAD_ATTEMPTS = Number($env.GETFILE_MAX_ATTEMPTS || 50);\nconst REQ_TIMEOUT_MS = Number($env.GETFILE_REQ_TIMEOUT_MS || 60 * 60 * 1000);\nconst BACKOFF_MS = Number($env.GETFILE_BACKOFF_MS || 800);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive:true }); }\nfunction sleep(ms){ return new Promise(r => setTimeout(r, ms)); }\nfunction getToken(){ const t2=s($env?.YANDEX_BOT_TOKEN_LOGPARSER).trim(); return t2 || s($env?.YANDEX_BOT_TOKEN).trim(); }\nfunction extractJsonErr(text){ try{ const obj=JSON.parse(String(text||'')); if(obj&&obj.ok===false) return obj.description||obj.code||'getFile failed'; }catch{} return null; }\nfunction readFirstBytes(fp, n=700){ const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(n); const r=fs.readSync(fd,buf,0,n,0); return buf.slice(0,r);} finally{try{fs.closeSync(fd);}catch{}} }\nfunction isZipMagicBuf(b){ return b&&b.length>=4&&b[0]===0x50&&b[1]===0x4b&&b[2]===0x03&&b[3]===0x04; }\nfunction zipHasEOCD(fp){ try{ const st=fs.statSync(fp); const size=st.size||0; if(size<64) return false; const tail=Math.min(70000,size); const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(tail); fs.readSync(fd,buf,0,tail,size-tail); for(let i=buf.length-22;i>=0;i--){ if(buf[i]===0x50&&buf[i+1]===0x4b&&buf[i+2]===0x05&&buf[i+3]===0x06) return true; } return false; } finally{try{fs.closeSync(fd);}catch{}} }catch{ return false; } }\nfunction hasCmd(cmd){ try{ execFileSync('which',[cmd],{stdio:'ignore'}); return true;}catch{ return false;} }\n\nfunction sniffTypeByMagic(filePath, nameLower){\n  const b = readFirstBytes(filePath, 520);\n  if (b.length>=4 && b[0]===0x50 && b[1]===0x4b && b[2]===0x03 && b[3]===0x04) return 'zip';\n  if (b.length>=3 && b[0]===0x42 && b[1]===0x5a && b[2]===0x68) return (nameLower.endsWith('.tbz2')||nameLower.endsWith('.tar.bz2')||nameLower.endsWith('.tbz'))?'tarbz2':'bz2';\n  if (b.length>=6 && b[0]===0xfd && b[1]===0x37 && b[2]===0x7a && b[3]===0x58 && b[4]===0x5a && b[5]===0x00) return (nameLower.endsWith('.tar.xz')||nameLower.endsWith('.txz'))?'tarxz':'xz';\n  if (b.length>=6 && b[0]===0x37 && b[1]===0x7a && b[2]===0xbc && b[3]===0xaf && b[4]===0x27 && b[5]===0x1c) return '7z';\n  if (b.length>=2 && b[0]===0x1f && b[1]===0x8b) return (nameLower.endsWith('.tar.gz')||nameLower.endsWith('.tgz'))?'targz':'gz';\n  if (b.length>=262 && b.slice(257,262).toString('utf8')==='ustar') return 'tar';\n  return 'plain';\n}\n\nfunction unpackToDir(inFile, type, outDir){\n  ensureDir(outDir);\n  if (type==='zip'){ if(hasCmd('unzip')){ execFileSync('unzip',['-qq',inFile,'-d',outDir]); return; } if(hasCmd('bsdtar')){ execFileSync('bsdtar',['-xf',inFile,'-C',outDir]); return; } if(hasCmd('7z')){ execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; } throw new Error('No unzip/bsdtar/7z'); }\n  if (type==='tar'){ execFileSync('tar',['-xf',inFile,'-C',outDir]); return; }\n  if (type==='targz'){ execFileSync('tar',['-xzf',inFile,'-C',outDir]); return; }\n  if (type==='tarbz2'){ execFileSync('tar',['-xjf',inFile,'-C',outDir]); return; }\n  if (type==='tarxz'){ execFileSync('tar',['-xJf',inFile,'-C',outDir]); return; }\n  if (type==='gz'){ const out=path.join(outDir,'file_from_gz'); execFileSync('bash',['-lc',`gzip -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='bz2'){ const out=path.join(outDir,'file_from_bz2'); execFileSync('bash',['-lc',`bzip2 -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='xz'){ const out=path.join(outDir,'file_from_xz'); execFileSync('bash',['-lc',`xz -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='7z'){ if(!hasCmd('7z')) throw new Error('7z not found'); execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; }\n  fs.copyFileSync(inFile, path.join(outDir, path.basename(inFile)||'file.bin'));\n}\n\nfunction requestRangeToFile(fileId, token, outPath, offset){\n  return new Promise((resolve, reject) => {\n    const p = `/bot/v1/messages/getFile/?file_id=${encodeURIComponent(fileId)}`;\n    const headers = { 'Authorization':`OAuth ${token}`, 'Accept-Encoding':'identity', 'Connection':'close' };\n    if (offset>0) headers['Range'] = `bytes=${offset}-`;\n\n    const req = https.request({ method:'GET', hostname:'botapi.messenger.yandex.net', path:p, headers, timeout: REQ_TIMEOUT_MS }, (res) => {\n      const status = res.statusCode|0;\n      if (offset>0 && status===200) { res.resume(); return resolve({ restart:true, status, bytes:0, headBuf: Buffer.alloc(0) }); }\n\n      const ws = fs.createWriteStream(outPath, { flags: offset>0 ? 'a' : 'w' });\n      let bytes = 0;\n      const headChunks = [];\n      let headLen = 0;\n\n      res.on('data', (chunk) => {\n        const b = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk);\n        bytes += b.length;\n        if (headLen < 2048) {\n          const take = Math.min(2048 - headLen, b.length);\n          if (take>0) headChunks.push(b.slice(0,take));\n          headLen += take;\n        }\n      });\n\n      res.on('aborted', () => reject(new Error('aborted')));\n      res.on('error', reject);\n      res.pipe(ws);\n\n      ws.on('finish', () => resolve({ restart:false, status, bytes, headBuf: Buffer.concat(headChunks).slice(0,2048) }));\n      ws.on('error', reject);\n    });\n\n    req.on('timeout', () => { try{ req.destroy(new Error('aborted')); }catch{} });\n    req.on('error', reject);\n    req.end();\n  });\n}\n\nasync function downloadWithResume(fileId, token, outPath, nameLower){\n  try { fs.rmSync(outPath, { force:true }); } catch {}\n  for (let attempt=1; attempt<=MAX_DOWNLOAD_ATTEMPTS; attempt++){\n    const offset = fs.existsSync(outPath) ? fs.statSync(outPath).size : 0;\n    try {\n      const r = await requestRangeToFile(fileId, token, outPath, offset);\n      if (r.restart) { try{ fs.rmSync(outPath,{force:true}); }catch{}; continue; }\n      const maybeErr = extractJsonErr(r.headBuf.toString('utf8'));\n      if (maybeErr) throw new Error(maybeErr);\n      if (!((r.status>=200 && r.status<300) || r.status===206)) throw new Error(`HTTP ${r.status}`);\n      if ((nameLower.endsWith('.zip') || isZipMagicBuf(readFirstBytes(outPath,4))) && zipHasEOCD(outPath)) return;\n      if (r.bytes===0) return;\n      await sleep(Math.min(10000, BACKOFF_MS * attempt));\n    } catch {\n      await sleep(Math.min(15000, BACKOFF_MS * attempt));\n    }\n  }\n  if (isZipMagicBuf(readFirstBytes(outPath,4)) && !zipHasEOCD(outPath)) throw new Error('download incomplete: zip EOCD not found');\n}\n\nfunction walkFiles(root){\n  const out=[]; const stack=[root];\n  while(stack.length){\n    const p=stack.pop();\n    let st; try{ st=fs.statSync(p);}catch{ continue;}\n    if(st.isDirectory()){\n      let names=[]; try{ names=fs.readdirSync(p);}catch{ names=[];}\n      for(const n of names) stack.push(path.join(p,n));\n    } else if(st.isFile()) out.push(p);\n  }\n  return out;\n}\nfunction looksBinaryByHead(buf){ return buf && buf.length ? buf.slice(0, Math.min(buf.length, 65536)).includes(0) : false; }\nfunction decodeSmart(buf){\n  if(!Buffer.isBuffer(buf)) buf=Buffer.from(String(buf||''),'utf8');\n  if(buf.length>=2 && buf[0]===0xff && buf[1]===0xfe) return buf.slice(2).toString('utf16le');\n  if(looksBinaryByHead(buf)) return '';\n  return buf.toString('utf8');\n}\nfunction pickPrimaryLog(files){\n  let best=null, bestScore=-1;\n  for(const fp of files){\n    const n=path.basename(fp).toLowerCase();\n    let st; try{ st=fs.statSync(fp);}catch{ continue;}\n    let sc=0;\n    if(/^store_.*\\.logs$/.test(n)) sc+=500;\n    if(n.endsWith('.logs')) sc+=30;\n    if(n.endsWith('.log')||n.endsWith('.txt')||n.endsWith('.out')) sc+=10;\n    sc += Math.min(40, Math.floor((st.size||0)/50_000_000));\n    if(sc>bestScore){ bestScore=sc; best=fp; }\n  }\n  return best || files[0];\n}\nfunction extractCliBlock(text, marker, maxLines=20000){\n  const lines=text.split(/\\r?\\n/);\n  let i=-1; for(let k=0;k<lines.length;k++){ if(lines[k].trim()===marker){ i=k; break; } }\n  if(i<0) return '';\n  const out=[];\n  for(let k=i;k<lines.length && out.length<maxLines;k++){\n    const ln=lines[k];\n    if(k>i && ln.startsWith('# ') && ln.trim()!==marker) break;\n    out.push(ln);\n  }\n  return out.join('\\n').trim();\n}\nfunction extractSfpLines(text){\n  const lines=text.split(/\\r?\\n/);\n  const out=[];\n  const re=/\\b(sfp-status|sfp-present|sfp-vendor|sfp-serial|sfp-part-number|sfp-partnumber|sfp-speed|sfp-name|sfp-connector)\\b/i;\n  for(const ln of lines){\n    if(re.test(ln)) out.push(ln.trim());\n    if(out.length>=1200) break;\n  }\n  return out.join('\\n').trim();\n}\n\nconst fileId = s(base.sourceFileId).trim();\nconst fileName = s(base.sourceFileName).trim() || 'attachment.bin';\nif(!fileId) return [{ json:{ ...base, digest_periphery_text:'[n8n] sourceFileId is empty.' } }];\n\nconst token = getToken();\nif(!token) throw new Error('YANDEX_BOT_TOKEN_LOGPARSER/YANDEX_BOT_TOKEN is not set');\n\nconst tmp = fs.mkdtempSync(path.join(os.tmpdir(),'st-per-'));\nconst outDir = path.join(tmp,'out'); ensureDir(outDir);\nconst inFile = path.join(tmp,'in.bin');\n\ntry{\n  await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n\n  const type = sniffTypeByMagic(inFile, fileName.toLowerCase());\n  let lastErr='';\n  for(let k=1;k<=3;k++){\n    try{\n      try{ fs.rmSync(outDir,{recursive:true,force:true}); }catch{}\n      ensureDir(outDir);\n      unpackToDir(inFile,type,outDir);\n      lastErr=''; break;\n    }catch(e){\n      lastErr=s(e?.message||e);\n      if(type==='zip' && /short read/i.test(lastErr)){\n        await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n        await sleep(400*k);\n        continue;\n      }\n      break;\n    }\n  }\n  if(lastErr) throw new Error(lastErr);\n\n  const files = walkFiles(outDir);\n  if(!files.length) return [{ json:{ ...base, digest_periphery_text:'[n8n] No files after unpack.' } }];\n\n  const primary = pickPrimaryLog(files);\n  let text='';\n  try{ text = decodeSmart(fs.readFileSync(primary)); }catch{ text=''; }\n\n  const psu = extractCliBlock(text, '# show power-supplies', 8000);\n  const sfp = extractSfpLines(text);\n\n  let digest = [psu ? '=== # show power-supplies ===\\n'+psu : '',\n                sfp ? '=== SFP (xml lines) ===\\n'+sfp : '']\n    .filter(Boolean).join('\\n\\n');\n\n  if(!digest) digest='[n8n] No periphery blocks found.';\n  if(digest.length>MAX_CHARS) digest=digest.slice(0,MAX_CHARS)+'\\n[n8n] TRUNCATED';\n\n  return [{ json:{ ...base, digest_periphery_text: digest } }];\n} finally {\n  try{ fs.rmSync(tmp,{recursive:true,force:true}); }catch{}\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1824,
        128
      ],
      "id": "8cd011bc-338f-4b05-b748-5a86c499c303",
      "name": "Storage Periphery Extract"
    },
    {
      "parameters": {
        "jsCode": "// Storage Error Candidates — FULL REPLACE v3-C-UNPACK-RESUME\n// (Same resilient download + unzip retry; extraction logic stays)\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst { execFileSync } = require('child_process');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\nconst MAX_CHARS = Number($env.ERRORS_EXTRACT_MAX_CHARS || 1400000);\n\nconst MAX_DOWNLOAD_ATTEMPTS = Number($env.GETFILE_MAX_ATTEMPTS || 50);\nconst REQ_TIMEOUT_MS = Number($env.GETFILE_REQ_TIMEOUT_MS || 60 * 60 * 1000);\nconst BACKOFF_MS = Number($env.GETFILE_BACKOFF_MS || 800);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive:true }); }\nfunction sleep(ms){ return new Promise(r => setTimeout(r, ms)); }\nfunction getToken(){ const t2=s($env?.YANDEX_BOT_TOKEN_LOGPARSER).trim(); return t2 || s($env?.YANDEX_BOT_TOKEN).trim(); }\nfunction extractJsonErr(text){ try{ const obj=JSON.parse(String(text||'')); if(obj&&obj.ok===false) return obj.description||obj.code||'getFile failed'; }catch{} return null; }\nfunction readFirstBytes(fp, n=700){ const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(n); const r=fs.readSync(fd,buf,0,n,0); return buf.slice(0,r);} finally{try{fs.closeSync(fd);}catch{}} }\nfunction isZipMagicBuf(b){ return b&&b.length>=4&&b[0]===0x50&&b[1]===0x4b&&b[2]===0x03&&b[3]===0x04; }\nfunction zipHasEOCD(fp){ try{ const st=fs.statSync(fp); const size=st.size||0; if(size<64) return false; const tail=Math.min(70000,size); const fd=fs.openSync(fp,'r'); try{ const buf=Buffer.allocUnsafe(tail); fs.readSync(fd,buf,0,tail,size-tail); for(let i=buf.length-22;i>=0;i--){ if(buf[i]===0x50&&buf[i+1]===0x4b&&buf[i+2]===0x05&&buf[i+3]===0x06) return true; } return false; } finally{try{fs.closeSync(fd);}catch{}} }catch{ return false; } }\nfunction hasCmd(cmd){ try{ execFileSync('which',[cmd],{stdio:'ignore'}); return true;}catch{ return false;} }\n\nfunction sniffTypeByMagic(filePath, nameLower){\n  const b = readFirstBytes(filePath, 520);\n  if (b.length>=4 && b[0]===0x50 && b[1]===0x4b && b[2]===0x03 && b[3]===0x04) return 'zip';\n  if (b.length>=3 && b[0]===0x42 && b[1]===0x5a && b[2]===0x68) return (nameLower.endsWith('.tbz2')||nameLower.endsWith('.tar.bz2')||nameLower.endsWith('.tbz'))?'tarbz2':'bz2';\n  if (b.length>=6 && b[0]===0xfd && b[1]===0x37 && b[2]===0x7a && b[3]===0x58 && b[4]===0x5a && b[5]===0x00) return (nameLower.endsWith('.tar.xz')||nameLower.endsWith('.txz'))?'tarxz':'xz';\n  if (b.length>=6 && b[0]===0x37 && b[1]===0x7a && b[2]===0xbc && b[3]===0xaf && b[4]===0x27 && b[5]===0x1c) return '7z';\n  if (b.length>=2 && b[0]===0x1f && b[1]===0x8b) return (nameLower.endsWith('.tar.gz')||nameLower.endsWith('.tgz'))?'targz':'gz';\n  if (b.length>=262 && b.slice(257,262).toString('utf8')==='ustar') return 'tar';\n  return 'plain';\n}\n\nfunction unpackToDir(inFile, type, outDir){\n  ensureDir(outDir);\n  if (type==='zip'){ if(hasCmd('unzip')){ execFileSync('unzip',['-qq',inFile,'-d',outDir]); return; } if(hasCmd('bsdtar')){ execFileSync('bsdtar',['-xf',inFile,'-C',outDir]); return; } if(hasCmd('7z')){ execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; } throw new Error('No unzip/bsdtar/7z'); }\n  if (type==='tar'){ execFileSync('tar',['-xf',inFile,'-C',outDir]); return; }\n  if (type==='targz'){ execFileSync('tar',['-xzf',inFile,'-C',outDir]); return; }\n  if (type==='tarbz2'){ execFileSync('tar',['-xjf',inFile,'-C',outDir]); return; }\n  if (type==='tarxz'){ execFileSync('tar',['-xJf',inFile,'-C',outDir]); return; }\n  if (type==='gz'){ const out=path.join(outDir,'file_from_gz'); execFileSync('bash',['-lc',`gzip -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='bz2'){ const out=path.join(outDir,'file_from_bz2'); execFileSync('bash',['-lc',`bzip2 -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='xz'){ const out=path.join(outDir,'file_from_xz'); execFileSync('bash',['-lc',`xz -dc \"${inFile.replace(/\"/g,'\\\\\"')}\" > \"${out.replace(/\"/g,'\\\\\"')}\"`]); return; }\n  if (type==='7z'){ if(!hasCmd('7z')) throw new Error('7z not found'); execFileSync('7z',['x','-y',`-o${outDir}`,inFile],{stdio:'ignore'}); return; }\n  fs.copyFileSync(inFile, path.join(outDir, path.basename(inFile)||'file.bin'));\n}\n\nfunction requestRangeToFile(fileId, token, outPath, offset){\n  return new Promise((resolve, reject) => {\n    const p = `/bot/v1/messages/getFile/?file_id=${encodeURIComponent(fileId)}`;\n    const headers = { 'Authorization':`OAuth ${token}`, 'Accept-Encoding':'identity', 'Connection':'close' };\n    if (offset>0) headers['Range'] = `bytes=${offset}-`;\n\n    const req = https.request({ method:'GET', hostname:'botapi.messenger.yandex.net', path:p, headers, timeout: REQ_TIMEOUT_MS }, (res) => {\n      const status = res.statusCode|0;\n      if (offset>0 && status===200) { res.resume(); return resolve({ restart:true, status, bytes:0, headBuf: Buffer.alloc(0) }); }\n\n      const ws = fs.createWriteStream(outPath, { flags: offset>0 ? 'a' : 'w' });\n      let bytes = 0;\n      const headChunks = [];\n      let headLen = 0;\n\n      res.on('data', (chunk) => {\n        const b = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk);\n        bytes += b.length;\n        if (headLen < 2048) {\n          const take = Math.min(2048 - headLen, b.length);\n          if (take>0) headChunks.push(b.slice(0,take));\n          headLen += take;\n        }\n      });\n\n      res.on('aborted', () => reject(new Error('aborted')));\n      res.on('error', reject);\n      res.pipe(ws);\n\n      ws.on('finish', () => resolve({ restart:false, status, bytes, headBuf: Buffer.concat(headChunks).slice(0,2048) }));\n      ws.on('error', reject);\n    });\n\n    req.on('timeout', () => { try{ req.destroy(new Error('aborted')); }catch{} });\n    req.on('error', reject);\n    req.end();\n  });\n}\n\nasync function downloadWithResume(fileId, token, outPath, nameLower){\n  try { fs.rmSync(outPath, { force:true }); } catch {}\n  for (let attempt=1; attempt<=MAX_DOWNLOAD_ATTEMPTS; attempt++){\n    const offset = fs.existsSync(outPath) ? fs.statSync(outPath).size : 0;\n    try {\n      const r = await requestRangeToFile(fileId, token, outPath, offset);\n      if (r.restart) { try{ fs.rmSync(outPath,{force:true}); }catch{}; continue; }\n      const maybeErr = extractJsonErr(r.headBuf.toString('utf8'));\n      if (maybeErr) throw new Error(maybeErr);\n      if (!((r.status>=200 && r.status<300) || r.status===206)) throw new Error(`HTTP ${r.status}`);\n      if ((nameLower.endsWith('.zip') || isZipMagicBuf(readFirstBytes(outPath,4))) && zipHasEOCD(outPath)) return;\n      if (r.bytes===0) return;\n      await sleep(Math.min(10000, BACKOFF_MS * attempt));\n    } catch {\n      await sleep(Math.min(15000, BACKOFF_MS * attempt));\n    }\n  }\n  if (isZipMagicBuf(readFirstBytes(outPath,4)) && !zipHasEOCD(outPath)) throw new Error('download incomplete: zip EOCD not found');\n}\n\nfunction walkFiles(root){\n  const out=[]; const stack=[root];\n  while(stack.length){\n    const p=stack.pop();\n    let st; try{ st=fs.statSync(p);}catch{ continue;}\n    if(st.isDirectory()){\n      let names=[]; try{ names=fs.readdirSync(p);}catch{ names=[];}\n      for(const n of names) stack.push(path.join(p,n));\n    } else if(st.isFile()) out.push(p);\n  }\n  return out;\n}\nfunction looksBinaryByHead(buf){ return buf && buf.length ? buf.slice(0, Math.min(buf.length, 65536)).includes(0) : false; }\nfunction decodeSmart(buf){\n  if(!Buffer.isBuffer(buf)) buf=Buffer.from(String(buf||''),'utf8');\n  if(buf.length>=2 && buf[0]===0xff && buf[1]===0xfe) return buf.slice(2).toString('utf16le');\n  if(looksBinaryByHead(buf)) return '';\n  return buf.toString('utf8');\n}\nfunction pickPrimaryLog(files){\n  let best=null, bestScore=-1;\n  for(const fp of files){\n    const n=path.basename(fp).toLowerCase();\n    let st; try{ st=fs.statSync(fp);}catch{ continue;}\n    let sc=0;\n    if(/^store_.*\\.logs$/.test(n)) sc+=500;\n    if(n.endsWith('.logs')) sc+=30;\n    if(n.endsWith('.log')||n.endsWith('.txt')||n.endsWith('.out')) sc+=10;\n    sc += Math.min(40, Math.floor((st.size||0)/50_000_000));\n    if(sc>bestScore){ bestScore=sc; best=fp; }\n  }\n  return best || files[0];\n}\n\nconst CONTEXT = Number($env.ERRORS_CONTEXT_LINES || 2);\n\nfunction pickLinesWithContext(lines, predicate){\n  const marked = new Set();\n  for (let i=0;i<lines.length;i++){\n    if (!predicate(lines[i], i)) continue;\n    for (let k=Math.max(0,i-CONTEXT); k<=Math.min(lines.length-1, i+CONTEXT); k++) marked.add(k);\n  }\n  return Array.from(marked).sort((a,b)=>a-b).map(i => lines[i]);\n}\n\nconst fileId = s(base.sourceFileId).trim();\nconst fileName = s(base.sourceFileName).trim() || 'attachment.bin';\nif(!fileId) return [{ json:{ ...base, digest_errors_text:'[n8n] sourceFileId is empty.' } }];\n\nconst token = getToken();\nif(!token) throw new Error('YANDEX_BOT_TOKEN_LOGPARSER/YANDEX_BOT_TOKEN is not set');\n\nconst tmp = fs.mkdtempSync(path.join(os.tmpdir(),'st-err-'));\nconst outDir = path.join(tmp,'out'); ensureDir(outDir);\nconst inFile = path.join(tmp,'in.bin');\n\ntry{\n  await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n\n  const type = sniffTypeByMagic(inFile, fileName.toLowerCase());\n  let lastErr='';\n  for(let k=1;k<=3;k++){\n    try{\n      try{ fs.rmSync(outDir,{recursive:true,force:true}); }catch{}\n      ensureDir(outDir);\n      unpackToDir(inFile,type,outDir);\n      lastErr=''; break;\n    }catch(e){\n      lastErr=s(e?.message||e);\n      if(type==='zip' && /short read/i.test(lastErr)){\n        await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n        await sleep(400*k);\n        continue;\n      }\n      break;\n    }\n  }\n  if(lastErr) throw new Error(lastErr);\n\n  const files = walkFiles(outDir);\n  if(!files.length) return [{ json:{ ...base, digest_errors_text:'[n8n] No files after unpack.' } }];\n\n  const primary = pickPrimaryLog(files);\n  let text='';\n  try{ text = decodeSmart(fs.readFileSync(primary)); }catch{ text=''; }\n\n  const lines = text.split(/\\r?\\n/);\n\n  const reHit = /(dgA01|disk group is quarantined|disk group.*quarant|quarantined|health\\s+Fault|unhealthy-component|A3826|A3834|A3835)/i;\n  const hitLines = pickLinesWithContext(lines, (ln)=>reHit.test(ln));\n\n  let digest = hitLines.length ? hitLines.join('\\n') : '[n8n] No error hits found.';\n  if(digest.length>MAX_CHARS) digest = digest.slice(0,MAX_CHARS) + '\\n[n8n] TRUNCATED';\n\n  return [{ json:{ ...base, digest_errors_text: digest } }];\n} finally {\n  try{ fs.rmSync(tmp,{recursive:true,force:true}); }catch{}\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1824,
        320
      ],
      "id": "583810f6-405c-474b-8c90-646df40fbdb6",
      "name": "Storage Error Candidates"
    },
    {
      "parameters": {
        "jsCode": "// Assemble Storage Report — FULL REPLACE v7-table-only\n// Fix: strip any narratives from LLM outputs. Keep ONLY required TABLE blocks.\n// Does not change LLM logic, only report assembly.\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction extractAssistantText(j) {\n  if (!j || typeof j !== 'object') return '';\n  if (typeof j.message?.content === 'string') return j.message.content;\n  if (typeof j.ollama?.message?.content === 'string') return j.ollama.message.content;\n  if (typeof j.response === 'string') return j.response;\n  if (typeof j.text === 'string') return j.text;\n  return '';\n}\n\nfunction stripThink(raw) {\n  return s(raw).replace(/<think>[\\s\\S]*?<\\/think>/gi, '').trim();\n}\n\nfunction stripCodeFences(raw) {\n  const t = s(raw).trim();\n  const m = t.match(/^```[a-z0-9_-]*\\s*([\\s\\S]*?)\\s*```$/i);\n  return m ? m[1].trim() : t;\n}\n\nfunction cleanText(raw) {\n  return stripCodeFences(stripThink(raw)).replace(/\\r/g, '').trim();\n}\n\nfunction extractTableBlock(text, tableName) {\n  const t = cleanText(text);\n  if (!t) return '';\n\n  const lines = t.split('\\n');\n  const reTitle = new RegExp(`^\\\\s*TABLE:\\\\s*${tableName}\\\\s*$`, 'i');\n\n  let start = -1;\n  for (let i = 0; i < lines.length; i++) {\n    if (reTitle.test(lines[i])) { start = i; break; }\n  }\n  if (start < 0) return '';\n\n  const out = [];\n  out.push(`TABLE: ${tableName}`);\n\n  // take next lines until next TABLE: or end\n  for (let i = start + 1; i < lines.length; i++) {\n    const ln = lines[i];\n\n    // stop at next table title\n    if (/^\\s*TABLE:\\s*/i.test(ln)) break;\n\n    // skip markdown headings/bullets that sometimes appear\n    // (but keep CSV-like lines with ;)\n    const trimmed = ln.trim();\n\n    // allow empty line inside table? stop on first strong section marker\n    if (/^\\s*\\d+\\)\\s+/i.test(trimmed)) break; // \"1) Parser metadata\" etc\n    if (/^\\s*#+\\s+/.test(trimmed)) continue; // markdown headings\n\n    out.push(ln);\n  }\n\n  // Normalize: if model wrote nothing after title -> N/A\n  const body = out.slice(1).join('\\n').trim();\n  if (!body) return `TABLE: ${tableName}\\nN/A`;\n\n  // If first body line is N/A -> keep just that\n  if (/^N\\/A\\s*$/i.test(body.split('\\n')[0].trim())) return `TABLE: ${tableName}\\nN/A`;\n\n  return out.join('\\n').trim();\n}\n\nfunction pickBestTask(items, taskName) {\n  const candidates = items.filter(it => s(it.task).toLowerCase() === taskName);\n  if (!candidates.length) return '';\n  candidates.sort((a,b) => extractAssistantText(b).length - extractAssistantText(a).length);\n  return extractAssistantText(candidates[0]);\n}\n\nfunction collectAllItems(){\n  const out = [];\n  try { for (const it of $input.all()) out.push(it.json || {}); }\n  catch { out.push($input.item.json || {}); }\n  return out;\n}\n\nfunction nowMSK() {\n  try {\n    const d = new Date();\n    const dtf = new Intl.DateTimeFormat('en-GB', {\n      timeZone: 'Europe/Moscow',\n      year:'numeric', month:'2-digit', day:'2-digit',\n      hour:'2-digit', minute:'2-digit', second:'2-digit', hour12:false,\n    });\n    const parts = dtf.format(d).replace(',', '');\n    const m = parts.match(/^(\\d{2})\\/(\\d{2})\\/(\\d{4})\\s+(\\d{2}):(\\d{2}):(\\d{2})$/);\n    if (!m) return parts + ' (MSK)';\n    return `${m[1]}-${m[2]}-${m[3]} ${m[4]}:${m[5]}:${m[6]} (MSK)`;\n  } catch {\n    return new Date().toISOString().replace('T',' ').replace('Z','') + ' (MSK~)';\n  }\n}\n\nconst items = collectAllItems();\nconst any = items.find(it => it.ReportTimestampMSK || it.hw_vendor || it.vendor || it.klass || it.modelDetected) || items[0] || {};\n\n// raw LLM outputs\nconst rawStruct = pickBestTask(items, 'storage_structure');\nconst rawDisks  = pickBestTask(items, 'storage_disks');\nconst rawPer    = pickBestTask(items, 'storage_periphery');\nconst rawErr    = pickBestTask(items, 'storage_errors');\n\n// metadata for section 1/2\nconst reportLLM = s(any.ReportLLM || $env.LOGPARSER_REPORT_LLM || 'qwen2.5:32b-instruct-q8_0').trim() || 'qwen2.5:32b-instruct-q8_0';\nconst reportTs  = s(any.ReportTimestampMSK).trim() || nowMSK();\n\nconst vendor = s(any.hw_vendor || any.vendor || '').trim() || 'N/A';\nconst klass  = s(any.hw_class || any.klass || any.class || '').trim() || 'N/A';\nconst model  = s(any.hw_model || any.modelDetected || any.model || '').trim() || 'N/A';\n\n// SECTION 2 tables (ONLY)\nconst tblEncl = extractTableBlock(rawStruct, 'Enclosures') || 'TABLE: Enclosures\\nN/A';\nconst tblCtrl = extractTableBlock(rawStruct, 'Controllers') || 'TABLE: Controllers\\nN/A';\n\n// SECTION 3 tables (ONLY)\nconst tblDisks = extractTableBlock(rawDisks, 'Disks') || 'TABLE: Disks\\nN/A';\nconst tblPSU   = extractTableBlock(rawPer, 'PSU')   || 'TABLE: PSU\\nN/A';\nconst tblSFP   = extractTableBlock(rawPer, 'SFP')   || 'TABLE: SFP\\nN/A';\n\n// SECTION 4 table (ONLY)\nconst tblErr   = extractTableBlock(rawErr, 'Errors') || 'TABLE: Errors\\nN/A';\n\nconst sec1 =\n`1) Parser metadata:\n   - LLM: ${reportLLM}\n   - Timestamp (MSK): ${reportTs}`.trim();\n\nconst sec2 =\n`2) Hardware identification:\nVendor: ${vendor}\nClass: ${klass}\nModel: ${model}\n${tblEncl}\n\n${tblCtrl}`.trim();\n\nconst sec3 =\n`3) Component status:\n${tblDisks}\n\n${tblPSU}\n\n${tblSFP}`.trim();\n\nconst sec4 =\n`4) Errors:\n${tblErr}`.trim();\n\nconst finalReport = [sec1, '', sec2, '', sec3, '', sec4].join('\\n').trim();\n\nreturn [{\n  json: Object.assign({}, any, {\n    message: { role: 'assistant', content: finalReport },\n  })\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3808,
        32
      ],
      "id": "a64b22c8-2e01-448c-abf8-2740a425411b",
      "name": "Assemble Storage Report"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Structure input — FULL REPLACE v2-B (Qwen, 1 task = 1 answer, tables only)\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_STRUCTURE).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  s($env.LOGPARSER_LLM_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nconst MAX = Number($env.STRUCTURE_PROMPT_MAX_CHARS || 600000);\n\nfunction clip(t, n) {\n  const x = s(t);\n  return x.length <= n ? x : (x.slice(0, n) + `\\n[n8n] TRUNCATED to ${n} chars`);\n}\n\nfunction getDigest(b) {\n  return (\n    s(b.digest_structure_text) ||\n    s(b.structure_digest) ||\n    s(b.structureText) ||\n    ''\n  );\n}\n\nconst digest = clip(getDigest(base), MAX);\n\n// context (do NOT ask model to output these)\nconst vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\nconst klass  = s(base.hw_class || base.klass || base.class || '').trim() || 'N/A';\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || '').trim() || 'N/A';\n\nconst llm_input =\n`TASK: STORAGE / SYSTEM CONFIG (SECTION 2 TABLES ONLY)\n\nYou are given evidence extracted from the ORIGINAL log archive.\nYour job: produce ONLY the two tables below. No other text. No explanations.\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  1) Enclosures table\n  2) Controllers table\n- Use semicolon-separated tables (CSV-like) with ONE header row per table.\n- Title line before each table exactly:\n  TABLE: Enclosures\n  TABLE: Controllers\n- If a table has 0 rows: output \"N/A\" on the next line and DO NOT print the header.\n- If a column is N/A for ALL rows, omit that column (header + cells).\n- Use exact values from evidence. Do NOT invent. If a cell is missing -> N/A.\n- Do NOT include Vendor/Class/Model lines in output (they are handled elsewhere).\n\nSUGGESTED COLUMNS:\n- Enclosures: EnclId;EnclWWN;Vendor;Model;TopLevelAssemblyPartNumber;EMP_A;EMP_B;MidplaneType;Health;Reason;Action\n- Controllers: Controller;Status;SerialNumber;RedundancyMode;RedundancyStatus\n\nCONTEXT (do not output):\nVendor=${vendor}\nClass=${klass}\nModel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${digest || '[n8n] No structure evidence found.'}\nEND EVIDENCE>>>`;\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_structure',\n    taskOrder: 2,\n    model,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2048,
        -256
      ],
      "id": "844a1772-9d70-42ba-bbf6-781a8e428929",
      "name": "Compose Storage Structure input"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Disk input — FULL REPLACE v3-B-FIX (FIX: force table-only; Qwen)\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? \"\" : String(x));\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_DISKS).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  \"qwen2.5:32b-instruct-q8_0\";\n\nconst MAX = Number($env.DISKS_PROMPT_MAX_CHARS || 800000);\n\nfunction clip(t, n) {\n  const x = s(t);\n  return x.length <= n ? x : (x.slice(0, n) + `\\n[n8n] TRUNCATED to ${n} chars`);\n}\n\nconst digest = clip(s(base.digest_disks_text).trim(), MAX);\n\nconst vendor = s(base.hw_vendor || base.vendor || \"\").trim() || \"N/A\";\nconst klass  = s(base.hw_class || base.klass || base.class || \"\").trim() || \"N/A\";\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || \"\").trim() || \"N/A\";\n\nconst llm_input =\n`TASK: STORAGE / DISKS INVENTORY (SECTION 3 - DISKS TABLE ONLY)\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  TABLE: Disks\n  <semicolon-separated table OR N/A>\n- NO other text. NO explanations.\n- If 0 rows: output \"N/A\" on the next line and DO NOT print the header.\n- If a column is N/A for ALL rows, omit that column (header + rows).\n- Use exact values from evidence. Do NOT invent.\n\nSUGGESTED COLUMNS:\nLocation;Enclosure;Slot;SerialNumber;Vendor;PartNumber;Description;Usage;DiskGroup;Pool;Tier;Size;SecFmt;SpeedKRPM;Health\n(Note: derive Enclosure/Slot from Location when possible, e.g. \"1.56\" => Enclosure=1 Slot=56)\n\nCONTEXT (do not output):\nVendor=${vendor}\nClass=${klass}\nModel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${digest || \"[n8n] DISKS DIGEST EMPTY\"}\nEND EVIDENCE>>>`;\n\nreturn [{\n  json: {\n    ...base,\n    task: \"storage_disks\",\n    taskOrder: 3,\n    model,\n    llm_input,\n    messages: [{ role: \"user\", content: llm_input }],\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2048,
        -64
      ],
      "id": "1de92138-945b-490a-b636-bf819a01307a",
      "name": "Compose Storage Disk input"
    },
    {
      "parameters": {
        "jsCode": "// Compose Storage Periphery input — FULL REPLACE v2-B (Qwen, 1 task = 1 answer, tables only)\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_PERIPHERY).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  s($env.LOGPARSER_LLM_MODEL).trim() ||\n  'qwen2.5:32b-instruct-q8_0';\n\nconst MAX = Number($env.PERIPHERY_PROMPT_MAX_CHARS || 600000);\n\nfunction clip(t, n) {\n  const x = s(t);\n  return x.length <= n ? x : (x.slice(0, n) + `\\n[n8n] TRUNCATED to ${n} chars`);\n}\n\nfunction getDigest(b) {\n  return (\n    s(b.digest_periphery_text) ||\n    s(b.periphery_digest) ||\n    s(b.peripheryText) ||\n    ''\n  );\n}\n\nconst digest = clip(getDigest(base), MAX);\n\n// context (do NOT ask model to output these)\nconst vendor = s(base.hw_vendor || base.vendor || '').trim() || 'N/A';\nconst klass  = s(base.hw_class || base.klass || base.class || '').trim() || 'N/A';\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || '').trim() || 'N/A';\n\nconst llm_input =\n`TASK: STORAGE / PERIPHERY (SECTION 3 PSU+SFP TABLES ONLY)\n\nYou are given evidence extracted from the ORIGINAL log archive.\nYour job: produce ONLY the two tables below. No other text. No explanations.\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  1) PSU table\n  2) SFP table\n- Use semicolon-separated tables (CSV-like) with ONE header row per table.\n- Title line before each table exactly:\n  TABLE: PSU\n  TABLE: SFP\n- If a table has 0 rows: output \"N/A\" on the next line and DO NOT print the header.\n- If a column is N/A for ALL rows, omit that column (header + cells).\n- Use exact values from evidence. Do NOT invent. If a cell is missing -> N/A.\n- Do NOT include any explanations.\n\nSUGGESTED COLUMNS:\n- PSU: EnclId;PSUId;SerialNumber;PartNumber;Name;FirmwareVersion;Health;Reason;Action\n- SFP: Port;Present;Status;Vendor;PartNumber;SerialNumber;Speed;Health;Reason\n\nCONTEXT (do not output):\nVendor=${vendor}\nClass=${klass}\nModel=${modelHw}\n\nEVIDENCE:\n<<<BEGIN EVIDENCE\n${digest || '[n8n] No periphery evidence found.'}\nEND EVIDENCE>>>`;\n\nreturn [{\n  json: {\n    ...base,\n    task: 'storage_periphery',\n    taskOrder: 4,\n    model,\n    llm_input,\n    messages: [{ role: 'user', content: llm_input }],\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2272,
        128
      ],
      "id": "58f6ce60-27a2-4232-973f-6ff0f84177c7",
      "name": "Compose Storage Periphery input"
    },
    {
      "parameters": {
        "jsCode": "// 1) RAG Retrieve Storage (offline) — FULL REPLACE v2-B-FIX6\n// FIX: .base -> ...base (success + catch), klass param, stable outputs\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? \"\" : String(x));\n\nconst RAG_URL = (String($env.RAG_API_URL || \"http://rag-api:8099\")).replace(/\\/$/, \"\");\nconst TOKEN = String($env.RAG_API_TOKEN || \"\").trim();\nconst TOP_K = Number($env.RAG_TOP_K || 20);\n\nconst MAX_QUERY = Number($env.RAG_QUERY_MAX_CHARS || 380);\nconst DOCS_MAX_CHARS = Number($env.RAG_DOCS_CONTEXT_MAX_CHARS || 22000);\nconst CAT_MAX_CHARS = Number($env.RAG_CATALOG_CONTEXT_MAX_CHARS || 18000);\nconst MAX_TARGETS = Number($env.RAG_MAX_TARGETS || 800);\n\nfunction normVendor(v) {\n  const u = s(v).trim();\n  const low = u.toLowerCase();\n  if (!u) return \"N/A\";\n  if (low === \"dell\" || low.includes(\"dell emc\")) return \"Dell EMC\";\n  if (low.includes(\"hewlett\") || low === \"hpe\" || low.includes(\"hp \")) return \"HPE\";\n  if (low.includes(\"lenovo\")) return \"Lenovo\";\n  if (low.includes(\"ibm\")) return \"IBM\";\n  if (low.includes(\"netapp\")) return \"NetApp\";\n  if (low.includes(\"cisco\")) return \"Cisco\";\n  if (low.includes(\"brocade\")) return \"Brocade\";\n  if (low.includes(\"huawei\")) return \"Huawei\";\n  return u;\n}\nfunction normKlass(k) {\n  const u = s(k).trim();\n  const low = u.toLowerCase();\n  if (!u) return \"Other\";\n  if (low.startsWith(\"stor\") || low.startsWith(\"starage\")) return \"Storage\";\n  if (low.startsWith(\"serv\")) return \"Server\";\n  if (low.startsWith(\"swit\") || low === \"switch\") return \"Switch\";\n  return u;\n}\nfunction clip(t, n) { t = s(t); return t.length <= n ? t : t.slice(0, n); }\n\nfunction buildQuerySeed() {\n  const parts = [];\n  const add = (v) => { v = s(v).trim(); if (v) parts.push(v); };\n  add(base.rag_query_seed);\n  add(base.digest_errors_text);\n  add(base.digest_structure_text);\n  add(base.digest_disks_text);\n\n  const raw = parts.join(\"\\n\");\n  if (!raw.trim()) return \"storage log errors degraded quarantined disk group\";\n\n  const lines = raw.split(/\\r?\\n/).map(l => l.trim()).filter(Boolean);\n  const kw = /(quarant|fault|disk group|vdisk|dgA\\d+|A3826|A3834|A3835|degrad|down|offline|failed|uncorrect|crc|current_event|sensor_alarm|health)/i;\n\n  const scored = [];\n  for (const l of lines.slice(0, 50000)) {\n    let score = 0;\n    if (kw.test(l)) score += 5;\n    if (l.length > 12 && l.length < 260) score += 1;\n    scored.push([score, l]);\n  }\n  scored.sort((a,b) => b[0]-a[0]);\n  return scored.slice(0, 10).map(x => x[1]).join(\"\\n\");\n}\n\nconst rag_vendor = normVendor(base.hw_vendor || base.vendor);\nconst rag_class  = normKlass(base.hw_class || base.klass || base.class);\nconst rag_model  = s(base.hw_model || base.modelDetected || base.model).trim() || \"N/A\";\n\nlet query = buildQuerySeed().replace(/\\s+/g, \" \").trim();\nquery = clip(query, MAX_QUERY);\n\nconst debug = { sent: null, got: null, note: null };\n\nif (!rag_vendor || rag_vendor === \"N/A\" || !rag_class) {\n  return [{\n    json: {\n      ...base,\n      rag_vendor, rag_class, rag_model,\n      rag_query: query,\n      _rag_error: \"vendor_or_class_missing\",\n      rag_debug: debug,\n      event_catalog_targets: [],\n      event_catalog_context_text: \"\",\n      docs_context_text: \"\",\n    }\n  }];\n}\n\ntry {\n  const body = { query, vendor: rag_vendor, klass: rag_class, top_k: TOP_K };\n  if (rag_model && rag_model !== \"N/A\" && rag_model.length <= 32) body.model = rag_model;\n  debug.sent = body;\n\n  const resp = await this.helpers.httpRequest({\n    method: \"POST\",\n    url: `${RAG_URL}/search/all`,\n    json: true,\n    timeout: 30000,\n    headers: {\n      ...(TOKEN ? { Authorization: TOKEN.startsWith(\"Bearer \") ? TOKEN : `Bearer ${TOKEN}` } : {}),\n      \"Content-Type\": \"application/json\",\n    },\n    body,\n  });\n\n  debug.got = { keys: resp && typeof resp === \"object\" ? Object.keys(resp) : typeof resp };\n\n  const docs = Array.isArray(resp?.docs) ? resp.docs : (Array.isArray(resp?.hits?.docs) ? resp.hits.docs : []);\n  const cat  = Array.isArray(resp?.event_catalog) ? resp.event_catalog : (Array.isArray(resp?.hits?.event_catalog) ? resp.hits.event_catalog : []);\n\n  function fmtDocsContext(list, maxChars) {\n    const out = [];\n    let used = 0;\n    for (const h of list || []) {\n      const file = s(h.file_name || h.file || h.rel_path || \"doc\").trim();\n      const page = (h.page == null || h.page === \"\") ? \"\" : ` p.${h.page}`;\n      const ex = s(h.excerpt || h.text || h.phrase || \"\").trim();\n      if (!ex) continue;\n      const line = `- [${file}${page}] ${ex}`.trim();\n      if (used + line.length + 1 > maxChars) break;\n      out.push(line); used += line.length + 1;\n    }\n    return out.length ? `DOCS CONTEXT (RAG)\\n${out.join(\"\\n\")}\\nEND DOCS CONTEXT` : \"\";\n  }\n\n  function fmtCatalogContext(list, maxChars) {\n    const out = [];\n    let used = 0;\n    for (const h of list || []) {\n      const phrase = s(h.phrase || h.text || \"\").trim();\n      if (!phrase) continue;\n      const sev = s(h.severity || \"\").trim();\n      const comp = s(h.component || \"\").trim();\n      const kb = s(h.kb || h.rel_path || h.file_name || \"\").trim();\n      const bits = [phrase];\n      if (sev) bits.push(`severity=${sev}`);\n      if (comp) bits.push(`component=${comp}`);\n      if (kb) bits.push(`ref=${kb}`);\n      const line = `- ${bits.join(\" | \")}`;\n      if (used + line.length + 1 > maxChars) break;\n      out.push(line); used += line.length + 1;\n      if (out.length >= MAX_TARGETS) break;\n    }\n    return out.length ? `EVENT CATALOG (RAG)\\n${out.join(\"\\n\")}\\nEND EVENT CATALOG` : \"\";\n  }\n\n  const event_catalog_targets = (cat || []).slice(0, MAX_TARGETS).map(h => ({\n    phrase: s(h.phrase || h.text || \"\").trim(),\n    severity: s(h.severity || \"\").trim(),\n    component: s(h.component || \"\").trim(),\n    kb: s(h.kb || \"\").trim(),\n    file_name: s(h.file_name || \"\").trim(),\n    rel_path: s(h.rel_path || \"\").trim(),\n    page: h.page,\n    score: h.score,\n  })).filter(x => x.phrase);\n\n  const docs_context_text = fmtDocsContext(docs, DOCS_MAX_CHARS);\n  const event_catalog_context_text = fmtCatalogContext(event_catalog_targets, CAT_MAX_CHARS);\n\n  return [{\n    json: {\n      ...base,\n      rag_vendor, rag_class, rag_model,\n      rag_query: query,\n      rag_debug: debug,\n      event_catalog_targets,\n      event_catalog_context_text,\n      docs_context_text,\n    }\n  }];\n\n} catch (e) {\n  debug.note = s(e?.message || e);\n  return [{\n    json: {\n      ...base,\n      rag_vendor, rag_class, rag_model,\n      rag_query: query,\n      _rag_error: \"rag_request_failed\",\n      rag_debug: debug,\n      event_catalog_targets: [],\n      event_catalog_context_text: \"\",\n      docs_context_text: \"\",\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2048,
        320
      ],
      "id": "df7e22ad-eaea-402b-9c2c-96f6238b0d9a",
      "name": "RAG Retrieve Storage (offline)"
    },
    {
      "parameters": {
        "jsCode": "// RAG Target Matcher — FULL REPLACE v3-B-FIX (FIX: ...base, no \".base\")\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? \"\" : String(x));\n\nconst targets = Array.isArray(base.event_catalog_targets) ? base.event_catalog_targets : [];\nconst text = s(base.digest_errors_text || base.errors_digest || base.errorsText || \"\");\n\nconst MAX_LINES = Number($env.MATCH_MAX_LINES || 120000);\nconst MAX_CONFIRMED = Number($env.MATCH_MAX_CONFIRMED || 4000);\nconst MAX_PER_PHRASE = Number($env.MATCH_MAX_PER_PHRASE || 4);\n\nfunction normLine(x) { return s(x).replace(/\\s+/g, \" \").trim(); }\nconst lines = text.split(/\\r?\\n/).slice(0, MAX_LINES);\n\nfunction safeRegex(str) { return str.replace(/[.*+?^${}()|[\\]\\\\]/g, \"\\\\$&\"); }\n\nfunction phraseToMatcher(phrase) {\n  const p = normLine(phrase);\n  if (!p) return null;\n\n  const tokens = p.split(/\\s+/).filter(t => t.length >= 4 && !/^\\d+$/.test(t)).slice(0, 8);\n  if (p.length > 80 && tokens.length >= 2) {\n    const a = safeRegex(tokens[0]);\n    const b = safeRegex(tokens[1]);\n    return new RegExp(`(?=.*${a})(?=.*${b})`, \"i\");\n  }\n  return new RegExp(safeRegex(p), \"i\");\n}\n\nconst confirmed = [];\nconst unmatched = [];\nconst debug = { targets: targets.length, lines: lines.length };\n\nfor (const t of targets) {\n  const phrase = normLine(t?.phrase);\n  if (!phrase) continue;\n\n  const re = phraseToMatcher(phrase);\n  if (!re) { unmatched.push(phrase); continue; }\n\n  let found = 0;\n  for (let i = 0; i < lines.length; i++) {\n    const ln = lines[i];\n    if (!ln) continue;\n\n    if (!re.test(ln)) continue;\n\n    confirmed.push({\n      phrase,\n      severity: normLine(t?.severity),\n      component: normLine(t?.component),\n      kb: normLine(t?.kb || t?.rel_path || t?.file_name),\n      source: normLine(t?.file_name || t?.rel_path),\n      line: i + 1,\n      evidence: normLine(ln).slice(0, 800),\n    });\n\n    found++;\n    if (found >= MAX_PER_PHRASE) break;\n    if (confirmed.length >= MAX_CONFIRMED) break;\n  }\n\n  if (!found) unmatched.push(phrase);\n  if (confirmed.length >= MAX_CONFIRMED) break;\n}\n\nconst dgHits = [];\nconst dgRe = /(disk group|vdisk)\\s+dgA01|dgA01.*quarant|quarant.*dgA01/i;\nfor (let i = 0; i < lines.length && dgHits.length < 20; i++) {\n  const ln = lines[i];\n  if (dgRe.test(ln)) dgHits.push({ line: i + 1, evidence: normLine(ln).slice(0, 800) });\n}\ndebug.dgA01_hits = dgHits.length;\n\nreturn [{\n  json: {\n    ...base,\n    confirmed_events: confirmed,\n    unmatched_targets: unmatched,\n    match_debug: debug,\n    dgA01_hits: dgHits,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2272,
        320
      ],
      "id": "1b2eedff-39c3-4520-a7d5-14856cf46b61",
      "name": "RAG Target Matcher"
    },
    {
      "parameters": {
        "jsCode": "// 5) Compose input ERRORS — FULL REPLACE v6-B-FIX7\n// FIX: убираем старую v2 (deepseek+4 секции). Теперь строго TABLE: Errors + HEADER + rows|N/A\n// Это чинит кейс Answer ID1149 где после \"TABLE: Errors\" нет хедера:contentReference[oaicite:4]{index=4}\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? \"\" : String(x));\n\nconst model =\n  s($env.STORAGE_LLM_MODEL_ERRORS).trim() ||\n  s($env.STORAGE_LLM_MODEL).trim() ||\n  \"qwen2.5:32b-instruct-q8_0\";\n\nconst MAX = Number($env.ERROR_PROMPT_MAX_CHARS || 900000);\nconst HEADER = \"Timestamp;Severity;Component;CodeOrPhrase;Message;Source;KB\";\n\nfunction clip(t, n) {\n  const x = s(t);\n  return x.length <= n ? x : (x.slice(0, n) + `\\n[n8n] TRUNCATED to ${n} chars`);\n}\n\nconst vendor = s(base.hw_vendor || base.vendor || \"\").trim() || \"N/A\";\nconst klass  = s(base.hw_class || base.klass || base.class || \"\").trim() || \"N/A\";\nconst modelHw = s(base.hw_model || base.modelDetected || base.model || \"\").trim() || \"N/A\";\n\nconst digest = clip(s(base.digest_errors_text).trim(), MAX);\nconst docsCtx = clip(s(base.docs_context_text || base.docs_context).trim(), 30000);\nconst catCtx  = clip(s(base.event_catalog_context_text).trim(), 25000);\n\nconst confirmed = Array.isArray(base.confirmed_events) ? base.confirmed_events : [];\nconst dgHits = Array.isArray(base.dgA01_hits) ? base.dgA01_hits : [];\n\nfunction fmtConfirmed(list, limit = 800) {\n  const out = [];\n  for (const e of list.slice(0, limit)) {\n    const phrase = s(e.phrase).trim();\n    if (!phrase) continue;\n    const sev = s(e.severity).trim() || \"N/A\";\n    const comp = s(e.component).trim() || \"N/A\";\n    const kb = s(e.kb).trim() || \"N/A\";\n    const src = s(e.source).trim() || \"unknown\";\n    const ln = s(e.line).trim() || \"?\";\n    const ev = s(e.evidence).trim().slice(0, 500);\n    out.push(`- phrase=\"${phrase}\" | severity=\"${sev}\" | component=\"${comp}\" | kb=\"${kb}\" | src=\"${src}:${ln}\" | evidence=\"${ev}\"`);\n  }\n  return out.length ? out.join(\"\\n\") : \"- (no confirmed targets)\";\n}\nfunction fmtDgHits(list) {\n  const out = [];\n  for (const h of list.slice(0, 20)) out.push(`- line ${h.line}: ${s(h.evidence).slice(0, 500)}`);\n  return out.length ? out.join(\"\\n\") : \"- (none)\";\n}\n\nconst llm_input =\n`TASK: STORAGE / ERRORS (SECTION 4 ONLY)\n\nOUTPUT RULES (STRICT):\n- Output must contain ONLY:\n  TABLE: Errors\n  ${HEADER}\n  <rows OR N/A>\n- If there are 0 rows: output \"N/A\" on the NEXT line (right after header) and DO NOT print any other rows.\n- NO other text. NO headings like \"1) Parser metadata\".\n- Evidence-first: do NOT invent. Use exact log wording for Message when possible.\n- NEWEST -> OLDEST if timestamps exist.\n- If DOCS CONTEXT contains a KB/manual reference relevant to an error, include it in KB column.\n\nMANDATORY FIRST ROW (IF EVIDENCE EXISTS):\n- If evidence indicates disk group dgA01 is quarantined/Fault, the FIRST row must summarize dgA01 quarantine\n  and list affected disks (enclosure/slot and/or serials) if present in evidence.\n\nCONTEXT (do not output as lines):\nVendor=${vendor}\nClass=${klass}\nModel=${modelHw}\n\nEVENT CATALOG (RAG):\n${catCtx || \"[no EVENT CATALOG (RAG)]\"}\n\nDOCS CONTEXT (RAG):\n${docsCtx || \"[no DOCS CONTEXT (RAG)]\"}\n\nMATCHED TARGETS:\n${fmtConfirmed(confirmed, 1200)}\n\ndgA01 QUICK HITS:\n${fmtDgHits(dgHits)}\n\nRAW EVIDENCE:\n<<<BEGIN EVIDENCE\n${digest || \"[n8n] ERRORS DIGEST EMPTY\"}\nEND EVIDENCE>>>`;\n\nreturn [{\n  json: {\n    ...base,\n    task: \"storage_errors\",\n    taskOrder: 5,\n    model,\n    llm_input,\n    messages: [{ role: \"user\", content: llm_input }],\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2496,
        320
      ],
      "id": "8a6216e9-ec76-4e13-be9d-98cde34d6418",
      "name": "Compose input ERRORS"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        2896,
        -192
      ],
      "id": "e9776c20-580d-4d73-8f3d-576954fb4e8c",
      "name": "Merge 1"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        3312,
        160
      ],
      "id": "b139b702-1103-40d8-9efc-17570a480364",
      "name": "Merge 3"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        3088,
        32
      ],
      "id": "cd56d299-967d-4c61-b882-3b27610a07fd",
      "name": "Merge 2"
    },
    {
      "parameters": {
        "jsCode": "// Prepare prompt — FULL REPLACE v3-B (LOGparser leader-by-file + report meta)\n// - Leader = newest message that HAS a non-image attachment\n// - If no non-image attachment in unprocessed rows -> return []\n// - Adds ReportTimestampMSK + ReportLLM (for final report section 1)\n// - Picks sourceFileId/sourceFileName from leader message (prefer archives)\n\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction toTs(row){ const t = Number(row?.timestamp); return Number.isFinite(t) ? t : 0; }\n\nfunction parseFiles(v){\n  if (!v) return [];\n  if (Array.isArray(v)) return v;\n  if (typeof v === 'string') {\n    try { const j = JSON.parse(v); return Array.isArray(j) ? j : []; } catch { return []; }\n  }\n  return [];\n}\n\nfunction isImageFile(f){\n  const kind = s(f?.kind).toLowerCase();\n  if (kind === 'image') return true;\n  const name = s(f?.name).toLowerCase();\n  return /\\.(png|jpg|jpeg|webp|bmp|tiff|gif)$/i.test(name);\n}\n\nfunction scoreAttachment(f){\n  const name = s(f?.name).toLowerCase();\n  let sc = 0;\n  if (/\\.(zip|tar\\.gz|tgz|tar|gz|7z)$/i.test(name)) sc += 50;\n  if (/\\.(log|logs|txt|csv|xml)$/i.test(name)) sc += 10;\n  const size = Number(f?.size);\n  if (Number.isFinite(size)) sc += Math.min(20, Math.floor(size / 5_000_000)); // +1 per 5MB\n  return sc;\n}\n\nfunction nowMSK(){\n  try {\n    const d = new Date();\n    const dtf = new Intl.DateTimeFormat('en-GB', {\n      timeZone: 'Europe/Moscow',\n      year:'numeric', month:'2-digit', day:'2-digit',\n      hour:'2-digit', minute:'2-digit', second:'2-digit',\n      hour12:false\n    });\n    const parts = dtf.format(d).replace(',', '');\n    const m = parts.match(/^(\\d{2})\\/(\\d{2})\\/(\\d{4})\\s+(\\d{2}):(\\d{2}):(\\d{2})$/);\n    if (!m) return parts + ' (MSK)';\n    return `${m[1]}-${m[2]}-${m[3]} ${m[4]}:${m[5]}:${m[6]} (MSK)`;\n  } catch {\n    const d = new Date(Date.now() + 3*3600*1000);\n    return d.toISOString().replace('T',' ').replace('Z','') + ' (MSK~)';\n  }\n}\n\n// Gather rows (Read messages returns multiple items)\nconst rows = $input.all().map(it => it.json || {});\nif (!rows.length) return [];\n\nrows.sort((a,b) => toTs(a)-toTs(b));\n\nconst withFiles = rows.map(r => {\n  const files = parseFiles(r.files).map(f => ({...f, _msgId: r.id, _msgTimestamp: toTs(r)}));\n  const nonImage = files.filter(f => !isImageFile(f));\n  const hasNonImage = nonImage.length > 0;\n  return { r, files, nonImage, hasNonImage };\n});\n\nconst candidates = withFiles.filter(x => x.hasNonImage);\nif (!candidates.length) return []; // no parsing if no file\n\nconst leader = candidates.sort((a,b) => toTs(a.r)-toTs(b.r)).slice(-1)[0];\nconst leaderRow = leader.r;\n\n// Aggregate all files from all rows (keep for debug)\nconst allFiles = [];\nfor (const x of withFiles) for (const f of x.files) allFiles.push(f);\n\n// Pick source attachment from leader non-image files\nconst leaderNon = leader.nonImage.slice().sort((a,b)=>scoreAttachment(b)-scoreAttachment(a));\nconst main = leaderNon[0] || null;\n\nconst ReportTimestampMSK = nowMSK();\nconst ReportLLM = (s($env.LOGPARSER_REPORT_LLM).trim() || 'qwen2.5:32b-instruct-q8_0');\n\nreturn [{\n  json: {\n    id: leaderRow.id,\n    chatId: leaderRow.chatId ?? null,\n    login: leaderRow.login ?? null,\n\n    files: allFiles,\n    hasFile: allFiles.length > 0,\n    hasImage: allFiles.some(isImageFile),\n    hasNonImage: allFiles.some(f => !isImageFile(f)),\n\n    // selected attachment for all downstream extractors\n    sourceFileId: main ? (main.file_id || main.id || '') : '',\n    sourceFileName: main ? (main.name || '') : '',\n\n    // report metadata\n    ReportTimestampMSK,\n    ReportLLM,\n\n    // routing\n    wantsLogParsing: true,\n    wantsTxt: true,\n    lastUserText: '',\n\n    // leader flags (single item anyway)\n    isLeader: true,\n    myId: leaderRow.id,\n    leaderId: leaderRow.id,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -336,
        32
      ],
      "id": "82956e5f-e22f-4c4d-9785-a3a366c1468b",
      "name": "Prepare prompt"
    },
    {
      "parameters": {
        "jsCode": "// Build Identify Pack — FULL REPLACE v5-FIX-SPREAD (resumable getFile + full unpack + evidence)\n// Only changes vs your current: FIX all \".base\" -> \"...base\" and keep download stable.\n// No vendor/class hacks.\n\nconst https = require('https');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\nconst { execFileSync } = require('child_process');\n\nconst base = $input.item.json || {};\nconst s = (x) => (x == null ? '' : String(x));\n\nconst MODEL_IDENTIFY = s($env.MODEL_IDENTIFY).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst MAX_EVID_CHARS = Number($env.IDENTIFY_EVID_MAX_CHARS || 220000);\n\nconst MAX_DOWNLOAD_ATTEMPTS = Number($env.GETFILE_MAX_ATTEMPTS || 80);\nconst REQ_TIMEOUT_MS = Number($env.GETFILE_REQ_TIMEOUT_MS || 60 * 60 * 1000);\nconst BACKOFF_MS = Number($env.GETFILE_BACKOFF_MS || 800);\n\nfunction ensureDir(p){ fs.mkdirSync(p, { recursive:true }); }\nfunction sleep(ms){ return new Promise(r => setTimeout(r, ms)); }\n\nfunction getToken(){\n  const t2 = s($env?.YANDEX_BOT_TOKEN_LOGPARSER).trim();\n  return t2 || s($env?.YANDEX_BOT_TOKEN).trim();\n}\n\nfunction readFirstBytes(fp, n=700){\n  const fd = fs.openSync(fp,'r');\n  try {\n    const buf = Buffer.allocUnsafe(n);\n    const r = fs.readSync(fd, buf, 0, n, 0);\n    return buf.slice(0,r);\n  } finally { try { fs.closeSync(fd); } catch {} }\n}\n\nfunction isZipMagicBuf(b){\n  return b && b.length >= 4 && b[0]===0x50 && b[1]===0x4b && b[2]===0x03 && b[3]===0x04;\n}\n\nfunction zipHasEOCD(fp){\n  try {\n    const st = fs.statSync(fp);\n    const size = st.size || 0;\n    if (size < 64) return false;\n    const tailSize = Math.min(70000, size);\n    const fd = fs.openSync(fp,'r');\n    try {\n      const buf = Buffer.allocUnsafe(tailSize);\n      fs.readSync(fd, buf, 0, tailSize, size - tailSize);\n      for (let i = buf.length - 22; i >= 0; i--) {\n        if (buf[i]===0x50 && buf[i+1]===0x4b && buf[i+2]===0x05 && buf[i+3]===0x06) return true;\n      }\n      return false;\n    } finally { try { fs.closeSync(fd); } catch {} }\n  } catch { return false; }\n}\n\nfunction extractJsonErr(text){\n  try {\n    const obj = JSON.parse(String(text||''));\n    if (obj && obj.ok === false) return obj.description || obj.code || 'getFile failed';\n  } catch {}\n  return null;\n}\n\nfunction requestRangeToFile(fileId, token, outPath, offset){\n  return new Promise((resolve, reject) => {\n    const p = `/bot/v1/messages/getFile/?file_id=${encodeURIComponent(fileId)}`;\n    const headers = {\n      'Authorization': `OAuth ${token}`,\n      'Accept-Encoding': 'identity',\n      'Connection': 'close',\n    };\n    if (offset > 0) headers['Range'] = `bytes=${offset}-`;\n\n    const req = https.request(\n      { method:'GET', hostname:'botapi.messenger.yandex.net', path: p, headers, timeout: REQ_TIMEOUT_MS },\n      (res) => {\n        const status = res.statusCode|0;\n\n        if (offset > 0 && status === 200) {\n          res.resume();\n          return resolve({ restart:true, status, bytes:0, headBuf: Buffer.alloc(0), headers: res.headers || {} });\n        }\n\n        const ws = fs.createWriteStream(outPath, { flags: offset > 0 ? 'a' : 'w' });\n        let bytes = 0;\n        const headChunks = [];\n        let headLen = 0;\n\n        res.on('data', (chunk) => {\n          const b = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk);\n          bytes += b.length;\n          if (headLen < 4096) {\n            const take = Math.min(4096 - headLen, b.length);\n            if (take > 0) headChunks.push(b.slice(0, take));\n            headLen += take;\n          }\n        });\n\n        res.on('aborted', () => reject(new Error('aborted')));\n        res.on('error', reject);\n\n        res.pipe(ws);\n\n        ws.on('finish', () => {\n          const headBuf = Buffer.concat(headChunks).slice(0, 4096);\n          resolve({ restart:false, status, bytes, headBuf, headers: res.headers || {} });\n        });\n        ws.on('error', reject);\n      }\n    );\n\n    req.on('timeout', () => { try { req.destroy(new Error('aborted')); } catch {} });\n    req.on('error', reject);\n    req.end();\n  });\n}\n\nasync function downloadWithResume(fileId, token, outPath, nameLower){\n  try { fs.rmSync(outPath, { force:true }); } catch {}\n  let total = null;\n\n  for (let attempt=1; attempt<=MAX_DOWNLOAD_ATTEMPTS; attempt++){\n    const offset = fs.existsSync(outPath) ? fs.statSync(outPath).size : 0;\n\n    try {\n      const r = await requestRangeToFile(fileId, token, outPath, offset);\n\n      if (r.restart) {\n        try { fs.rmSync(outPath, { force:true }); } catch {}\n        continue;\n      }\n\n      const maybeErr = extractJsonErr(r.headBuf.toString('utf8'));\n      if (maybeErr) throw new Error(maybeErr);\n\n      if (!((r.status>=200 && r.status<300) || r.status===206)) throw new Error(`HTTP ${r.status}`);\n\n      const cr = s(r.headers['content-range']).trim();\n      if (cr) {\n        const m = cr.match(/bytes\\s+\\d+-\\d+\\/(\\d+|\\*)/i);\n        if (m && m[1] && m[1] !== '*') total = Number(m[1]);\n      } else if (offset === 0) {\n        const clen = Number(r.headers['content-length']);\n        if (Number.isFinite(clen) && clen > 0) total = clen;\n      }\n\n      const newSize = fs.existsSync(outPath) ? fs.statSync(outPath).size : 0;\n\n      if (total != null && newSize >= total) break;\n      if ((nameLower.endsWith('.zip') || isZipMagicBuf(readFirstBytes(outPath,4))) && zipHasEOCD(outPath)) break;\n      if (r.bytes === 0) break;\n\n      await sleep(Math.min(15000, BACKOFF_MS * attempt));\n    } catch {\n      await sleep(Math.min(20000, BACKOFF_MS * attempt));\n    }\n  }\n\n  const head = readFirstBytes(outPath, 4);\n  if (isZipMagicBuf(head) && !zipHasEOCD(outPath)) throw new Error('download incomplete: zip EOCD not found');\n}\n\nfunction hasCmd(cmd){\n  try { execFileSync('which', [cmd], { stdio:'ignore' }); return true; } catch { return false; }\n}\n\nfunction sniffTypeByMagic(filePath, nameLower){\n  const b = readFirstBytes(filePath, 520);\n  if (b.length>=4 && b[0]===0x50 && b[1]===0x4b && b[2]===0x03 && b[3]===0x04) return 'zip';\n  if (b.length>=3 && b[0]===0x42 && b[1]===0x5a && b[2]===0x68) return (nameLower.endsWith('.tbz2')||nameLower.endsWith('.tar.bz2')||nameLower.endsWith('.tbz'))?'tarbz2':'bz2';\n  if (b.length>=6 && b[0]===0xfd && b[1]===0x37 && b[2]===0x7a && b[3]===0x58 && b[4]===0x5a && b[5]===0x00) return (nameLower.endsWith('.tar.xz')||nameLower.endsWith('.txz'))?'tarxz':'xz';\n  if (b.length>=6 && b[0]===0x37 && b[1]===0x7a && b[2]===0xbc && b[3]===0xaf && b[4]===0x27 && b[5]===0x1c) return '7z';\n  if (b.length>=2 && b[0]===0x1f && b[1]===0x8b) return (nameLower.endsWith('.tar.gz')||nameLower.endsWith('.tgz'))?'targz':'gz';\n  if (b.length>=262 && b.slice(257,262).toString('utf8')==='ustar') return 'tar';\n  return 'plain';\n}\n\nfunction unpackToDir(inFile, type, outDir){\n  ensureDir(outDir);\n\n  if (type === 'zip') {\n    if (hasCmd('unzip')) { execFileSync('unzip', ['-qq', inFile, '-d', outDir]); return; }\n    if (hasCmd('bsdtar')) { execFileSync('bsdtar', ['-xf', inFile, '-C', outDir]); return; }\n    if (hasCmd('7z')) { execFileSync('7z', ['x','-y',`-o${outDir}`, inFile], { stdio:'ignore' }); return; }\n    throw new Error('No unzip/bsdtar/7z found');\n  }\n\n  if (type === 'tar')   { execFileSync('tar', ['-xf',  inFile, '-C', outDir]); return; }\n  if (type === 'targz') { execFileSync('tar', ['-xzf', inFile, '-C', outDir]); return; }\n  if (type === 'tarbz2'){ execFileSync('tar', ['-xjf', inFile, '-C', outDir]); return; }\n  if (type === 'tarxz') { execFileSync('tar', ['-xJf', inFile, '-C', outDir]); return; }\n\n  if (type === 'gz')  { execFileSync('bash', ['-lc', `gzip -dc \"${inFile}\" > \"${path.join(outDir,'file_from_gz')}\"`]); return; }\n  if (type === 'bz2') { execFileSync('bash', ['-lc', `bzip2 -dc \"${inFile}\" > \"${path.join(outDir,'file_from_bz2')}\"`]); return; }\n  if (type === 'xz')  { execFileSync('bash', ['-lc', `xz -dc \"${inFile}\" > \"${path.join(outDir,'file_from_xz')}\"`]); return; }\n  if (type === '7z')  { if(!hasCmd('7z')) throw new Error('7z not found'); execFileSync('7z', ['x','-y',`-o${outDir}`, inFile], { stdio:'ignore' }); return; }\n\n  fs.copyFileSync(inFile, path.join(outDir, path.basename(inFile) || 'file.bin'));\n}\n\nfunction walkFiles(root){\n  const out = [];\n  const stack = [root];\n  while (stack.length) {\n    const p = stack.pop();\n    let st;\n    try { st = fs.statSync(p); } catch { continue; }\n    if (st.isDirectory()) {\n      let names = [];\n      try { names = fs.readdirSync(p); } catch { names = []; }\n      for (const name of names) stack.push(path.join(p, name));\n    } else if (st.isFile()) out.push(p);\n  }\n  return out;\n}\n\nfunction looksBinaryByHead(buf){\n  if (!buf || !buf.length) return false;\n  return buf.slice(0, Math.min(buf.length, 65536)).includes(0);\n}\n\nfunction decodeSmart(buf){\n  if (!Buffer.isBuffer(buf)) buf = Buffer.from(String(buf||''), 'utf8');\n  if (buf.length >= 2 && buf[0]===0xff && buf[1]===0xfe) return buf.slice(2).toString('utf16le');\n  if (looksBinaryByHead(buf)) return '';\n  return buf.toString('utf8');\n}\n\nfunction clipText(t, n){\n  t = s(t);\n  return t.length <= n ? t : (t.slice(0, n) + '\\n[n8n] TRUNCATED');\n}\n\nfunction collectKeyLines(text){\n  const lines = s(text).split(/\\r?\\n/);\n  const out = [];\n  const reList = [\n    /Vendor\\s*Name\\s*:\\s*.+/i,\n    /Product\\s*(ID|Name|Brand)?\\s*:\\s*.+/i,\n    /\\bModel\\b\\s*:\\s*.+/i,\n    /\\bSerial\\b\\s*:\\s*.+/i,\n    /\\bME\\d{3,5}\\b/i,\n    /\\bDELL EMC\\b/i,\n    /\\bUNITY\\b/i,\n    /\\bVNX\\b/i,\n    /\\b3PAR\\b/i,\n    /\\bMSA\\b/i,\n    /\\bFLASH\\s*SYSTEM\\b/i,\n    /\\bSTORWIZE\\b/i,\n    /\\bOCEANSTOR\\b/i,\n    /\\bDORADO\\b/i,\n  ];\n  for (const ln of lines) {\n    const t = ln.trim();\n    if (!t) continue;\n    if (reList.some(re => re.test(t))) out.push(t);\n    if (out.length >= 80) break;\n  }\n  return out;\n}\n\nconst fileId = s(base.sourceFileId).trim();\nconst fileName = s(base.sourceFileName).trim() || 'attachment.bin';\nif (!fileId) throw new Error('sourceFileId is empty');\n\nconst token = getToken();\nif (!token) throw new Error('YANDEX_BOT_TOKEN_LOGPARSER/YANDEX_BOT_TOKEN is not set');\n\nconst tmp = fs.mkdtempSync(path.join(os.tmpdir(), 'identify-'));\nconst inFile = path.join(tmp, 'in.bin');\nconst outDir = path.join(tmp, 'out');\nensureDir(outDir);\n\ntry {\n  await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n\n  const type = sniffTypeByMagic(inFile, fileName.toLowerCase());\n\n  // unzip retry if short read\n  let lastErr = '';\n  for (let k=1; k<=3; k++){\n    try {\n      try { fs.rmSync(outDir, { recursive:true, force:true }); } catch {}\n      ensureDir(outDir);\n      unpackToDir(inFile, type, outDir);\n      lastErr = '';\n      break;\n    } catch (e) {\n      lastErr = s(e?.message || e);\n      if (type === 'zip' && /short read/i.test(lastErr)) {\n        await downloadWithResume(fileId, token, inFile, fileName.toLowerCase());\n        await sleep(400*k);\n        continue;\n      }\n      break;\n    }\n  }\n  if (lastErr) throw new Error(lastErr);\n\n  const files = walkFiles(outDir);\n  const inv = files.slice(0, 200).map(fp => {\n    try { return `${path.relative(outDir, fp)} (${fs.statSync(fp).size} bytes)`; }\n    catch { return `${path.relative(outDir, fp)} (?)`; }\n  });\n\n  // take a few best text-like files\n  const scored = files.map(fp => {\n    const n = path.basename(fp).toLowerCase();\n    let sc = 0;\n    if (/^store_.*\\.logs$/.test(n)) sc += 500;\n    if (/\\.(logs|log|txt|out|xml|json|csv)$/i.test(n)) sc += 50;\n    let sz = 0;\n    try { sz = fs.statSync(fp).size || 0; } catch {}\n    sc += Math.min(30, Math.floor(sz / 50_000_000));\n    return { fp, sc, sz };\n  }).sort((a,b)=> (b.sc-a.sc) || (b.sz-a.sz)).slice(0, 3);\n\n  const keyLinesAll = [];\n  const snips = [];\n\n  for (const it of scored) {\n    let buf = Buffer.alloc(0);\n    try { buf = fs.readFileSync(it.fp); } catch { buf = Buffer.alloc(0); }\n    const txt = decodeSmart(buf.slice(0, 500000));\n    const rel = path.relative(outDir, it.fp);\n    if (txt) {\n      snips.push(`=== TEXT SNIP: ${rel} ===\\n` + txt);\n      keyLinesAll.push(...collectKeyLines(txt));\n    }\n  }\n\n  let identify_evidence_text =\n`ARCHIVE: ${fileName}\nTYPE: ${type}\nFILES: ${files.length}\n\n=== INVENTORY (first ${inv.length}) ===\n${inv.join('\\n')}\n\n=== KEY LINES (merged) ===\n${keyLinesAll.length ? keyLinesAll.slice(0,120).join('\\n') : 'N/A'}\n\n${snips.length ? snips.join('\\n\\n') : ''}`.trim();\n\n  identify_evidence_text = clipText(identify_evidence_text, MAX_EVID_CHARS);\n\n  const rag_query_seed = (keyLinesAll.join(' ') || fileName).replace(/\\s+/g,' ').trim().slice(0, 380);\n\n  const llm_input =\n`TASK: IDENTIFY (LLM1)\n\nReturn ONLY valid JSON (no markdown) with fields:\n{\n  \"vendor\": \"...\",\n  \"class\": \"Storage|Server|Switch|Other\",\n  \"model\": \"...\",\n  \"product_name\": \"...\",\n  \"confidence\": 0.0-1.0,\n  \"evidence\": { \"vendor\":[...], \"class\":[...], \"model\":[...] }\n}\n\nRULES:\n- Evidence-first. Do NOT invent.\n- If unsure -> \"N/A\".\n\nEVIDENCE:\n<<<BEGIN\n${identify_evidence_text}\nEND>>>`;\n\n  return [{\n    json: {\n      ...base,\n      task: 'identify',\n      model: MODEL_IDENTIFY,\n      identify_evidence_text,\n      rag_query_seed,\n      messages: [{ role:'user', content: llm_input }],\n    }\n  }];\n\n} finally {\n  try { fs.rmSync(tmp, { recursive:true, force:true }); } catch {}\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        544,
        32
      ],
      "id": "31217512-fdbd-4b8a-9016-e6d14192755d",
      "name": "Build Identify Pack"
    },
    {
      "parameters": {
        "jsCode": "// Parse Identify Result — FULL REPLACE v2-FIX-SPREAD\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nfunction extractAssistantText(j){\n  if (typeof j?.message?.content === 'string') return j.message.content;\n  if (typeof j?.ollama?.message?.content === 'string') return j.ollama.message.content;\n  if (typeof j?.response === 'string') return j.response;\n  if (typeof j?.text === 'string') return j.text;\n  return '';\n}\nfunction stripFences(t){\n  t = s(t).trim();\n  const m = t.match(/^```(?:json)?\\s*([\\s\\S]*?)\\s*```$/i);\n  return m ? m[1].trim() : t;\n}\nfunction safeJsonParse(txt){\n  const t = stripFences(txt);\n  try { return JSON.parse(t); } catch {}\n  const m = t.match(/\\{[\\s\\S]*\\}/);\n  if (m) { try { return JSON.parse(m[0]); } catch {} }\n  return null;\n}\nfunction normVendor(v){\n  const u = s(v).trim();\n  const up = u.toUpperCase();\n  if (!u || u === 'N/A') return 'N/A';\n  if (up.includes('DELL EMC')) return 'Dell EMC';\n  if (up === 'DELL') return 'Dell EMC';\n  if (up.includes('HPE') || up.includes('HEWLETT')) return 'HPE';\n  if (up.includes('LENOVO')) return 'Lenovo';\n  if (up.includes('IBM')) return 'IBM';\n  if (up.includes('NETAPP')) return 'NetApp';\n  if (up.includes('CISCO')) return 'Cisco';\n  if (up.includes('BROCADE')) return 'Brocade';\n  if (up.includes('HUAWEI')) return 'Huawei';\n  return u;\n}\nfunction normClass(c){\n  const u = s(c).trim();\n  const low = u.toLowerCase();\n  if (!u || u === 'N/A') return 'N/A';\n  if (low.startsWith('stor') || low === 'starage') return 'Storage';\n  if (low.startsWith('serv')) return 'Server';\n  if (low.startsWith('swit')) return 'Switch';\n  if (low.startsWith('oth')) return 'Other';\n  return u;\n}\n\nconst raw = extractAssistantText(base);\nconst obj = safeJsonParse(raw) || {};\n\nlet vendor = normVendor(obj.vendor || '');\nlet klass  = normClass(obj.class || obj.klass || '');\nlet model  = s(obj.model || '').trim() || 'N/A';\nlet product_name = s(obj.product_name || '').trim() || 'N/A';\nlet confidence = Number(obj.confidence);\nif (!Number.isFinite(confidence)) confidence = 0.0;\n\n// optional RAG override\nconst ragBest = base.identify_rag_best;\nif (ragBest && typeof ragBest === 'object') {\n  const rbVendor = normVendor(ragBest.vendor || '');\n  const rbClass  = normClass(ragBest.class || ragBest.klass || '');\n  const rbModel  = s(ragBest.model || '').trim();\n  if (vendor === 'N/A' && rbVendor && rbVendor !== 'N/A') vendor = rbVendor;\n  if ((klass === 'N/A' || klass === 'Other') && rbClass && rbClass !== 'N/A') klass = rbClass;\n  if ((model === 'N/A' || !model) && rbModel) model = rbModel;\n}\n\nreturn [{\n  json: {\n    ...base,\n    vendor,\n    klass,\n    modelDetected: model,\n    productNameDetected: product_name,\n    confidence,\n    hw_vendor: vendor,\n    hw_class: klass,\n    hw_model: model,\n    hw_product_name: product_name,\n    identify_raw: raw,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1248,
        32
      ],
      "id": "cd536967-63a5-4648-a83a-020200dbbe73",
      "name": "Parse Identify Result"
    },
    {
      "parameters": {
        "jsCode": "// LLM Request Identify — FULL REPLACE v3-B (Ollama chat, Qwen defaults + num_ctx 131072)\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || String($env.MODEL_IDENTIFY || 'qwen2.5:32b-instruct-q8_0').trim();\n\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role:'user', content:'' }];\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: {\n      model,\n      stream: false,\n      messages,\n      options: {\n        temperature: 0,\n        top_k: 1,\n        top_p: 1,\n        num_ctx: Number($env.IDENTIFY_NUM_CTX || 131072),\n        num_predict: Number($env.IDENTIFY_NUM_PREDICT || 600),\n      }\n    },\n    timeout: 3600000,\n  });\n\n  const content =\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (resp && resp.response && typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp);\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama: resp,\n      message: { role:'assistant', content },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 2000) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role:'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1008,
        32
      ],
      "id": "3ed11c7d-6043-4a6f-aad6-cc7b784fae49",
      "name": "LLM Request Identify"
    },
    {
      "parameters": {
        "jsCode": "// Ingest Attachments (Universal) — FULL REPLACE v2-B-FIX5\n// FIX: .base -> ...base\n\nfunction s(x){ return x == null ? '' : String(x); }\nconst base = $input.item.json || {};\n\nfunction isImageFile(f){\n  const kind = s(f?.kind).toLowerCase();\n  if (kind === 'image') return true;\n  const name = s(f?.name).toLowerCase();\n  return /\\.(png|jpg|jpeg|webp|bmp|tiff|gif)$/i.test(name);\n}\nfunction scoreAttachment(f){\n  const name = s(f?.name).toLowerCase();\n  let sc = 0;\n  if (/\\.(zip|tar\\.gz|tgz|tar|gz|7z)$/i.test(name)) sc += 50;\n  if (/\\.(log|logs|txt|csv|xml)$/i.test(name)) sc += 10;\n  const size = Number(f?.size);\n  if (Number.isFinite(size)) sc += Math.min(20, Math.floor(size / 5_000_000));\n  return sc;\n}\n\nlet sourceFileId = s(base.sourceFileId).trim();\nlet sourceFileName = s(base.sourceFileName).trim();\n\nif ((!sourceFileId || !sourceFileName) && Array.isArray(base.files)) {\n  const non = base.files\n    .filter(f => !isImageFile(f))\n    .slice()\n    .sort((a,b)=>scoreAttachment(b)-scoreAttachment(a));\n  const main = non[0];\n  if (main) {\n    sourceFileId = sourceFileId || s(main.file_id || main.id).trim();\n    sourceFileName = sourceFileName || s(main.name).trim();\n  }\n}\n\nreturn [{\n  json: {\n    ...base,\n    sourceFileId,\n    sourceFileName,\n  }\n}];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        320,
        32
      ],
      "id": "79978435-6228-4c3d-a146-a1ef17ea91a1",
      "name": "Ingest Attachments (Universal)"
    },
    {
      "parameters": {
        "jsCode": "// RAG Identify Lookup (offline) — FULL REPLACE v2-FIX-SPREAD\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst RAG_URL = (String($env.RAG_API_URL || 'http://rag-api:8099')).replace(/\\/$/, '');\nconst TOKEN = String($env.RAG_API_TOKEN || '').trim();\nconst TOP_K = Number($env.RAG_IDENTIFY_TOP_K || 25);\n\nfunction guessVendor(text){\n  const t = (text||'').toUpperCase();\n  if (t.includes('DELL EMC') || t.includes('DELL')) return 'Dell EMC';\n  if (t.includes('HPE') || t.includes('HEWLETT')) return 'HPE';\n  if (t.includes('LENOVO')) return 'Lenovo';\n  if (t.includes('NETAPP')) return 'NetApp';\n  if (t.includes('CISCO')) return 'Cisco';\n  if (t.includes('BROCADE')) return 'Brocade';\n  if (t.includes('HUAWEI')) return 'Huawei';\n  if (t.includes('IBM')) return 'IBM';\n  return 'N/A';\n}\nfunction guessKlass(_text){ return 'Other'; }\nfunction clip(x, n){ x=s(x); return x.length<=n?x:x.slice(0,n); }\n\nconst evidence = s(base.identify_evidence_text);\nconst vendor = guessVendor(evidence);\nconst klass = guessKlass(evidence);\nconst query = clip((s(base.rag_query_seed).trim() || evidence.replace(/\\s+/g,' ').trim()), 380);\n\nif (!vendor || vendor === 'N/A') {\n  return [{\n    json: {\n      ...base,\n      identify_rag_best: null,\n      identify_rag_hints: '',\n      _rag_identify_error: 'vendor_missing',\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n    }\n  }];\n}\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${RAG_URL}/search/all`,\n    json: true,\n    timeout: 30000,\n    headers: {\n      ...(TOKEN ? { Authorization: TOKEN.startsWith('Bearer ') ? TOKEN : `Bearer ${TOKEN}` } : {}),\n      'Content-Type': 'application/json',\n    },\n    body: { query, vendor, klass, top_k: TOP_K },\n  });\n\n  const cat = Array.isArray(resp?.event_catalog) ? resp.event_catalog : (Array.isArray(resp?.hits?.event_catalog) ? resp.hits.event_catalog : []);\n  let best = null;\n  for (const h of cat) {\n    const m = s(h.model).trim();\n    const c = s(h.class || h.klass).trim();\n    if (!m && !c) continue;\n    best = { vendor: s(h.vendor).trim() || vendor, class: c || klass, model: m || 'N/A', score: h.score, rel_path: s(h.rel_path||h.file_name||'') };\n    break;\n  }\n\n  return [{\n    json: {\n      ...base,\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n      identify_rag_best: best,\n      identify_rag_hints: best ? `BEST: vendor=\"${best.vendor}\" class=\"${best.class}\" model=\"${best.model}\"` : '',\n    }\n  }];\n\n} catch (e) {\n  return [{\n    json: {\n      ...base,\n      rag_identify_query: query,\n      rag_identify_vendor: vendor,\n      rag_identify_klass: klass,\n      identify_rag_best: null,\n      identify_rag_hints: '',\n      _rag_identify_error: s(e?.message || e),\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        752,
        32
      ],
      "id": "b0f1bd6a-61ff-4e3b-bc04-5ac13fa536d6",
      "name": "RAG Identify Lookup (offline)"
    },
    {
      "parameters": {
        "jsCode": "// LLM Request Storage Structure — FULL REPLACE v3-B-FIX5\n// FIX: \".base\" -> \"...base\" (catch), Qwen default, num_ctx=131072\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role: 'user', content: '' }];\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: {\n      model,\n      stream: false,\n      messages,\n      options: {\n        temperature: 0,\n        top_k: 1,\n        top_p: 1,\n        num_ctx: Number($env.STORAGE_NUM_CTX || 131072),\n        num_predict: Number($env.STORAGE_NUM_PREDICT || 3500),\n      },\n    },\n    timeout: 3600000,\n  });\n\n  const content =\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp);\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama: resp,\n      message: { role: 'assistant', content: String(content || '').trim() },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 2000) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role: 'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2640,
        -256
      ],
      "id": "260b709e-cbca-4e5d-82a3-535c0d36204e",
      "name": "LLM Request Storage Structure"
    },
    {
      "parameters": {
        "jsCode": "// LLM Request Storage Disk — FULL REPLACE v3-B-FIX9\n// FIX: .base -> ...base in catch; do NOT store full resp (memory)\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role: 'user', content: '' }];\n\nfunction pickMeta(resp){\n  const o = resp || {};\n  return {\n    model: o.model,\n    created_at: o.created_at,\n    done: o.done,\n    total_duration: o.total_duration,\n    load_duration: o.load_duration,\n    prompt_eval_count: o.prompt_eval_count,\n    prompt_eval_duration: o.prompt_eval_duration,\n    eval_count: o.eval_count,\n    eval_duration: o.eval_duration,\n  };\n}\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: {\n      model,\n      stream: false,\n      messages,\n      options: {\n        temperature: 0,\n        top_k: 1,\n        top_p: 1,\n        num_ctx: Number($env.STORAGE_NUM_CTX || 131072),\n        num_predict: Number($env.STORAGE_NUM_PREDICT || 3500),\n      },\n    },\n    timeout: 3600000,\n  });\n\n  const content =\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp);\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama_meta: pickMeta(resp),\n      message: { role: 'assistant', content: String(content || '').trim() },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 1200) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role: 'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2640,
        -64
      ],
      "id": "fcec32b4-6f92-4735-9ea6-4f27693cdc37",
      "name": "LLM Request Storage Disk"
    },
    {
      "parameters": {
        "jsCode": "// LLM Request Storage Errors — FULL REPLACE v7-B-FIX9\n// FIX: syntax + memory (не сохраняем resp), строгая валидация TABLE+HEADER, 1 repair попытка\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role: 'user', content: '' }];\n\nconst OPTS = {\n  temperature: 0,\n  top_k: 1,\n  top_p: 1,\n  num_ctx: Number($env.STORAGE_NUM_CTX || 131072),\n  num_predict: Number($env.STORAGE_NUM_PREDICT || 3500),\n};\n\nconst HEADER = 'Timestamp;Severity;Component;CodeOrPhrase;Message;Source;KB';\n\nfunction pickMeta(resp){\n  const o = resp || {};\n  return {\n    model: o.model,\n    created_at: o.created_at,\n    done: o.done,\n    total_duration: o.total_duration,\n    load_duration: o.load_duration,\n    prompt_eval_count: o.prompt_eval_count,\n    prompt_eval_duration: o.prompt_eval_duration,\n    eval_count: o.eval_count,\n    eval_duration: o.eval_duration,\n  };\n}\n\nfunction extractContent(resp){\n  return (\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (resp && typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp)\n  );\n}\n\nfunction normalizeLines(txt){\n  return s(txt).replace(/\\r/g,'').split('\\n').map(x=>x.trim()).filter(Boolean);\n}\n\nfunction isValidErrorsTable(txt){\n  const lines = normalizeLines(txt);\n  if (!lines.length) return false;\n\n  const idx = lines.findIndex(l => /^TABLE:\\s*Errors$/i.test(l));\n  if (idx < 0) return false;\n\n  const next = lines[idx + 1] || '';\n  if (next.toLowerCase() === 'n/a') return true;\n\n  if (next.replace(/\\s+/g,'').toLowerCase() !== HEADER.replace(/\\s+/g,'').toLowerCase()) return false;\n\n  const row = lines[idx + 2] || '';\n  return row.toLowerCase() === 'n/a' || row.includes(';');\n}\n\nasync function callOllama(msgs){\n  return await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: { model, stream: false, messages: msgs, options: OPTS },\n    timeout: 3600000,\n  });\n}\n\ntry {\n  const resp1 = await callOllama(messages);\n  let out1 = String(extractContent(resp1) || '').trim();\n\n  if (!isValidErrorsTable(out1)) {\n    const basePrompt = s(base.llm_input || messages?.[0]?.content || '').trim();\n    const bad = out1.slice(0, 12000);\n\n    const repair =\n`${basePrompt}\n\n[INVALID OUTPUT RECEIVED - DO NOT REPEAT IT]\n${bad}\n\nREPAIR (STRICT):\nOutput ONLY:\nTABLE: Errors\n${HEADER}\n<rows OR N/A>\nNo other text.`;\n\n    const resp2 = await callOllama([{ role: 'user', content: repair }]);\n    const out2 = String(extractContent(resp2) || '').trim();\n\n    if (isValidErrorsTable(out2)) {\n      return [{\n        json: {\n          ...base,\n          model,\n          ollama_meta: pickMeta(resp2),\n          message: { role: 'assistant', content: out2 },\n          _repaired: true,\n        }\n      }];\n    }\n\n    out1 = `TABLE: Errors\\n${HEADER}\\nN/A`;\n  }\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama_meta: pickMeta(resp1),\n      message: { role: 'assistant', content: out1 },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 1200) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role: 'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2784,
        320
      ],
      "id": "dd9a90f5-7312-4100-b38e-d1d32e851d97",
      "name": "LLM Request Storage Errors"
    },
    {
      "parameters": {
        "jsCode": "// 4) LLM Request Storage Periphery — FULL REPLACE v3-B-FIX7\n// FIX: \".base\" in catch -> \"...base\"\n\nconst base = $input.item.json || {};\nfunction s(x){ return x == null ? '' : String(x); }\n\nconst OLLAMA_URL = (String($env.OLLAMA_URL || 'http://10.10.200.9:11434')).replace(/\\/$/, '');\nconst model = s(base.model).trim() || 'qwen2.5:32b-instruct-q8_0';\nconst messages = Array.isArray(base.messages) ? base.messages : [{ role: 'user', content: '' }];\n\ntry {\n  const resp = await this.helpers.httpRequest({\n    method: 'POST',\n    url: `${OLLAMA_URL}/api/chat`,\n    json: true,\n    body: {\n      model,\n      stream: false,\n      messages,\n      options: {\n        temperature: 0,\n        top_k: 1,\n        top_p: 1,\n        num_ctx: Number($env.STORAGE_NUM_CTX || 131072),\n        num_predict: Number($env.STORAGE_NUM_PREDICT || 3500),\n      },\n    },\n    timeout: 3600000,\n  });\n\n  const content =\n    (resp && resp.message && typeof resp.message.content === 'string') ? resp.message.content :\n    (typeof resp.response === 'string') ? resp.response :\n    JSON.stringify(resp);\n\n  return [{\n    json: {\n      ...base,\n      model,\n      ollama: resp,\n      message: { role: 'assistant', content: String(content || '').trim() },\n    }\n  }];\n\n} catch (e) {\n  const status = e?.response?.status || e?.statusCode || '';\n  const data = e?.response?.data ? JSON.stringify(e.response.data).slice(0, 2000) : '';\n  return [{\n    json: {\n      ...base,\n      model,\n      message: { role: 'assistant', content: `[LLM ERROR] status=${status} ${e?.message || e}` },\n      _llm_error: true,\n      _llm_error_status: status,\n      _llm_error_data: data,\n    }\n  }];\n}\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2640,
        128
      ],
      "id": "411dcfff-e5a3-49fb-96b2-59203622321b",
      "name": "LLM Request Storage Periphery"
    }
  ],
  "pinData": {
    "Webhook": [
      {
        "json": {
          "headers": {
            "connection": "upgrade",
            "host": "n8n.itcost.ru",
            "x-real-ip": "10.10.200.1",
            "x-forwarded-for": "10.10.200.1",
            "x-forwarded-proto": "https",
            "content-length": "375",
            "user-agent": "Mozilla/5.0 (compatible; YandexUserproxy; robot; +http://yandex.com/bots)",
            "accept-encoding": "gzip, x-gzip, deflate",
            "content-type": "application/json; charset=UTF-8"
          },
          "params": {},
          "query": {},
          "body": {
            "updates": [
              {
                "message_id": 1763730909592004,
                "timestamp": 1763730909,
                "chat": {
                  "type": "private"
                },
                "from": {
                  "id": "360eed3e-84b1-40cd-8af5-f9042df164ad",
                  "display_name": "Кузин Евгений",
                  "login": "kuzin@itcost.ru",
                  "robot": false
                },
                "update_id": 1763730909592004,
                "text": "Как сам?"
              }
            ]
          },
          "webhookUrl": "http://localhost:5678/webhook/Assistbot",
          "executionMode": "production"
        }
      }
    ]
  },
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Extract message",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract message": {
      "main": [
        [
          {
            "node": "Data message",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare reply": {
      "main": [
        [
          {
            "node": "If file",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Data message": {
      "main": [
        [
          {
            "node": "Read messages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Read messages": {
      "main": [
        [
          {
            "node": "Prepare prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update row(s)": {
      "main": [
        []
      ]
    },
    "If  leader": {
      "main": [
        [
          {
            "node": "Update row(s)",
            "type": "main",
            "index": 0
          },
          {
            "node": "Ingest Attachments (Universal)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Stop",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Send Text": {
      "main": [
        []
      ]
    },
    "If file": {
      "main": [
        [
          {
            "node": "Send file",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Send Text",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Send file": {
      "main": [
        []
      ]
    },
    "Stop": {
      "main": [
        []
      ]
    },
    "IF klass == \"Storage\"": {
      "main": [
        [
          {
            "node": "Storage Structure Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Disks Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Periphery Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Error Candidates",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Storage Error Candidates",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Periphery Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Disks Extract",
            "type": "main",
            "index": 0
          },
          {
            "node": "Storage Structure Extract",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Structure Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Structure input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Disks Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Disk input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Periphery Extract": {
      "main": [
        [
          {
            "node": "Compose Storage Periphery input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Storage Error Candidates": {
      "main": [
        [
          {
            "node": "RAG Retrieve Storage (offline)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Assemble Storage Report": {
      "main": [
        [
          {
            "node": "Prepare reply",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Structure input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Structure",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Disk input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Disk",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose Storage Periphery input": {
      "main": [
        [
          {
            "node": "LLM Request Storage Periphery",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Retrieve Storage (offline)": {
      "main": [
        [
          {
            "node": "RAG Target Matcher",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Target Matcher": {
      "main": [
        [
          {
            "node": "Compose input ERRORS",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Compose input ERRORS": {
      "main": [
        [
          {
            "node": "LLM Request Storage Errors",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge 1": {
      "main": [
        [
          {
            "node": "Merge 2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge 3": {
      "main": [
        [
          {
            "node": "Assemble Storage Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge 2": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare prompt": {
      "main": [
        [
          {
            "node": "If  leader",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Identify Pack": {
      "main": [
        [
          {
            "node": "RAG Identify Lookup (offline)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Identify Result": {
      "main": [
        [
          {
            "node": "IF klass == \"Storage\"",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Identify": {
      "main": [
        [
          {
            "node": "Parse Identify Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ingest Attachments (Universal)": {
      "main": [
        [
          {
            "node": "Build Identify Pack",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Identify Lookup (offline)": {
      "main": [
        [
          {
            "node": "LLM Request Identify",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Storage Structure": {
      "main": [
        [
          {
            "node": "Merge 1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Request Storage Disk": {
      "main": [
        [
          {
            "node": "Merge 1",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "LLM Request Storage Errors": {
      "main": [
        [
          {
            "node": "Merge 3",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "LLM Request Storage Periphery": {
      "main": [
        [
          {
            "node": "Merge 2",
            "type": "main",
            "index": 1
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "timezone": "Europe/Moscow",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": false
  },
  "versionId": "04912df1-feb7-4755-9a1e-6077c61800bc",
  "meta": {
    "instanceId": "b7aa380705656969fa39759e0868f4a2cdd733e71248db37848cd014e207c2bc"
  },
  "id": "D2kDm2q0ZjabLod1",
  "tags": []
}